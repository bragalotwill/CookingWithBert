{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kMqSXCa9ZZF"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "I5AmUAVT3zFV",
    "outputId": "bcca0aad-7427-404e-c446-37a892cdabd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor the local demo, synching with google drive is not needed.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access datasets stored in personal google drive: grant permissions to access drive.\n",
    "\n",
    "\"\"\"\n",
    "For the local demo, synching with google drive is not needed.\n",
    "\"\"\"\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "V0j768CTGfTj",
    "outputId": "888ad288-8625-4962-c287-7867621193dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train length:  1\n",
      "test length:  1\n",
      "dev length:  1\n"
     ]
    }
   ],
   "source": [
    "# In order to open all of the recipe bearing files in the drive:\n",
    "\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "recipe_files = glob.glob(\"recipes\" + \"/*.mmf\")\n",
    "\n",
    "def get_recipe(filepath):\n",
    "  file = open(filepath, 'rb')\n",
    "  return file.read().decode('ISO-8859-1')\n",
    "\n",
    "def get_recipes(directory):\n",
    "  text = \"\"\n",
    "  for filename in directory:\n",
    "    file = open(filename, 'rb')\n",
    "    text += file.read().decode('ISO-8859-1')\n",
    "  return text.lower()\n",
    "\n",
    "\"\"\"\n",
    "train_index = int(len(recipe_files) * 0.6)\n",
    "test_index = int(train_index + len(recipe_files) * 0.2) + 1\n",
    "\n",
    "\n",
    "train_files = recipe_files[: train_index]\n",
    "test_files = recipe_files[train_index: test_index]\n",
    "dev_files = recipe_files[test_index: ]\n",
    "\"\"\"\n",
    "\n",
    "train_files = test_files =  dev_files = recipe_files\n",
    "\n",
    "\n",
    "print(\"train length: \", len(train_files))\n",
    "print(\"test length: \", len(test_files))\n",
    "print(\"dev length: \", len(dev_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqO9UbANHfp4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRECIPE_SEPERATOR = \"[END]\"\\n\\nfor filename in recipe_files:\\n  recipe = get_recipe(filename)\\n  lines = recipe.splitlines(keepends=True)\\n  for i, line in enumerate(lines):\\n    if line.startswith(\"MMMM\"):\\n      lines[i] = RECIPE_SEPERATOR\\n  out_file = open(filename, \\'w\\', encoding=\\'utf-8\\')\\n  out_file.writelines(lines)\\n  out_file.close()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Text data already processed and written back. Unecessary for demo.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "RECIPE_SEPERATOR = \"[END]\"\n",
    "\n",
    "for filename in recipe_files:\n",
    "  recipe = get_recipe(filename)\n",
    "  lines = recipe.splitlines(keepends=True)\n",
    "  for i, line in enumerate(lines):\n",
    "    if line.startswith(\"MMMM\"):\n",
    "      lines[i] = RECIPE_SEPERATOR\n",
    "  out_file = open(filename, 'w', encoding='utf-8')\n",
    "  out_file.writelines(lines)\n",
    "  out_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8iwDv639nYs"
   },
   "source": [
    "# Char-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dk3Qtvvuxr1e",
    "outputId": "0d2cba08-0933-4f31-a0fd-46f8a50cee38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 125\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def create_char_mapping():\n",
    "  chars = set()\n",
    "  for file in recipe_files:\n",
    "    recipe = get_recipe(file)\n",
    "    chars.update(recipe.lower())\n",
    "  chars = sorted(list(chars))\n",
    "  char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "  idx_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "  return char_to_idx, idx_to_char\n",
    "\n",
    "\n",
    "def write_char_vocab_to_file(idx_to_char):\n",
    "  chars = [idx_to_char[i] for i in range(len(idx_to_char))]\n",
    "  with open(\"chars.pkl\", \"wb\") as file:\n",
    "    pickle.dump(chars, file)\n",
    "\n",
    "\n",
    "def load_char_mapping():\n",
    "  with open(\"chars.pkl\", \"rb\") as file:\n",
    "    chars = pickle.load(file)\n",
    "\n",
    "  char_to_idx = dict((c, i) for i, c in enumerate(chars))\n",
    "  idx_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "  return char_to_idx, idx_to_char\n",
    "\n",
    "\n",
    "# char_to_idx, idx_to_char = create_char_mapping()\n",
    "# write_char_vocab_to_file(idx_to_char)\n",
    "\n",
    "char_to_idx, idx_to_char = load_char_mapping()\n",
    "print(len(char_to_idx), len(idx_to_char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bZyxzQASVMl"
   },
   "outputs": [],
   "source": [
    "CHAR_LSTM_LAYERS = 2\n",
    "CHAR_LSTM_UNITS = 256\n",
    "CHAR_SEQUENCE_LEN = 25\n",
    "CHAR_STEP_SIZE = 5\n",
    "CHAR_EPOCHS = 100\n",
    "CHAR_BATCH_SIZE = 256\n",
    "CHAR_DROPOUT = .1\n",
    "CHAR_LR = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rmUveHQuXWxz",
    "outputId": "a1c7c03b-9f45-4ff8-feaa-a52d5436b050"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ae7e6ab5200b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCuDNNLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, Dropout, CuDNNLSTM\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "def text_to_char_features(text, sequence_length, step_size):\n",
    "  \"\"\"\n",
    "    Transforms the raw text into training and fitting data.\n",
    "\n",
    "    x: Train data is a vectorized list of sequences pulled from the text.\n",
    "    y: fit data is the character following the corresponding sequence in x\n",
    "    indices_char: dict used to transform back from index to character\n",
    "  \"\"\"\n",
    "  maxlen = sequence_length\n",
    "  step = step_size\n",
    "  sequences = []\n",
    "  next_chars = []\n",
    "  for i in range(0, len(text) - maxlen, step):\n",
    "      sequences.append(text[i: i + maxlen])\n",
    "      next_chars.append(text[i + maxlen])\n",
    "  print('nb sequences:', len(sequences))\n",
    "  \n",
    "  return sequences, next_chars\n",
    "\n",
    "def create_char_rnn(sequence_length, vocab_size):\n",
    "  model = Sequential()\n",
    "  if CHAR_LSTM_LAYERS == 1:\n",
    "    model.add(CuDNNLSTM(CHAR_LSTM_UNITS, batch_input_shape = (CHAR_BATCH_SIZE, sequence_length, vocab_size), stateful=True))\n",
    "    model.add(Dropout(CHAR_DROPOUT))\n",
    "  else:\n",
    "    for i in range(CHAR_LSTM_LAYERS):\n",
    "      if i == 0:\n",
    "        model.add(CuDNNLSTM(CHAR_LSTM_UNITS, batch_input_shape = (CHAR_BATCH_SIZE, sequence_length, vocab_size), return_sequences=True, stateful=True))\n",
    "      elif i == CHAR_LSTM_LAYERS - 1:\n",
    "        model.add(CuDNNLSTM(CHAR_LSTM_UNITS, return_sequences=False, stateful=True))\n",
    "      else:\n",
    "        model.add(CuDNNLSTM(CHAR_LSTM_UNITS, return_sequences=True, stateful=True))\n",
    "      model.add(Dropout(CHAR_DROPOUT))\n",
    "  model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "  return model\n",
    "\n",
    "def generator(sequences, next_chars, batch_size):\n",
    "  i = 0\n",
    "  while True:\n",
    "    x = np.zeros((batch_size, CHAR_SEQUENCE_LEN, len(char_to_idx)))\n",
    "    y = np.zeros((batch_size, len(char_to_idx)))\n",
    "    for j in range(batch_size):\n",
    "      sequence = sequences[i]\n",
    "      for t, char in enumerate(sequence):\n",
    "        x[j, t, char_to_idx[char]] = 1\n",
    "      y[j, char_to_idx[next_chars[i]]] = 1\n",
    "      i += 1\n",
    "      if i == len(sequences) - 1:\n",
    "        i = 0\n",
    "    yield x, y\n",
    "\n",
    "vocab_size = len(char_to_idx)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "sequence_length = CHAR_SEQUENCE_LEN\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"models/char_rnn_weights.hdf5\",\n",
    " save_weights_only=True, monitor='loss', verbose=1,\n",
    " save_best_only=False, mode='auto', save_freq='epoch')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = CHAR_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "IjMT_YO84gZI",
    "outputId": "76120785-81d0-424a-faf0-ba65fb8cf705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 4548834\n",
      "nb sequences: 1682037\n"
     ]
    }
   ],
   "source": [
    "recipe_batch = get_recipes(train_files)\n",
    "recipe_batch_val = get_recipes(dev_files)\n",
    "char_sequences, next_chars = text_to_char_features(recipe_batch, CHAR_SEQUENCE_LEN, CHAR_STEP_SIZE)\n",
    "char_sequences_val, next_chars_val = text_to_char_features(recipe_batch_val, CHAR_SEQUENCE_LEN, CHAR_STEP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LAdvMwpTSotF",
    "outputId": "3b97471c-6c1c-4e09-b501-9525c5e40fc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 4667563\n",
      "nb sequences: 2080212\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm_7 (CuDNNLSTM)     (256, 25, 256)            391168    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (256, 25, 256)            0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_8 (CuDNNLSTM)     (256, 256)                526336    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (256, 256)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (256, 124)                31868     \n",
      "=================================================================\n",
      "Total params: 949,372\n",
      "Trainable params: 949,372\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 1.2888 - acc: 0.6158\n",
      "Epoch 00001: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 281s 15ms/step - loss: 1.2888 - acc: 0.6158\n",
      "Epoch 2/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.8533 - acc: 0.7443\n",
      "Epoch 00002: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 279s 15ms/step - loss: 0.8534 - acc: 0.7443\n",
      "Epoch 3/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.7743 - acc: 0.7656\n",
      "Epoch 00003: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 279s 15ms/step - loss: 0.7744 - acc: 0.7656\n",
      "Epoch 4/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.7356 - acc: 0.7757\n",
      "Epoch 00004: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.7357 - acc: 0.7757\n",
      "Epoch 5/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.7118 - acc: 0.7821\n",
      "Epoch 00005: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.7118 - acc: 0.7821\n",
      "Epoch 6/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6948 - acc: 0.7866\n",
      "Epoch 00006: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.6949 - acc: 0.7866\n",
      "Epoch 7/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.6823 - acc: 0.7898\n",
      "Epoch 00007: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6824 - acc: 0.7898\n",
      "Epoch 8/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6724 - acc: 0.7925\n",
      "Epoch 00008: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6725 - acc: 0.7924\n",
      "Epoch 9/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.6642 - acc: 0.7946\n",
      "Epoch 00009: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6643 - acc: 0.7945\n",
      "Epoch 10/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6576 - acc: 0.7964\n",
      "Epoch 00010: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6577 - acc: 0.7964\n",
      "Epoch 11/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.6520 - acc: 0.7979\n",
      "Epoch 00011: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6521 - acc: 0.7978\n",
      "Epoch 12/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.7992\n",
      "Epoch 00012: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6472 - acc: 0.7991\n",
      "Epoch 13/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.6426 - acc: 0.8003\n",
      "Epoch 00013: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.6427 - acc: 0.8003\n",
      "Epoch 14/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6390 - acc: 0.8012\n",
      "Epoch 00014: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6391 - acc: 0.8012\n",
      "Epoch 15/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.6355 - acc: 0.8022\n",
      "Epoch 00015: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.6356 - acc: 0.8022\n",
      "Epoch 16/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6326 - acc: 0.8028\n",
      "Epoch 00016: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6327 - acc: 0.8028\n",
      "Epoch 17/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.6297 - acc: 0.8036\n",
      "Epoch 00017: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6299 - acc: 0.8035\n",
      "Epoch 18/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.8043\n",
      "Epoch 00018: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6271 - acc: 0.8043\n",
      "Epoch 19/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6250 - acc: 0.8047\n",
      "Epoch 00019: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6251 - acc: 0.8047\n",
      "Epoch 20/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6226 - acc: 0.8055\n",
      "Epoch 00020: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.6227 - acc: 0.8054\n",
      "Epoch 21/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.6207 - acc: 0.8057\n",
      "Epoch 00021: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6208 - acc: 0.8057\n",
      "Epoch 22/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6187 - acc: 0.8063\n",
      "Epoch 00022: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6188 - acc: 0.8063\n",
      "Epoch 23/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6171 - acc: 0.8069\n",
      "Epoch 00023: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6172 - acc: 0.8068\n",
      "Epoch 24/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6153 - acc: 0.8073\n",
      "Epoch 00024: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.6154 - acc: 0.8072\n",
      "Epoch 25/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.6142 - acc: 0.8074\n",
      "Epoch 00025: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6143 - acc: 0.8074\n",
      "Epoch 26/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.6125 - acc: 0.8081\n",
      "Epoch 00026: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6126 - acc: 0.8080\n",
      "Epoch 27/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6113 - acc: 0.8082\n",
      "Epoch 00027: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.6114 - acc: 0.8082\n",
      "Epoch 28/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.6098 - acc: 0.8086\n",
      "Epoch 00028: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6099 - acc: 0.8085\n",
      "Epoch 29/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.8088\n",
      "Epoch 00029: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6087 - acc: 0.8088\n",
      "Epoch 30/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.6077 - acc: 0.8092\n",
      "Epoch 00030: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6077 - acc: 0.8092\n",
      "Epoch 31/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6067 - acc: 0.8095\n",
      "Epoch 00031: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6067 - acc: 0.8094\n",
      "Epoch 32/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6056 - acc: 0.8097\n",
      "Epoch 00032: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6057 - acc: 0.8097\n",
      "Epoch 33/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.8099\n",
      "Epoch 00033: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6042 - acc: 0.8099\n",
      "Epoch 34/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.6032 - acc: 0.8104\n",
      "Epoch 00034: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.6032 - acc: 0.8104\n",
      "Epoch 35/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.6022 - acc: 0.8105\n",
      "Epoch 00035: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.6023 - acc: 0.8105\n",
      "Epoch 36/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.6013 - acc: 0.8107\n",
      "Epoch 00036: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.6013 - acc: 0.8107\n",
      "Epoch 37/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.6007 - acc: 0.8110\n",
      "Epoch 00037: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.6007 - acc: 0.8110\n",
      "Epoch 38/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5995 - acc: 0.8113\n",
      "Epoch 00038: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5995 - acc: 0.8113\n",
      "Epoch 39/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5989 - acc: 0.8114\n",
      "Epoch 00039: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5989 - acc: 0.8114\n",
      "Epoch 40/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5982 - acc: 0.8115\n",
      "Epoch 00040: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5982 - acc: 0.8115\n",
      "Epoch 41/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5973 - acc: 0.8118\n",
      "Epoch 00041: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 274s 15ms/step - loss: 0.5974 - acc: 0.8118\n",
      "Epoch 42/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8120\n",
      "Epoch 00042: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5967 - acc: 0.8119\n",
      "Epoch 43/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5959 - acc: 0.8120\n",
      "Epoch 00043: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.5960 - acc: 0.8120\n",
      "Epoch 44/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.8125\n",
      "Epoch 00044: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5948 - acc: 0.8125\n",
      "Epoch 45/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5942 - acc: 0.8127\n",
      "Epoch 00045: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5942 - acc: 0.8127\n",
      "Epoch 46/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5938 - acc: 0.8128\n",
      "Epoch 00046: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5938 - acc: 0.8128\n",
      "Epoch 47/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.8127\n",
      "Epoch 00047: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5931 - acc: 0.8127\n",
      "Epoch 48/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5926 - acc: 0.8131\n",
      "Epoch 00048: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.5926 - acc: 0.8131\n",
      "Epoch 49/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5919 - acc: 0.8133\n",
      "Epoch 00049: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5919 - acc: 0.8132\n",
      "Epoch 50/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5916 - acc: 0.8131\n",
      "Epoch 00050: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5916 - acc: 0.8131\n",
      "Epoch 51/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5908 - acc: 0.8135\n",
      "Epoch 00051: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5908 - acc: 0.8135\n",
      "Epoch 52/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5901 - acc: 0.8136\n",
      "Epoch 00052: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5901 - acc: 0.8136\n",
      "Epoch 53/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5892 - acc: 0.8139\n",
      "Epoch 00053: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5892 - acc: 0.8139\n",
      "Epoch 54/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5888 - acc: 0.8139\n",
      "Epoch 00054: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5889 - acc: 0.8139\n",
      "Epoch 55/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5883 - acc: 0.8140\n",
      "Epoch 00055: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.5883 - acc: 0.8140\n",
      "Epoch 56/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5881 - acc: 0.8141\n",
      "Epoch 00056: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5881 - acc: 0.8140\n",
      "Epoch 57/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.8146\n",
      "Epoch 00057: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5874 - acc: 0.8146\n",
      "Epoch 58/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8147\n",
      "Epoch 00058: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5865 - acc: 0.8147\n",
      "Epoch 59/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5865 - acc: 0.8146\n",
      "Epoch 00059: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5865 - acc: 0.8146\n",
      "Epoch 60/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5858 - acc: 0.8148\n",
      "Epoch 00060: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5858 - acc: 0.8147\n",
      "Epoch 61/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5851 - acc: 0.8148\n",
      "Epoch 00061: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5851 - acc: 0.8148\n",
      "Epoch 62/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5846 - acc: 0.8150\n",
      "Epoch 00062: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 274s 15ms/step - loss: 0.5847 - acc: 0.8150\n",
      "Epoch 63/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5841 - acc: 0.8152\n",
      "Epoch 00063: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5841 - acc: 0.8152\n",
      "Epoch 64/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5835 - acc: 0.8153\n",
      "Epoch 00064: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5835 - acc: 0.8153\n",
      "Epoch 65/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5832 - acc: 0.8154\n",
      "Epoch 00065: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5832 - acc: 0.8154\n",
      "Epoch 66/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5829 - acc: 0.8155\n",
      "Epoch 00066: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5829 - acc: 0.8155\n",
      "Epoch 67/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5822 - acc: 0.8156\n",
      "Epoch 00067: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5823 - acc: 0.8156\n",
      "Epoch 68/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5821 - acc: 0.8155\n",
      "Epoch 00068: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5821 - acc: 0.8155\n",
      "Epoch 69/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5817 - acc: 0.8158\n",
      "Epoch 00069: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 273s 15ms/step - loss: 0.5817 - acc: 0.8158\n",
      "Epoch 70/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.8159\n",
      "Epoch 00070: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5810 - acc: 0.8159\n",
      "Epoch 71/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5804 - acc: 0.8161\n",
      "Epoch 00071: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5804 - acc: 0.8161\n",
      "Epoch 72/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5801 - acc: 0.8161\n",
      "Epoch 00072: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5801 - acc: 0.8161\n",
      "Epoch 73/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5798 - acc: 0.8162\n",
      "Epoch 00073: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5798 - acc: 0.8162\n",
      "Epoch 74/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5795 - acc: 0.8162\n",
      "Epoch 00074: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5795 - acc: 0.8162\n",
      "Epoch 75/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5791 - acc: 0.8163\n",
      "Epoch 00075: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5791 - acc: 0.8163\n",
      "Epoch 76/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5786 - acc: 0.8167\n",
      "Epoch 00076: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 274s 15ms/step - loss: 0.5786 - acc: 0.8166\n",
      "Epoch 77/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5779 - acc: 0.8166\n",
      "Epoch 00077: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5780 - acc: 0.8166\n",
      "Epoch 78/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5778 - acc: 0.8166\n",
      "Epoch 00078: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5778 - acc: 0.8166\n",
      "Epoch 79/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5775 - acc: 0.8169\n",
      "Epoch 00079: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5775 - acc: 0.8169\n",
      "Epoch 80/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5767 - acc: 0.8171\n",
      "Epoch 00080: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5767 - acc: 0.8171\n",
      "Epoch 81/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5765 - acc: 0.8171\n",
      "Epoch 00081: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5765 - acc: 0.8171\n",
      "Epoch 82/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5763 - acc: 0.8172\n",
      "Epoch 00082: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5763 - acc: 0.8172\n",
      "Epoch 83/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5758 - acc: 0.8173\n",
      "Epoch 00083: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 274s 15ms/step - loss: 0.5758 - acc: 0.8173\n",
      "Epoch 84/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5755 - acc: 0.8174\n",
      "Epoch 00084: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5755 - acc: 0.8174\n",
      "Epoch 85/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5754 - acc: 0.8173\n",
      "Epoch 00085: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.5754 - acc: 0.8173\n",
      "Epoch 86/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5751 - acc: 0.8175\n",
      "Epoch 00086: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 278s 15ms/step - loss: 0.5751 - acc: 0.8175\n",
      "Epoch 87/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5746 - acc: 0.8175\n",
      "Epoch 00087: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5746 - acc: 0.8175\n",
      "Epoch 88/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5747 - acc: 0.8176\n",
      "Epoch 00088: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5747 - acc: 0.8176\n",
      "Epoch 89/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5740 - acc: 0.8177\n",
      "Epoch 00089: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5740 - acc: 0.8177\n",
      "Epoch 90/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5738 - acc: 0.8178\n",
      "Epoch 00090: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 275s 15ms/step - loss: 0.5738 - acc: 0.8178\n",
      "Epoch 91/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5733 - acc: 0.8180\n",
      "Epoch 00091: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5733 - acc: 0.8180\n",
      "Epoch 92/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.8179\n",
      "Epoch 00092: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5731 - acc: 0.8179\n",
      "Epoch 93/100\n",
      "18230/18233 [============================>.] - ETA: 0s - loss: 0.5730 - acc: 0.8180\n",
      "Epoch 00093: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5730 - acc: 0.8180\n",
      "Epoch 94/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5725 - acc: 0.8179\n",
      "Epoch 00094: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5725 - acc: 0.8179\n",
      "Epoch 95/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5722 - acc: 0.8183\n",
      "Epoch 00095: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5722 - acc: 0.8183\n",
      "Epoch 96/100\n",
      "18232/18233 [============================>.] - ETA: 0s - loss: 0.5718 - acc: 0.8183\n",
      "Epoch 00096: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 276s 15ms/step - loss: 0.5718 - acc: 0.8183\n",
      "Epoch 97/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.8185\n",
      "Epoch 00097: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 274s 15ms/step - loss: 0.5713 - acc: 0.8185\n",
      "Epoch 98/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.8184\n",
      "Epoch 00098: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5713 - acc: 0.8184\n",
      "Epoch 99/100\n",
      "18231/18233 [============================>.] - ETA: 0s - loss: 0.5707 - acc: 0.8186\n",
      "Epoch 00099: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5708 - acc: 0.8186\n",
      "Epoch 100/100\n",
      "18229/18233 [============================>.] - ETA: 0s - loss: 0.5708 - acc: 0.8186\n",
      "Epoch 00100: saving model to content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\n",
      "18233/18233 [==============================] - 277s 15ms/step - loss: 0.5709 - acc: 0.8186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c34c7b57c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_char_rnn(CHAR_SEQUENCE_LEN, vocab_size)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(generator(char_sequences, next_chars, CHAR_BATCH_SIZE), steps_per_epoch = int(len(char_sequences)/CHAR_BATCH_SIZE) + 1, epochs = CHAR_EPOCHS, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "DyGsZAupcY0k",
    "outputId": "47183d65-098c-440b-bf5c-aabe4f575933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6571/6571 [==============================] - 39s 6ms/step - loss: 2.0760 - accuracy: 0.7037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.075989007949829, 0.7036564350128174]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from saved weight file\n",
    "# so that training can continue between sessions and testing can happen at will\n",
    "char_rnn = create_char_rnn(CHAR_SEQUENCE_LEN, vocab_size)\n",
    "char_rnn.load_weights(\"models/char_rnn_weights.hdf5\")\n",
    "char_rnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# evaluate on cross validation set\n",
    "recipe_batch_val = get_recipes(dev_files)\n",
    "char_rnn.evaluate(generator(char_sequences_val, next_chars_val, CHAR_BATCH_SIZE), steps = int(len(char_sequences_val)/CHAR_BATCH_SIZE) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "colab_type": "code",
    "id": "w9eUBBBP5RRU",
    "outputId": "64e89366-c407-48af-fc01-ad8d6a434c50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       -chopped\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "      1    egg\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "      2 c  brown sugar\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "    1/4 c  sugar\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "      4 c  sugar\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      " \r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "[end          sour cream\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "[]          -deer printed cookbook by jean harr & davis & powdered sugar frosting\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "           -recipes\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "\r\n",
      "[end]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a2dff46d33bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mgen_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/CookingWithBert/generated/char_gentext_{:%Y%m%d_%H%M%S}.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/CookingWithBert/generated/char_gentext_20200418_230420.txt'"
     ]
    }
   ],
   "source": [
    "def char_sample(predictions, temperature = 1):\n",
    "    pred = np.asarray(predictions).astype('float64')\n",
    "    pred = np.log(pred) / temperature\n",
    "    exp_pred = np.exp(pred)\n",
    "    pred = exp_pred / np.sum(exp_pred)\n",
    "    prob = np.random.multinomial(1, pred, 1)\n",
    "    return np.argmax(prob)\n",
    "\n",
    "def char_predict(max_len = 10000, temperature = 0.5):\n",
    "  index = np.random.randint(len(char_sequences_val))\n",
    "  sentence = char_sequences_val[index]\n",
    "  predicted_text = \"\"\n",
    "  while '[end]' not in predicted_text and len(predicted_text) < max_len:\n",
    "    x = np.zeros((CHAR_BATCH_SIZE, CHAR_SEQUENCE_LEN, len(char_to_idx)))\n",
    "    for t, char in enumerate(sentence):\n",
    "      x[0, t, char_to_idx[char]] = 1\n",
    "    pred = char_rnn.predict_on_batch(x)[0]\n",
    "    prediction_index = char_sample(pred, temperature)\n",
    "    predicted_letter = idx_to_char[prediction_index]\n",
    "    predicted_text += predicted_letter\n",
    "    sentence = sentence[1:] + predicted_letter\n",
    "  return predicted_text\n",
    "\n",
    "char_predicted_text = char_predict()\n",
    "print(char_predicted_text)\n",
    "\n",
    "gen_file = \"generated/char_gentext_{:%Y%m%d_%H%M%S}.txt\".format(datetime.utcnow())\n",
    "file = open(gen_file, \"w\")\n",
    "file.write(char_predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEjYNPYh91ZJ"
   },
   "source": [
    "# Word-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tjPHhkTacV8i",
    "outputId": "bd787191-8313-4b11-e294-62295f6691d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7909454 4567616 1655639\n",
      "16198 16198\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "\n",
    "def create_tokens(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  return tokens\n",
    "\n",
    "def write_tokens_to_file(tokens, name):\n",
    "  with open(\"tokens.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tokens, file)\n",
    "\n",
    "def load_tokens(name):\n",
    "  with open(name, \"rb\") as file:\n",
    "    return pickle.load(file)\n",
    "\n",
    "def create_word_mapping(tokens):\n",
    "  words = set(tokens)\n",
    "  word_to_idx = dict((c, i) for i, c in enumerate(words))\n",
    "  idx_to_word = dict((i, c) for i, c in enumerate(words))\n",
    "  return word_to_idx, idx_to_word\n",
    "\n",
    "def write_word_vocab_to_file(idx_to_word):\n",
    "  words = [idx_to_word[i] for i in range(len(idx_to_word))]\n",
    "  with open(\"words.pkl\", \"wb\") as file:\n",
    "    pickle.dump(words, file)\n",
    "\n",
    "\n",
    "def load_word_mapping():\n",
    "  with open(\"words.pkl\", \"rb\") as file:\n",
    "    words = pickle.load(file)\n",
    "\n",
    "  words_to_idx = dict((c, i) for i, c in enumerate(words))\n",
    "  idx_to_words = dict((i, c) for i, c in enumerate(words))\n",
    "  return words_to_idx, idx_to_words\n",
    "\n",
    "# all_tokens = create_tokens(get_recipes(recipe_files))\n",
    "# write_tokens_to_file(all_tokens, \"alltokens.pk1\")\n",
    "# train_tokens = create_tokens(get_recipes(train_files))\n",
    "# write_tokens_to_file(all_tokens, \"traintokens.pk1\")\n",
    "# val_tokens = create_tokens(get_recipes(dev_files))\n",
    "# write_tokens_to_file(val_tokens, \"valtokens.pk1\")\n",
    "\n",
    "all_tokens = load_tokens(\"alltokens.pk1\")\n",
    "train_tokens = load_tokens(\"traintokens.pk1\")\n",
    "val_tokens = load_tokens(\"valtokens.pk1\")\n",
    "\n",
    "# word_to_idx, idx_to_word = create_word_mapping(all_tokens)\n",
    "# write_word_vocab_to_file(idx_to_word)\n",
    "\n",
    "word_to_idx, idx_to_word = load_word_mapping()\n",
    "print(len(all_tokens), len(train_tokens), len(val_tokens))\n",
    "print(len(word_to_idx), len(idx_to_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODoOKcAdbfYZ"
   },
   "outputs": [],
   "source": [
    "WORD_LSTM_LAYERS = 1\n",
    "WORD_LSTM_UNITS = 256\n",
    "WORD_SEQUENCE_LEN = 20\n",
    "WORD_STEP_SIZE = 15\n",
    "WORD_EPOCHS = 10\n",
    "WORD_BATCH_SIZE = 32\n",
    "WORD_DROPOUT = .1\n",
    "WORD_LR = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wS7-2k1nkPMP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Embedding, CuDNNLSTM, Dropout\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "\n",
    "def text_to_word_features(tokens, sequence_length, step_size):\n",
    "  # cut the text in semi-redundant sequences of maxlen characters\n",
    "\n",
    "  maxlen = sequence_length\n",
    "  step = step_size\n",
    "  sequences = []\n",
    "  next_words = []\n",
    "  for i in range(0, len(tokens) - maxlen, step):\n",
    "      sequences.append(tokens[i:i+maxlen])\n",
    "      next_words.append(tokens[i + maxlen])\n",
    "  print('nb sequences:', len(sequences))\n",
    "\n",
    "  return sequences, next_words\n",
    "\n",
    "def create_word_rnn(sequence_length, vocab_size, batch_size):\n",
    "  model = Sequential()\n",
    "  if WORD_LSTM_LAYERS == 1:\n",
    "    model.add(CuDNNLSTM(WORD_LSTM_UNITS, batch_input_shape = (WORD_BATCH_SIZE, sequence_length, vocab_size), stateful=True))\n",
    "    model.add(Dropout(WORD_DROPOUT))\n",
    "  else:\n",
    "    for i in range(WORD_LSTM_LAYERS):\n",
    "      if i == 0:\n",
    "        model.add(CuDNNLSTM(WORD_LSTM_UNITS, batch_input_shape = (WORD_BATCH_SIZE, sequence_length, vocab_size), return_sequences=True, stateful=True))\n",
    "      elif i == WORD_LSTM_LAYERS - 1:\n",
    "        model.add(CuDNNLSTM(WORD_LSTM_UNITS, return_sequences=False, stateful=True))\n",
    "      else:\n",
    "        model.add(CuDNNLSTM(WORD_LSTM_UNITS, return_sequences=True, stateful=True))\n",
    "      model.add(Dropout(WORD_DROPOUT))\n",
    "  model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "  return model\n",
    "\n",
    "def generator(sequences, next_words, batch_size):\n",
    "  i = 0\n",
    "  while True:\n",
    "    x = np.zeros((batch_size, WORD_SEQUENCE_LEN, len(word_to_idx)))\n",
    "    y = np.zeros((batch_size, len(word_to_idx)))\n",
    "    for j in range(batch_size):\n",
    "      sequence = sequences[i]\n",
    "      for t, word in enumerate(sequence):\n",
    "        x[j, t, word_to_idx[word]] = 1\n",
    "      y[j, word_to_idx[next_words[i]]] = 1\n",
    "      i += 1\n",
    "      if i == len(sequences) - 1:\n",
    "        i = 0\n",
    "    yield x, y\n",
    "\n",
    "word_vocab_size = len(word_to_idx)\n",
    "word_sequence_length = WORD_SEQUENCE_LEN\n",
    "\n",
    "word_checkpoint = ModelCheckpoint(\"models/word_rnn_weights.hdf5\",\n",
    " save_weights_only=True, monitor='loss', verbose=1,\n",
    " save_best_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr = WORD_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hnjEzyL9hbkK",
    "outputId": "ffffc07d-aa73-4117-b296-2f0e853f0d0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 416646\n",
      "nb sequences: 96108\n"
     ]
    }
   ],
   "source": [
    "word_sequences, next_words = text_to_word_features(train_tokens, WORD_SEQUENCE_LEN, WORD_STEP_SIZE)\n",
    "word_sequences_val, next_words_val = text_to_word_features(val_tokens, WORD_SEQUENCE_LEN, WORD_STEP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "totq9MqXX6ER",
    "outputId": "0ca6fb85-b064-4d8e-98fd-1abe74405dc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\William\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "cu_dnnlstm (CuDNNLSTM)       (32, 256)                 68114432  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (32, 256)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, 66260)               17028820  \n",
      "=================================================================\n",
      "Total params: 85,143,252\n",
      "Trainable params: 85,143,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 5.3691 - acc: 0.1760\n",
      "Epoch 00001: loss improved from inf to 5.36922, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3746s 288ms/step - loss: 5.3692 - acc: 0.1760\n",
      "Epoch 2/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 4.1751 - acc: 0.2878\n",
      "Epoch 00002: loss improved from 5.36922 to 4.17517, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3745s 288ms/step - loss: 4.1752 - acc: 0.2878\n",
      "Epoch 3/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 3.7784 - acc: 0.3257\n",
      "Epoch 00003: loss improved from 4.17517 to 3.77849, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3746s 288ms/step - loss: 3.7785 - acc: 0.3257\n",
      "Epoch 4/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 3.5130 - acc: 0.3523\n",
      "Epoch 00004: loss improved from 3.77849 to 3.51294, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3742s 287ms/step - loss: 3.5129 - acc: 0.3523\n",
      "Epoch 5/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 3.2967 - acc: 0.3757\n",
      "Epoch 00005: loss improved from 3.51294 to 3.29680, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3747s 288ms/step - loss: 3.2968 - acc: 0.3757\n",
      "Epoch 6/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 3.1037 - acc: 0.3990\n",
      "Epoch 00006: loss improved from 3.29680 to 3.10371, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3747s 288ms/step - loss: 3.1037 - acc: 0.3990\n",
      "Epoch 7/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 2.9285 - acc: 0.4224\n",
      "Epoch 00007: loss improved from 3.10371 to 2.92846, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3754s 288ms/step - loss: 2.9285 - acc: 0.4224\n",
      "Epoch 8/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 2.7711 - acc: 0.4450\n",
      "Epoch 00008: loss improved from 2.92846 to 2.77110, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3756s 288ms/step - loss: 2.7711 - acc: 0.4450\n",
      "Epoch 9/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 2.6268 - acc: 0.4676\n",
      "Epoch 00009: loss improved from 2.77110 to 2.62687, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3759s 289ms/step - loss: 2.6269 - acc: 0.4676\n",
      "Epoch 10/10\n",
      "13020/13021 [============================>.] - ETA: 0s - loss: 2.4962 - acc: 0.4883\n",
      "Epoch 00010: loss improved from 2.62687 to 2.49613, saving model to content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
      "13021/13021 [==============================] - 3750s 288ms/step - loss: 2.4961 - acc: 0.4883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x240f59b9d08>"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_word_rnn(word_sequence_length, word_vocab_size, WORD_BATCH_SIZE)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(generator(word_sequences, next_words, WORD_BATCH_SIZE), steps_per_epoch = int(len(sequences)/WORD_BATCH_SIZE) + 1, epochs = WORD_EPOCHS, callbacks=[word_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2vP24yt_X86O",
    "outputId": "1dd42349-0e31-4e03-f1e5-3182f5efc8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 136s 136ms/step - loss: 4.1885 - accuracy: 0.3184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.188460350036621, 0.31840625405311584]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rnn = create_word_rnn(word_sequence_length, word_vocab_size, WORD_BATCH_SIZE)\n",
    "word_rnn.load_weights(\"models/word_rnn_weights.hdf5\")\n",
    "word_rnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# word_rnn.evaluate(generator(word_sequences_val, next_words_val, WORD_BATCH_SIZE), steps = int(len(sequences)/WORD_BATCH_SIZE) + 1)\n",
    "word_rnn.evaluate(generator(word_sequences_val, next_words_val, WORD_BATCH_SIZE), steps = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "6Jpqdcb27uv2",
    "outputId": "d8fc54ee-747d-4907-e2f1-def10d5ce402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": butter : rub 's dressing categories : fish yield : 4 servings 3 lb boneless chicken breasts 1 lb butter 1 cn ( 7 oz ) tomato paste 6 oz can tomato sauce 2 tb chili powder 1 tb wine vinegar 1 tb honey 1 ts dry mustard 1 tb lemon juice 1/2 ts salt 1/2 ts pepper 1/4 ts dried tarragon 2 ts dried oregano 1/4 ts dried rosemary 1/4 ts dried basil 1/4 ts dried oregano 1/4 ts dried thyme 1/4 ts dried rosemary 1/2 ts white pepper 1/4 ts salt 1/4 ts pepper 1 c buttermilk , mixed with 1/2 c water 1/4 c sugar 1/4 c butter flavor melted butter 1 ts vanilla 2 tb butter 1 ts vanilla 2 ts baking powder 1 ts baking soda 1 ts baking soda 1 c milk 2 tb butter 2 tb butter 2 c flour 1 ts baking soda 1 ts baking soda 1 ts baking soda 1 ts vanilla 1 c chopped walnuts 1 1/2 c chopped pecans ( or less ) 1 ts vanilla extract [ end ]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-ad499704faa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mgen_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/CookingWithBert/generated/word_gentext_{:%Y%m%d_%H%M%S}.txt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutcnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_predicted_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/CookingWithBert/generated/word_gentext_20200418_231400.txt'"
     ]
    }
   ],
   "source": [
    "def word_sample(predictions, temperature = 1):\n",
    "    pred = np.asarray(predictions).astype('float64')\n",
    "    pred = np.log(pred) / temperature\n",
    "    exp_pred = np.exp(pred)\n",
    "    pred = exp_pred / np.sum(exp_pred)\n",
    "    prob = np.random.multinomial(1, pred, 1)\n",
    "    return np.argmax(prob)\n",
    "\n",
    "def word_prediction(max_len = 1000, temp = 0.5):\n",
    "  index = np.random.randint(len(word_sequences_val))\n",
    "  sentence = word_sequences_val[index]\n",
    "  predicted_text = []\n",
    "  while (len(predicted_text) == 0 or predicted_text[-1] is not ']') and len(predicted_text) < max_len:\n",
    "    x = np.zeros((WORD_BATCH_SIZE, WORD_SEQUENCE_LEN, len(word_to_idx)))\n",
    "    for t, word in enumerate(sentence):\n",
    "      x[0, t, word_to_idx[word]] = 1\n",
    "    pred = word_rnn.predict(x, verbose = 0)[0]\n",
    "    prediction_index = sample(pred, temp)\n",
    "    predicted_word = idx_to_word[prediction_index]\n",
    "    predicted_text += [predicted_word]\n",
    "    sentence = sentence[1:] + [predicted_word]\n",
    "  return ' '.join(predicted_text)\n",
    "\n",
    "word_predicted_text = word_prediction()\n",
    "print(word_predicted_text)\n",
    "\n",
    "gen_file = \"generated/word_gentext_{:%Y%m%d_%H%M%S}.txt\".format(datetime.utcnow())\n",
    "file = open(gen_file, \"w\")\n",
    "file.write(word_predicted_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EURY8Rq997TX"
   },
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "ejqZZZXGI3ql",
    "outputId": "7ab4a488-7ad5-45ca-e3cf-99052836c4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Collecting gpt-2-simple\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/e4/a90add0c3328eed38a46c3ed137f2363b5d6a07bf13ee5d5d4d1e480b8c3/gpt_2_simple-0.7.1.tar.gz\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.38.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.18.2)\n",
      "Collecting toposort\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2020.4.5.1)\n",
      "Building wheels for collected packages: gpt-2-simple\n",
      "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.7.1-cp36-none-any.whl size=23581 sha256=c32c4a6aa052e877ae14fdf3ad7bdddd1c148b0397633729c9a80a27a392b53f\n",
      "  Stored in directory: /root/.cache/pip/wheels/0c/f8/23/b53ce437504597edff76bf9c3b8de08ad716f74f6c6baaa91a\n",
      "Successfully built gpt-2-simple\n",
      "Installing collected packages: toposort, gpt-2-simple\n",
      "Successfully installed gpt-2-simple-0.7.1 toposort-1.5\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "# !pip uninstall tensorflow\n",
    "# !pip install tensorflow==1.15\n",
    "!pip3 install gpt-2-simple\n",
    "\n",
    "import tensorflow as tf\n",
    "import gpt_2_simple as gpt2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9g9-cbhQJg3b",
    "outputId": "14103f46-28a9-4b49-f1c7-5fec091bf793"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 538Mit/s]                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 124M model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching encoder.json: 1.05Mit [00:00, 75.3Mit/s]                                                   \n",
      "Fetching hparams.json: 1.05Mit [00:00, 587Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:03, 151Mit/s]\n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 392Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 157Mit/s]                                                 \n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 149Mit/s]                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made it to 18 file index.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Loading checkpoint checkpoint/run1/model-20000\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 378721 tokens\n",
      "Training...\n",
      "Saving checkpoint/run1/model-20000\n",
      "======== SAMPLE 1 ========\n",
      ".  Put bacon strips in a flameproof casserole and\n",
      "  pour in half-and-half.  Mix flour, sugar and salt into saucepan, then add\n",
      "  water and bring to the boil.  Add salt and baking powder and mix well. Put\n",
      "  turkey in casserole, cover, and marinate overnight.  (I have found that\n",
      "  egg dishes have better than that, but I like my turkey good.)\n",
      "  \n",
      "  Preheat oven to 325 deg f.  Unmednately melt butter in a pan and put in\n",
      "  the oven, turn off the oven and leave turkey in the casserole overnight.\n",
      "  Unmold turkey and marinate with sauce overnight, or put turkey on a\n",
      "  plate and top with sauce.  Sprinkle with grated cheese, or serve with\n",
      "  butter. From: Stephanie Da Silva\n",
      " \n",
      "[END] \n",
      "[END] \n",
      "      Title: GOLDEN TURKEY BREAST PARMIGIANA\n",
      " Categories: Poultry\n",
      "      Yield: 4 Servings\n",
      " \n",
      "      2 lb Whole turkey breast\n",
      "      1 lg Turkey breast\n",
      "    1/2 c  Melted butter\n",
      "      2 tb Flour\n",
      "      1 ts Lemon pepper\n",
      "    1/3 c  Parmesan cheese, grated\n",
      "    3/4 c  Golden flour\n",
      "    3/4 c  Flour\n",
      "           Sauce:\n",
      "    1/4 c  Tomato sauce\n",
      "      2 tb Sugar\n",
      "      1 ts Finely shredded orange peel\n",
      "      1 tb Cornstarch\n",
      "      1    Egg\n",
      " \n",
      "  1. Cut each chicken breast in half to make two 4\" pieces.\n",
      "  \n",
      "  2. On a 16x10x1-inch baking dish, arrange half the egg yolks. Cover\n",
      "  with half the sauce.  Cover.  On a low surface turn glace over half\n",
      "  of the pieces. Cover.  On a high surface turn savory side over.\n",
      "  \n",
      "  3. In a small saucepan over low heat, melt the butter. Stir in\n",
      "  the flour and cook 1 minute. Remove from heat. Stir in the lemon pepper\n",
      "  slices. Cover. Cook in simmering water about 1 minute, or until\n",
      "  crisp-tender. Pass the sauce separately.\n",
      "  \n",
      "  4. Reduce heat to low.  Cover. Simmer 1 1/2 to 2 hours, or until\n",
      "  chicken is completely cooked and tender-crisp. ( boiled, that is.)\n",
      "  \n",
      "  5. Steak may be substituted for chicken if desired. Serves 4 to 6\n",
      " \n",
      "[END] \n",
      "[END] \n",
      "      Title: GOLDEN TURKEY skimily\n",
      " Categories: Poultry\n",
      "      Yield: 6 Servings\n",
      " \n",
      "      1 ea Onions\n",
      "      2 ea Celery ribs\n",
      "    1/2 lb Turkey bacon\n",
      "      1 ea Carrots\n",
      "      1 tb Butter\n",
      "      1 cl Garlic, minced\n",
      "      2 tb Catsup\n",
      "      1 cn Cream of mushroom soup\n",
      "      1 cn Cream of chicken soup\n",
      "  3 1/2 tb Honey\n",
      "          lic slicer tow sauce\n",
      "      2 tb Butter, melted\n",
      " \n",
      "  Place onion, celery, ribs, butter and garlic in medium bowl.\n",
      "  \n",
      "  Mix cream of mushroom soup, butter,lic slicer and blend well.\n",
      "  \n",
      "  Pour into greased 1.5-quart casserole.\n",
      "  \n",
      "  Bake 375 3/4-inch bake pan for 45 to 50 minutes or until center\n",
      "  is no longer pink inside.\n",
      "  \n",
      "  Remove from oven.\n",
      "  \n",
      "  Serve plain, with steamed vegetables or as a pt, for the packaged\n",
      "  exception: warm 1 cup broccoli orisk or quillon cook rice and\n",
      "  stir into vegetable mixture unbeaten. For an entire grain such as\n",
      "  millet,ESEOURS, and CKE, serve 3 or 4 ounces of sweetened condensed\n",
      "  milk between layers forating and serving.\n",
      "  \n",
      "  Source: [IceMEATball] on Prodigy and Sep. 11, reserve calories\n",
      "  combined\n",
      " \n",
      "[END] \n",
      "[END] \n",
      " \n",
      "\n",
      "[20025 | 135.54] loss=0.22 avg=0.22\n",
      "[20050 | 242.57] loss=0.13 avg=0.17\n",
      "[20075 | 349.65] loss=0.19 avg=0.18\n",
      "[20100 | 456.80] loss=0.15 avg=0.17\n",
      "[20125 | 563.93] loss=0.15 avg=0.17\n",
      "[20150 | 671.14] loss=0.13 avg=0.16\n",
      "[20175 | 778.23] loss=0.09 avg=0.15\n",
      "[20200 | 885.31] loss=0.13 avg=0.15\n",
      "[20225 | 992.38] loss=0.10 avg=0.14\n",
      "[20250 | 1099.40] loss=0.10 avg=0.14\n",
      "Saving checkpoint/run1/model-20250\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "[20275 | 1208.20] loss=0.08 avg=0.13\n",
      "[20300 | 1315.24] loss=0.13 avg=0.13\n",
      "[20325 | 1422.28] loss=0.10 avg=0.13\n",
      "[20350 | 1529.42] loss=0.11 avg=0.13\n",
      "[20375 | 1636.55] loss=0.09 avg=0.13\n",
      "[20400 | 1743.67] loss=0.09 avg=0.12\n"
     ]
    }
   ],
   "source": [
    "# load from google drive and run trainng regemine\n",
    "\n",
    "model_name = \"124M\"\n",
    "\n",
    "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
    "\tprint(f\"Downloading {model_name} model...\")\n",
    "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
    "\n",
    "\n",
    "run = \"run1\"\n",
    "gpt2.copy_checkpoint_from_gdrive(run_name=run)\n",
    "\n",
    "# in case restarting training in the middle due to a disconnect.\n",
    "start_index = 18\n",
    "\n",
    "for i in range(start_index, len(train_files)):\n",
    "  print(\"Made it to %d file index.\" % (i))\n",
    "  recipe_file = train_files[i]\n",
    "\n",
    "  tf.reset_default_graph()\n",
    "  sess = gpt2.start_tf_sess()\n",
    "  gpt2.finetune(sess,\n",
    "                dataset=recipe_file,\n",
    "                model_name='124M',\n",
    "                steps=500,\n",
    "                restore_from='latest',\n",
    "                run_name=run,\n",
    "                print_every=25,\n",
    "                sample_every=500,\n",
    "                save_every=250\n",
    "                )\n",
    "  gpt2.copy_checkpoint_to_gdrive(run_name=run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "WssIvEatci_b",
    "outputId": "93b48581-fc48-497b-9dcc-3aa7e9174e48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-20500\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/run1/model-20500\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# To reload from google drive and generate\n",
    "\n",
    "run = \"run1\"\n",
    "gpt2.copy_checkpoint_from_gdrive(run_name=run)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess, run_name=run)\n",
    "\n",
    "gen_file = \"/content/drive/My Drive/CookingWithBert/generated/gpt2_gentext_{:%Y%m%d_%H%M%S}.txt\".format(datetime.utcnow())\n",
    "gpt2.generate_to_file(sess,\n",
    "                      destination_path=gen_file,\n",
    "                      temperature=0.7,\n",
    "                      nsamples=100,\n",
    "                      batch_size=20\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCQy7SGlZyXO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Cooking With BERT",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
