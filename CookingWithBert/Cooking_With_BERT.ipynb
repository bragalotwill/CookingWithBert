{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cooking With BERT",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5AmUAVT3zFV",
        "colab_type": "code",
        "outputId": "62ada614-5912-46ac-98ac-6f5670e001e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# To access datasets stored in personal google drive: grant permissions to access drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJH_BE4zD__m",
        "colab_type": "code",
        "outputId": "46b56ab2-7710-4508-e0f5-62ddf7ca304e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "source": [
        "# In order to open one recipe bearing file from the drive:\n",
        "\n",
        "file = open(\"/content/drive/My Drive/CookingWithBert/recipes/1000.mmf\", 'rb')\n",
        "a_recipe = file.read().decode('ISO-8859-1')\n",
        "\n",
        "print(a_recipe[:500])\n",
        "print(a_recipe[-500:])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MMMMM----- Recipe via Meal-Master (tm) v8.05\r\n",
            " \r\n",
            "      Title: \"BE MINE\" LOLLIPOPS\r\n",
            " Categories: Candies, Valentine\r\n",
            "      Yield: 8 Servings\r\n",
            " \r\n",
            "           Text only\r\n",
            " \r\n",
            "  Source: Better Homes and Gardens, Febuary 1998 Prep time: 20 minutes\r\n",
            "  Cook: 6 to 8 minutes\r\n",
            "  \r\n",
            "  2 1/2 to 3 1/2-inch round or heart-shaped metal cookie cutters 8 oz.\r\n",
            "  assorted red, pink, and/or clearhard candies 35 to 60 (2 to 3 oz.)\r\n",
            "  assorted small decorative candies, such as red cinnamon candies,\r\n",
            "  small nonpareils, c\n",
            "ed until ready to use.\r\n",
            "  \r\n",
            "  Source: Sunset Magazine. November, 1991  \"Vegetables, pilaf, wild\r\n",
            "  rice\".\r\n",
            "  \r\n",
            "  Posted by SANDY BETH - WINONAPL <SANDRA_B@selco.lib.mn.us> to the\r\n",
            "  Fatfree Digest [Volume 15 Issue 15] Feb. 15, 1995.\r\n",
            "  \r\n",
            "  Individual recipes copyrighted by originator. FATFREE Recipe\r\n",
            "  collections copyrighted by Michelle Dick 1995. Formatted by Sue Smith,\r\n",
            "  SueSmith9@aol.com using MMCONV. Archived through kindness of Karen\r\n",
            "  Mintzias, km@salata.com.\r\n",
            "  \r\n",
            "  1.80รก\r\n",
            " \r\n",
            "MMMMM\r\n",
            " \r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0j768CTGfTj",
        "colab_type": "code",
        "outputId": "8d583175-f5a4-48d2-c59a-927c0c7f57bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        }
      },
      "source": [
        "# In order to open all of the recipe bearing files in the drive:\n",
        "\n",
        "import glob\n",
        "\n",
        "filenames = glob.glob(\"/content/drive/My Drive/CookingWithBert/recipes\" + \"/*.mmf\")\n",
        "\n",
        "recipes = \"\"\n",
        "for filename in filenames:\n",
        "    file = open(filename, 'rb')\n",
        "    recipes += file.read().decode('ISO-8859-1')\n",
        "\n",
        "print(recipes[:500])\n",
        "print(recipes[-500:])\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MMMMM----- Recipe via Meal-Master (tm) v8.05\r\n",
            " \r\n",
            "      Title: \"BE MINE\" LOLLIPOPS\r\n",
            " Categories: Candies, Valentine\r\n",
            "      Yield: 8 Servings\r\n",
            " \r\n",
            "           Text only\r\n",
            " \r\n",
            "  Source: Better Homes and Gardens, Febuary 1998 Prep time: 20 minutes\r\n",
            "  Cook: 6 to 8 minutes\r\n",
            "  \r\n",
            "  2 1/2 to 3 1/2-inch round or heart-shaped metal cookie cutters 8 oz.\r\n",
            "  assorted red, pink, and/or clearhard candies 35 to 60 (2 to 3 oz.)\r\n",
            "  assorted small decorative candies, such as red cinnamon candies,\r\n",
            "  small nonpareils, c\n",
            " from the cabbage and discard. Quarter\r\n",
            "  the cabbage; cut out the core. Using a mandoline or a thin sharp\r\n",
            "  knife, slice the cabbage lengthwise into 1/4-inch wide shreds.\u0014\r\n",
            "  In a large bowl, toss the cabbage with the red onion, parsley, and the\r\n",
            "  dressing.  At this point the slaw can be refrigerated for up to 1 hour\r\n",
            "  before serving.  Add the basil, if desired, at the last minute.\u0014\r\n",
            "  Nutritional info per serving: 59 calories; 1.9 G protein; 2.8 G fat;\r\n",
            "  7.4 G carbohydrate.\u0014\r\n",
            " \r\n",
            "MMMMM\r\n",
            " \r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqO9UbANHfp4",
        "colab_type": "code",
        "outputId": "2bf37df2-c7ed-454d-9d6b-a1fa61a73ad6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "RECIPE_SEPERATOR = \"[END]\"\n",
        "\n",
        "lines = recipes.splitlines()\n",
        "for i, line in enumerate(lines):\n",
        "  if line.startswith(\"MMMM\"):\n",
        "    lines[i] = RECIPE_SEPERATOR\n",
        "\n",
        "recipes = \"\\n\".join(lines)\n",
        "print (recipes[:2000])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[END]\n",
            " \n",
            "      Title: \"BE MINE\" LOLLIPOPS\n",
            " Categories: Candies, Valentine\n",
            "      Yield: 8 Servings\n",
            " \n",
            "           Text only\n",
            " \n",
            "  Source: Better Homes and Gardens, Febuary 1998 Prep time: 20 minutes\n",
            "  Cook: 6 to 8 minutes\n",
            "  \n",
            "  2 1/2 to 3 1/2-inch round or heart-shaped metal cookie cutters 8 oz.\n",
            "  assorted red, pink, and/or clearhard candies 35 to 60 (2 to 3 oz.)\n",
            "  assorted small decorative candies, such as red cinnamon candies,\n",
            "  small nonpareils, colored candy hearts, spice drops, and gumdrops\n",
            "  Edible rose petals or other flower petals (optional) Lollipop sticks\n",
            "  \n",
            "  Place unwrapped hard candies in a heavy plastic bag, then place bag\n",
            "  on top of folded towel and crush candies into small chunks wiht meat\n",
            "  mallet or small hammer.\n",
            "  \n",
            "  Make only three or four lollipops at one time.  Line a baking sheet\n",
            "  with foil.  Place desired cookie cutters on foil, at least 2 inches\n",
            "  apart. Divide crushed candies evenly among cutters, approximately 1\n",
            "  1/2 to 2 tablespoons per lollipop.  Candy layer should be 1/4 to 1/2\n",
            "  inch thick. Add small decorative candies or edible rose or other\n",
            "  flower petals to crushed candies.  (If using flower petals, make sure\n",
            "  they are covered with a layer of the crushed candy.)\n",
            "  \n",
            "  Bake in a 350 degree oven for 6 to 8 minutes or till candies are\n",
            "  completely melted.  Col 30 seconds.  Remove cookie cutters with\n",
            "  tongs, allowing melted candy to spread slightly.\n",
            "  \n",
            "  Quickly attach a stick to base of each lollipop, twisting the stick to\n",
            "  cover lollipop end with melted candy.  If desired, press more small\n",
            "  candies or flower petals into hot lolipops.  Cool.  Peel foil from\n",
            "  lollipops. Makes 8\n",
            " \n",
            "[END]\n",
            " \n",
            "[END]\n",
            " \n",
            "      Title: \"BLUE\" FETTUCCINE\n",
            " Categories: Pasta\n",
            "      Yield: 4 Servings\n",
            " \n",
            "      4 oz Danish blue cheese or 8 oz.\n",
            "           Danish blue Castello cheese,\n",
            "           Chilled\n",
            "    1/4 c  Marinated, dried tomatoes\n",
            "      8 oz Green fettuccine or spinach\n",
            "           Egg noodles\n",
            "      2 tb Minced shallots\n",
            "      1    Garlic clove, minced\n",
            "      2 tb Dry\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAdvMwpTSotF",
        "colab_type": "code",
        "outputId": "5954a6df-a3f3-4bee-94c7-bf1cde4e3481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def text_to_char_features(text, sequence_length, step_size):\n",
        "  \"\"\"\n",
        "    Transforms the raw text into training and fitting data.\n",
        "\n",
        "    x: Train data is a vectorized list of sequences pulled from the text.\n",
        "    y: fit data is the character following the corresponding sequence in x\n",
        "    indices_char: dict used to transform back from index to character\n",
        "  \"\"\"\n",
        "  chars = sorted(list(set(text)))\n",
        "  print('total chars:', len(chars))\n",
        "  char_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "  indices_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "  # cut the text in semi-redundant sequences of maxlen characters\n",
        "  maxlen = sequence_length\n",
        "  step = step_size\n",
        "  sequences = []\n",
        "  next_chars = []\n",
        "  for i in range(0, len(text) - maxlen, step):\n",
        "      sequences.append(text[i: i + maxlen])\n",
        "      next_chars.append(text[i + maxlen])\n",
        "  print('nb sequences:', len(sequences))\n",
        "\n",
        "  print('Vectorization...')\n",
        "  # x = np.zeros((len(sequences), maxlen))\n",
        "  x = np.zeros((len(sequences), maxlen, len(chars)))\n",
        "  # y = np.zeros(len(sequences))\n",
        "  y = np.zeros((len(sequences), len(chars)))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    for t, char in enumerate(sequence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "  \n",
        "  return x, y, indices_char\n",
        "\n",
        "def create_char_rnn(sequence_length, vocab_size):\n",
        "  model = Sequential()\n",
        "  # model.add(LSTM(200))\n",
        "  # model.add(Dense(200))\n",
        "  # model.add(Dense(1))\n",
        "  model.add(LSTM(128, input_shape = (sequence_length, vocab_size)))\n",
        "  model.add(Dense(vocab_size, activation = 'softmax'))\n",
        "  return model\n",
        "\n",
        "x, y, idx_to_char = text_to_char_features(a_recipe, 40, 3)\n",
        "\n",
        "vocab_size = len(idx_to_char)\n",
        "sequence_length = 40\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"/content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\",\n",
        " save_weights_only=True, monitor='loss', verbose=1,\n",
        " save_best_only=True, mode='auto', save_freq=1)\n",
        "\n",
        "model = create_char_rnn(sequence_length, vocab_size)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(x, y, epoch=2, callbacks=[checkpoint])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total chars: 109\n",
            "nb sequences: 420894\n",
            "Vectorization...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6cc1bcdea8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_char_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_recipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_to_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-6cc1bcdea8a8>\u001b[0m in \u001b[0;36mtext_to_char_features\u001b[0;34m(text, sequence_length, step_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vectorization...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# x = np.zeros((len(sequences), maxlen))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;31m# y = np.zeros(len(sequences))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyGsZAupcY0k",
        "colab_type": "code",
        "outputId": "fd17fdb9-9c2f-4c71-b6f7-c91a942b44fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# load model from saved weight file\n",
        "# so that training can continue between sessions and testing can happen at will\n",
        "char_rnn = create_char_rnn(sequence_length, vocab_size)\n",
        "char_rnn.load_weights(\"/content/drive/My Drive/CookingWithBert/models/char_rnn_weights.hdf5\")\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.evaluate(x, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13153/13153 [==============================] - 39s 3ms/step - loss: 1.5771 - accuracy: 0.5517\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.5770748853683472, 0.5517327189445496]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS7-2k1nkPMP",
        "colab_type": "code",
        "outputId": "f6cf2f31-026c-496d-936b-bd4381555ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "def text_to_word_features(text, sequence_length, step_size):\n",
        "  text = word_tokenize(text.lower())\n",
        "  words = sorted(list(set(text)))\n",
        "  print('total words:', len(words))\n",
        "  word_indices = dict((c, i) for i, c in enumerate(words))\n",
        "  indices_word = dict((i, c) for i, c in enumerate(words))\n",
        "\n",
        "  # cut the text in semi-redundant sequences of maxlen characters\n",
        "  maxlen = sequence_length\n",
        "  step = step_size\n",
        "  sequences = []\n",
        "  next_words = []\n",
        "  for i in range(0, len(text) - maxlen, step):\n",
        "      sequences.append(text[i: i + maxlen])\n",
        "      # sequences.append([word_indices[word] for word in text[i:i+ maxlen]])\n",
        "      next_words.append(text[i + maxlen])\n",
        "      # next_words.append(word_indices[text[i + maxlen]])\n",
        "  print('nb sequences:', len(sequences))\n",
        "\n",
        "  return sequences, next_words, word_indices\n",
        "\n",
        "def create_word_rnn(sequence_length, vocab_size):\n",
        "  model = Sequential()\n",
        "  # model.add(LSTM(200))\n",
        "  # model.add(Dense(200))\n",
        "  # model.add(Dense(1))\n",
        "  model.add(LSTM(128, input_shape = (sequence_length, vocab_size)))\n",
        "  model.add(Dense(vocab_size, activation = 'softmax'))\n",
        "  return model\n",
        "\n",
        "def generator(sequences, next_words, word_indices, batch_size):\n",
        "  for i in range(len(sequences)):\n",
        "    sequence = sequences[i]\n",
        "    x = np.zeros((batch_size, len(sequence), len(word_indices)))\n",
        "    y = np.zeros((batch_size, len(word_indices)))\n",
        "    for j in range(batch_size):\n",
        "      for t, word in enumerate(sequence):\n",
        "        x[j, t, word_indices[word]] = 1\n",
        "      y[j, word_indices[word]] = 1\n",
        "    if i == len(sequences) - 1:\n",
        "      i = 0\n",
        "    yield x, y\n",
        "      \n",
        "\n",
        "sequences, next_words, word_indices = text_to_word_features(a_recipe, 10, 1)\n",
        "\n",
        "word_checkpoint = ModelCheckpoint(\"/content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\",\n",
        " save_weights_only=True, monitor='loss', verbose=1,\n",
        " save_best_only=True, mode='auto', save_freq=1)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor = 'val_acc', patience = 20)\n",
        "\n",
        "word_vocab_size = len(word_indices)\n",
        "word_sequence_length = 10\n",
        "\n",
        "word_model = create_word_rnn(word_sequence_length, word_vocab_size)\n",
        "word_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "word_model.summary()\n",
        "word_model.fit_generator(generator(sequences, next_words, word_indices, 64), steps_per_epoch = int(len(sequences)/64) + 1, epochs=2, callbacks=[word_checkpoint, early_stopping])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 00001: loss improved from 6.72292 to 6.72130, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2641/3649 [====================>.........] - ETA: 4:41 - loss: 6.7213 - accuracy: 0.1011WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.199376). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.72130 to 6.71961, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2642/3649 [====================>.........] - ETA: 4:41 - loss: 6.7196 - accuracy: 0.1011WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309957). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.71961\n",
            "2643/3649 [====================>.........] - ETA: 4:41 - loss: 6.7213 - accuracy: 0.1010WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309957). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.71961 to 6.71930, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2644/3649 [====================>.........] - ETA: 4:40 - loss: 6.7193 - accuracy: 0.1010WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.211143). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.71930\n",
            "2645/3649 [====================>.........] - ETA: 4:40 - loss: 6.7214 - accuracy: 0.1009\n",
            "Epoch 00001: loss did not improve from 6.71930\n",
            "2646/3649 [====================>.........] - ETA: 4:40 - loss: 6.7201 - accuracy: 0.1009\n",
            "Epoch 00001: loss improved from 6.71930 to 6.71883, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2647/3649 [====================>.........] - ETA: 4:40 - loss: 6.7188 - accuracy: 0.1009\n",
            "Epoch 00001: loss improved from 6.71883 to 6.71820, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2648/3649 [====================>.........] - ETA: 4:40 - loss: 6.7182 - accuracy: 0.1008WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.211143). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.71820 to 6.71783, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2649/3649 [====================>.........] - ETA: 4:39 - loss: 6.7178 - accuracy: 0.1008WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.211143). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.71783 to 6.71583, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2650/3649 [====================>.........] - ETA: 4:39 - loss: 6.7158 - accuracy: 0.1008WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.210988). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.71583 to 6.71539, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2651/3649 [====================>.........] - ETA: 4:39 - loss: 6.7154 - accuracy: 0.1007\n",
            "Epoch 00001: loss improved from 6.71539 to 6.71337, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2652/3649 [====================>.........] - ETA: 4:39 - loss: 6.7134 - accuracy: 0.1011\n",
            "Epoch 00001: loss did not improve from 6.71337\n",
            "2653/3649 [====================>.........] - ETA: 4:38 - loss: 6.7147 - accuracy: 0.1010\n",
            "Epoch 00001: loss improved from 6.71337 to 6.71296, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2654/3649 [====================>.........] - ETA: 4:38 - loss: 6.7130 - accuracy: 0.1010\n",
            "Epoch 00001: loss did not improve from 6.71296\n",
            "2655/3649 [====================>.........] - ETA: 4:38 - loss: 6.7142 - accuracy: 0.1009\n",
            "Epoch 00001: loss improved from 6.71296 to 6.71196, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2656/3649 [====================>.........] - ETA: 4:38 - loss: 6.7120 - accuracy: 0.1013\n",
            "Epoch 00001: loss improved from 6.71196 to 6.71186, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2657/3649 [====================>.........] - ETA: 4:37 - loss: 6.7119 - accuracy: 0.1012\n",
            "Epoch 00001: loss improved from 6.71186 to 6.71171, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2658/3649 [====================>.........] - ETA: 4:37 - loss: 6.7117 - accuracy: 0.1012\n",
            "Epoch 00001: loss improved from 6.71171 to 6.70945, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2659/3649 [====================>.........] - ETA: 4:37 - loss: 6.7095 - accuracy: 0.1015\n",
            "Epoch 00001: loss improved from 6.70945 to 6.70885, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2660/3649 [====================>.........] - ETA: 4:37 - loss: 6.7088 - accuracy: 0.1015\n",
            "Epoch 00001: loss improved from 6.70885 to 6.70717, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2661/3649 [====================>.........] - ETA: 4:37 - loss: 6.7072 - accuracy: 0.1018WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.247777). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.70717 to 6.70664, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2662/3649 [====================>.........] - ETA: 4:36 - loss: 6.7066 - accuracy: 0.1018WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.195992). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.70664 to 6.70600, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2663/3649 [====================>.........] - ETA: 4:36 - loss: 6.7060 - accuracy: 0.1018WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285119). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.70600 to 6.70555, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2664/3649 [====================>.........] - ETA: 4:36 - loss: 6.7056 - accuracy: 0.1017WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302271). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.70555 to 6.70495, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2665/3649 [====================>.........] - ETA: 4:36 - loss: 6.7049 - accuracy: 0.1017WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307311). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.70495 to 6.70371, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2666/3649 [====================>.........] - ETA: 4:36 - loss: 6.7037 - accuracy: 0.1017WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.310160). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.70371 to 6.70271, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2667/3649 [====================>.........] - ETA: 4:35 - loss: 6.7027 - accuracy: 0.1016WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.310160). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.70271\n",
            "2668/3649 [====================>.........] - ETA: 4:35 - loss: 6.7045 - accuracy: 0.1016WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.310160). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.70271\n",
            "2669/3649 [====================>.........] - ETA: 4:35 - loss: 6.7034 - accuracy: 0.1015WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.310160). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.70271\n",
            "2670/3649 [====================>.........] - ETA: 4:35 - loss: 6.7033 - accuracy: 0.1015WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.297174). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.70271\n",
            "2671/3649 [====================>.........] - ETA: 4:34 - loss: 6.7030 - accuracy: 0.1015\n",
            "Epoch 00001: loss improved from 6.70271 to 6.70126, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2672/3649 [====================>.........] - ETA: 4:34 - loss: 6.7013 - accuracy: 0.1014\n",
            "Epoch 00001: loss improved from 6.70126 to 6.70056, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2673/3649 [====================>.........] - ETA: 4:34 - loss: 6.7006 - accuracy: 0.1014\n",
            "Epoch 00001: loss improved from 6.70056 to 6.69884, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2674/3649 [====================>.........] - ETA: 4:34 - loss: 6.6988 - accuracy: 0.1017\n",
            "Epoch 00001: loss did not improve from 6.69884\n",
            "2675/3649 [====================>.........] - ETA: 4:33 - loss: 6.6989 - accuracy: 0.1017\n",
            "Epoch 00001: loss improved from 6.69884 to 6.69652, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2676/3649 [=====================>........] - ETA: 4:33 - loss: 6.6965 - accuracy: 0.1020\n",
            "Epoch 00001: loss did not improve from 6.69652\n",
            "2677/3649 [=====================>........] - ETA: 4:33 - loss: 6.6973 - accuracy: 0.1020\n",
            "Epoch 00001: loss improved from 6.69652 to 6.69638, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2678/3649 [=====================>........] - ETA: 4:32 - loss: 6.6964 - accuracy: 0.1019\n",
            "Epoch 00001: loss did not improve from 6.69638\n",
            "2679/3649 [=====================>........] - ETA: 4:32 - loss: 6.6982 - accuracy: 0.1019\n",
            "Epoch 00001: loss did not improve from 6.69638\n",
            "2680/3649 [=====================>........] - ETA: 4:32 - loss: 6.6965 - accuracy: 0.1019\n",
            "Epoch 00001: loss improved from 6.69638 to 6.69620, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2681/3649 [=====================>........] - ETA: 4:32 - loss: 6.6962 - accuracy: 0.1018\n",
            "Epoch 00001: loss improved from 6.69620 to 6.69540, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2682/3649 [=====================>........] - ETA: 4:31 - loss: 6.6954 - accuracy: 0.1018\n",
            "Epoch 00001: loss improved from 6.69540 to 6.69377, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2683/3649 [=====================>........] - ETA: 4:31 - loss: 6.6938 - accuracy: 0.1021\n",
            "Epoch 00001: loss did not improve from 6.69377\n",
            "2684/3649 [=====================>........] - ETA: 4:31 - loss: 6.6951 - accuracy: 0.1021\n",
            "Epoch 00001: loss improved from 6.69377 to 6.69327, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2685/3649 [=====================>........] - ETA: 4:31 - loss: 6.6933 - accuracy: 0.1020\n",
            "Epoch 00001: loss did not improve from 6.69327\n",
            "2686/3649 [=====================>........] - ETA: 4:30 - loss: 6.6945 - accuracy: 0.1020\n",
            "Epoch 00001: loss improved from 6.69327 to 6.69230, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2687/3649 [=====================>........] - ETA: 4:30 - loss: 6.6923 - accuracy: 0.1023\n",
            "Epoch 00001: loss improved from 6.69230 to 6.69228, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2688/3649 [=====================>........] - ETA: 4:30 - loss: 6.6923 - accuracy: 0.1023\n",
            "Epoch 00001: loss improved from 6.69228 to 6.69126, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2689/3649 [=====================>........] - ETA: 4:30 - loss: 6.6913 - accuracy: 0.1023\n",
            "Epoch 00001: loss did not improve from 6.69126\n",
            "2690/3649 [=====================>........] - ETA: 4:29 - loss: 6.6916 - accuracy: 0.1022\n",
            "Epoch 00001: loss did not improve from 6.69126\n",
            "2691/3649 [=====================>........] - ETA: 4:29 - loss: 6.6922 - accuracy: 0.1022\n",
            "Epoch 00001: loss did not improve from 6.69126\n",
            "2692/3649 [=====================>........] - ETA: 4:29 - loss: 6.6923 - accuracy: 0.1022\n",
            "Epoch 00001: loss improved from 6.69126 to 6.68997, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2693/3649 [=====================>........] - ETA: 4:28 - loss: 6.6900 - accuracy: 0.1025\n",
            "Epoch 00001: loss did not improve from 6.68997\n",
            "2694/3649 [=====================>........] - ETA: 4:28 - loss: 6.6904 - accuracy: 0.1024\n",
            "Epoch 00001: loss improved from 6.68997 to 6.68914, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2695/3649 [=====================>........] - ETA: 4:28 - loss: 6.6891 - accuracy: 0.1024\n",
            "Epoch 00001: loss improved from 6.68914 to 6.68751, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2696/3649 [=====================>........] - ETA: 4:28 - loss: 6.6875 - accuracy: 0.1027\n",
            "Epoch 00001: loss did not improve from 6.68751\n",
            "2697/3649 [=====================>........] - ETA: 4:27 - loss: 6.6887 - accuracy: 0.1027\n",
            "Epoch 00001: loss improved from 6.68751 to 6.68684, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2698/3649 [=====================>........] - ETA: 4:27 - loss: 6.6868 - accuracy: 0.1027\n",
            "Epoch 00001: loss did not improve from 6.68684\n",
            "2699/3649 [=====================>........] - ETA: 4:27 - loss: 6.6880 - accuracy: 0.1026\n",
            "Epoch 00001: loss did not improve from 6.68684\n",
            "2700/3649 [=====================>........] - ETA: 4:26 - loss: 6.6897 - accuracy: 0.1026\n",
            "Epoch 00001: loss did not improve from 6.68684\n",
            "2701/3649 [=====================>........] - ETA: 4:26 - loss: 6.6878 - accuracy: 0.1026\n",
            "Epoch 00001: loss did not improve from 6.68684\n",
            "2702/3649 [=====================>........] - ETA: 4:26 - loss: 6.6870 - accuracy: 0.1029\n",
            "Epoch 00001: loss did not improve from 6.68684\n",
            "2703/3649 [=====================>........] - ETA: 4:26 - loss: 6.6888 - accuracy: 0.1028\n",
            "Epoch 00001: loss did not improve from 6.68684\n",
            "2704/3649 [=====================>........] - ETA: 4:25 - loss: 6.6869 - accuracy: 0.1028\n",
            "Epoch 00001: loss improved from 6.68684 to 6.68582, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2705/3649 [=====================>........] - ETA: 4:25 - loss: 6.6858 - accuracy: 0.1028\n",
            "Epoch 00001: loss did not improve from 6.68582\n",
            "2706/3649 [=====================>........] - ETA: 4:25 - loss: 6.6871 - accuracy: 0.1027\n",
            "Epoch 00001: loss improved from 6.68582 to 6.68533, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2707/3649 [=====================>........] - ETA: 4:24 - loss: 6.6853 - accuracy: 0.1027\n",
            "Epoch 00001: loss improved from 6.68533 to 6.68495, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2708/3649 [=====================>........] - ETA: 4:24 - loss: 6.6850 - accuracy: 0.1027\n",
            "Epoch 00001: loss improved from 6.68495 to 6.68388, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2709/3649 [=====================>........] - ETA: 4:24 - loss: 6.6839 - accuracy: 0.1026\n",
            "Epoch 00001: loss did not improve from 6.68388\n",
            "2710/3649 [=====================>........] - ETA: 4:24 - loss: 6.6839 - accuracy: 0.1026\n",
            "Epoch 00001: loss improved from 6.68388 to 6.68339, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2711/3649 [=====================>........] - ETA: 4:23 - loss: 6.6834 - accuracy: 0.1025\n",
            "Epoch 00001: loss improved from 6.68339 to 6.68288, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2712/3649 [=====================>........] - ETA: 4:23 - loss: 6.6829 - accuracy: 0.1025\n",
            "Epoch 00001: loss improved from 6.68288 to 6.68086, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2713/3649 [=====================>........] - ETA: 4:23 - loss: 6.6809 - accuracy: 0.1025\n",
            "Epoch 00001: loss did not improve from 6.68086\n",
            "2714/3649 [=====================>........] - ETA: 4:23 - loss: 6.6813 - accuracy: 0.1024\n",
            "Epoch 00001: loss improved from 6.68086 to 6.67963, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2715/3649 [=====================>........] - ETA: 4:22 - loss: 6.6796 - accuracy: 0.1028\n",
            "Epoch 00001: loss did not improve from 6.67963\n",
            "2716/3649 [=====================>........] - ETA: 4:22 - loss: 6.6804 - accuracy: 0.1027\n",
            "Epoch 00001: loss did not improve from 6.67963\n",
            "2717/3649 [=====================>........] - ETA: 4:22 - loss: 6.6799 - accuracy: 0.1027\n",
            "Epoch 00001: loss improved from 6.67963 to 6.67882, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2718/3649 [=====================>........] - ETA: 4:22 - loss: 6.6788 - accuracy: 0.1026\n",
            "Epoch 00001: loss did not improve from 6.67882\n",
            "2719/3649 [=====================>........] - ETA: 4:21 - loss: 6.6803 - accuracy: 0.1026\n",
            "Epoch 00001: loss improved from 6.67882 to 6.67841, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2720/3649 [=====================>........] - ETA: 4:21 - loss: 6.6784 - accuracy: 0.1029\n",
            "Epoch 00001: loss improved from 6.67841 to 6.67805, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2721/3649 [=====================>........] - ETA: 4:21 - loss: 6.6780 - accuracy: 0.1029\n",
            "Epoch 00001: loss improved from 6.67805 to 6.67728, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2722/3649 [=====================>........] - ETA: 4:21 - loss: 6.6773 - accuracy: 0.1029\n",
            "Epoch 00001: loss improved from 6.67728 to 6.67622, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2723/3649 [=====================>........] - ETA: 4:20 - loss: 6.6762 - accuracy: 0.1028\n",
            "Epoch 00001: loss improved from 6.67622 to 6.67462, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2724/3649 [=====================>........] - ETA: 4:20 - loss: 6.6746 - accuracy: 0.1032\n",
            "Epoch 00001: loss improved from 6.67462 to 6.67448, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2725/3649 [=====================>........] - ETA: 4:20 - loss: 6.6745 - accuracy: 0.1031WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.240858). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.67448 to 6.67265, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2726/3649 [=====================>........] - ETA: 4:20 - loss: 6.6726 - accuracy: 0.1031WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285411). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.67265\n",
            "2727/3649 [=====================>........] - ETA: 4:19 - loss: 6.6740 - accuracy: 0.1030WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285411). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.67265 to 6.67231, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2728/3649 [=====================>........] - ETA: 4:19 - loss: 6.6723 - accuracy: 0.1034WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285411). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.67231 to 6.67167, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2729/3649 [=====================>........] - ETA: 4:19 - loss: 6.6717 - accuracy: 0.1033WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298900). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.67167 to 6.66954, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2730/3649 [=====================>........] - ETA: 4:19 - loss: 6.6695 - accuracy: 0.1037WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298900). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66954\n",
            "2731/3649 [=====================>........] - ETA: 4:18 - loss: 6.6696 - accuracy: 0.1036WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285411). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66954 to 6.66858, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2732/3649 [=====================>........] - ETA: 4:18 - loss: 6.6686 - accuracy: 0.1036WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271461). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66858 to 6.66846, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2733/3649 [=====================>........] - ETA: 4:18 - loss: 6.6685 - accuracy: 0.1035WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271461). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66846 to 6.66797, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2734/3649 [=====================>........] - ETA: 4:18 - loss: 6.6680 - accuracy: 0.1035WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271461). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66797 to 6.66609, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2735/3649 [=====================>........] - ETA: 4:18 - loss: 6.6661 - accuracy: 0.1035WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271461). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66609\n",
            "2736/3649 [=====================>........] - ETA: 4:17 - loss: 6.6662 - accuracy: 0.1034\n",
            "Epoch 00001: loss did not improve from 6.66609\n",
            "2737/3649 [=====================>........] - ETA: 4:17 - loss: 6.6682 - accuracy: 0.1034\n",
            "Epoch 00001: loss improved from 6.66609 to 6.66596, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2738/3649 [=====================>........] - ETA: 4:17 - loss: 6.6660 - accuracy: 0.1037\n",
            "Epoch 00001: loss did not improve from 6.66596\n",
            "2739/3649 [=====================>........] - ETA: 4:16 - loss: 6.6661 - accuracy: 0.1037\n",
            "Epoch 00001: loss improved from 6.66596 to 6.66489, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2740/3649 [=====================>........] - ETA: 4:16 - loss: 6.6649 - accuracy: 0.1036\n",
            "Epoch 00001: loss improved from 6.66489 to 6.66306, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2741/3649 [=====================>........] - ETA: 4:16 - loss: 6.6631 - accuracy: 0.1040\n",
            "Epoch 00001: loss improved from 6.66306 to 6.66293, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2742/3649 [=====================>........] - ETA: 4:16 - loss: 6.6629 - accuracy: 0.1039WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.236210). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66293 to 6.66262, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2743/3649 [=====================>........] - ETA: 4:16 - loss: 6.6626 - accuracy: 0.1039WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.237229). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66262 to 6.66066, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2744/3649 [=====================>........] - ETA: 4:15 - loss: 6.6607 - accuracy: 0.1039WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.237229). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.66066 to 6.66052, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2745/3649 [=====================>........] - ETA: 4:15 - loss: 6.6605 - accuracy: 0.1038WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.236882). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66052\n",
            "2746/3649 [=====================>........] - ETA: 4:15 - loss: 6.6607 - accuracy: 0.1038WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.236882). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66052\n",
            "2747/3649 [=====================>........] - ETA: 4:15 - loss: 6.6625 - accuracy: 0.1037WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.236882). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66052\n",
            "2748/3649 [=====================>........] - ETA: 4:14 - loss: 6.6642 - accuracy: 0.1037WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.236882). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66052\n",
            "2749/3649 [=====================>........] - ETA: 4:14 - loss: 6.6623 - accuracy: 0.1040WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.236882). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.66052\n",
            "2750/3649 [=====================>........] - ETA: 4:14 - loss: 6.6609 - accuracy: 0.1040\n",
            "Epoch 00001: loss did not improve from 6.66052\n",
            "2751/3649 [=====================>........] - ETA: 4:13 - loss: 6.6606 - accuracy: 0.1040\n",
            "Epoch 00001: loss improved from 6.66052 to 6.66022, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2752/3649 [=====================>........] - ETA: 4:13 - loss: 6.6602 - accuracy: 0.1039\n",
            "Epoch 00001: loss improved from 6.66022 to 6.65808, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2753/3649 [=====================>........] - ETA: 4:13 - loss: 6.6581 - accuracy: 0.1042\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2754/3649 [=====================>........] - ETA: 4:13 - loss: 6.6596 - accuracy: 0.1042\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2755/3649 [=====================>........] - ETA: 4:12 - loss: 6.6614 - accuracy: 0.1042\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2756/3649 [=====================>........] - ETA: 4:12 - loss: 6.6611 - accuracy: 0.1041\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2757/3649 [=====================>........] - ETA: 4:12 - loss: 6.6602 - accuracy: 0.1041\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2758/3649 [=====================>........] - ETA: 4:11 - loss: 6.6615 - accuracy: 0.1041\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2759/3649 [=====================>........] - ETA: 4:11 - loss: 6.6630 - accuracy: 0.1040\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2760/3649 [=====================>........] - ETA: 4:11 - loss: 6.6644 - accuracy: 0.1040\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2761/3649 [=====================>........] - ETA: 4:10 - loss: 6.6657 - accuracy: 0.1039\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2762/3649 [=====================>........] - ETA: 4:10 - loss: 6.6669 - accuracy: 0.1039\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2763/3649 [=====================>........] - ETA: 4:10 - loss: 6.6653 - accuracy: 0.1039\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2764/3649 [=====================>........] - ETA: 4:09 - loss: 6.6666 - accuracy: 0.1038\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2765/3649 [=====================>........] - ETA: 4:09 - loss: 6.6679 - accuracy: 0.1038\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2766/3649 [=====================>........] - ETA: 4:09 - loss: 6.6661 - accuracy: 0.1041\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2767/3649 [=====================>........] - ETA: 4:08 - loss: 6.6674 - accuracy: 0.1041\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2768/3649 [=====================>........] - ETA: 4:08 - loss: 6.6675 - accuracy: 0.1040\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2769/3649 [=====================>........] - ETA: 4:08 - loss: 6.6687 - accuracy: 0.1040\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2770/3649 [=====================>........] - ETA: 4:08 - loss: 6.6698 - accuracy: 0.1040\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2771/3649 [=====================>........] - ETA: 4:07 - loss: 6.6707 - accuracy: 0.1039\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2772/3649 [=====================>........] - ETA: 4:07 - loss: 6.6717 - accuracy: 0.1039\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2773/3649 [=====================>........] - ETA: 4:07 - loss: 6.6705 - accuracy: 0.1042\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2774/3649 [=====================>........] - ETA: 4:06 - loss: 6.6716 - accuracy: 0.1042\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2775/3649 [=====================>........] - ETA: 4:06 - loss: 6.6727 - accuracy: 0.1041\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2776/3649 [=====================>........] - ETA: 4:06 - loss: 6.6708 - accuracy: 0.1045\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2777/3649 [=====================>........] - ETA: 4:05 - loss: 6.6720 - accuracy: 0.1044\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2778/3649 [=====================>........] - ETA: 4:05 - loss: 6.6732 - accuracy: 0.1044\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2779/3649 [=====================>........] - ETA: 4:05 - loss: 6.6735 - accuracy: 0.1044\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2780/3649 [=====================>........] - ETA: 4:04 - loss: 6.6745 - accuracy: 0.1043\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2781/3649 [=====================>........] - ETA: 4:04 - loss: 6.6755 - accuracy: 0.1043\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2782/3649 [=====================>........] - ETA: 4:04 - loss: 6.6735 - accuracy: 0.1046\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2783/3649 [=====================>........] - ETA: 4:04 - loss: 6.6747 - accuracy: 0.1046\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2784/3649 [=====================>........] - ETA: 4:03 - loss: 6.6729 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2785/3649 [=====================>........] - ETA: 4:03 - loss: 6.6741 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2786/3649 [=====================>........] - ETA: 4:03 - loss: 6.6720 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2787/3649 [=====================>........] - ETA: 4:02 - loss: 6.6732 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2788/3649 [=====================>........] - ETA: 4:02 - loss: 6.6715 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2789/3649 [=====================>........] - ETA: 4:02 - loss: 6.6726 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2790/3649 [=====================>........] - ETA: 4:01 - loss: 6.6718 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2791/3649 [=====================>........] - ETA: 4:01 - loss: 6.6728 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2792/3649 [=====================>........] - ETA: 4:01 - loss: 6.6739 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2793/3649 [=====================>........] - ETA: 4:00 - loss: 6.6750 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2794/3649 [=====================>........] - ETA: 4:00 - loss: 6.6731 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2795/3649 [=====================>........] - ETA: 4:00 - loss: 6.6741 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2796/3649 [=====================>........] - ETA: 4:00 - loss: 6.6721 - accuracy: 0.1059\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2797/3649 [=====================>........] - ETA: 3:59 - loss: 6.6716 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2798/3649 [======================>.......] - ETA: 3:59 - loss: 6.6708 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2799/3649 [======================>.......] - ETA: 3:59 - loss: 6.6693 - accuracy: 0.1061\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2800/3649 [======================>.......] - ETA: 3:58 - loss: 6.6679 - accuracy: 0.1064\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2801/3649 [======================>.......] - ETA: 3:58 - loss: 6.6670 - accuracy: 0.1064\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2802/3649 [======================>.......] - ETA: 3:58 - loss: 6.6664 - accuracy: 0.1064\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2803/3649 [======================>.......] - ETA: 3:57 - loss: 6.6659 - accuracy: 0.1063\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2804/3649 [======================>.......] - ETA: 3:57 - loss: 6.6654 - accuracy: 0.1063\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2805/3649 [======================>.......] - ETA: 3:57 - loss: 6.6644 - accuracy: 0.1062\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2806/3649 [======================>.......] - ETA: 3:56 - loss: 6.6638 - accuracy: 0.1062\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2807/3649 [======================>.......] - ETA: 3:56 - loss: 6.6626 - accuracy: 0.1062\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2808/3649 [======================>.......] - ETA: 3:56 - loss: 6.6615 - accuracy: 0.1061\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2809/3649 [======================>.......] - ETA: 3:56 - loss: 6.6605 - accuracy: 0.1061\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2810/3649 [======================>.......] - ETA: 3:55 - loss: 6.6590 - accuracy: 0.1060\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2811/3649 [======================>.......] - ETA: 3:55 - loss: 6.6604 - accuracy: 0.1060\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2812/3649 [======================>.......] - ETA: 3:55 - loss: 6.6618 - accuracy: 0.1060\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2813/3649 [======================>.......] - ETA: 3:54 - loss: 6.6631 - accuracy: 0.1059\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2814/3649 [======================>.......] - ETA: 3:54 - loss: 6.6626 - accuracy: 0.1059\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2815/3649 [======================>.......] - ETA: 3:54 - loss: 6.6638 - accuracy: 0.1059\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2816/3649 [======================>.......] - ETA: 3:53 - loss: 6.6632 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2817/3649 [======================>.......] - ETA: 3:53 - loss: 6.6618 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2818/3649 [======================>.......] - ETA: 3:53 - loss: 6.6630 - accuracy: 0.1057\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2819/3649 [======================>.......] - ETA: 3:52 - loss: 6.6610 - accuracy: 0.1061\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2820/3649 [======================>.......] - ETA: 3:52 - loss: 6.6614 - accuracy: 0.1060\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2821/3649 [======================>.......] - ETA: 3:52 - loss: 6.6608 - accuracy: 0.1060\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2822/3649 [======================>.......] - ETA: 3:52 - loss: 6.6593 - accuracy: 0.1060\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2823/3649 [======================>.......] - ETA: 3:51 - loss: 6.6590 - accuracy: 0.1059\n",
            "Epoch 00001: loss did not improve from 6.65808\n",
            "2824/3649 [======================>.......] - ETA: 3:51 - loss: 6.6584 - accuracy: 0.1059\n",
            "Epoch 00001: loss improved from 6.65808 to 6.65787, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2825/3649 [======================>.......] - ETA: 3:51 - loss: 6.6579 - accuracy: 0.1058\n",
            "Epoch 00001: loss improved from 6.65787 to 6.65703, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2826/3649 [======================>.......] - ETA: 3:51 - loss: 6.6570 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2827/3649 [======================>.......] - ETA: 3:50 - loss: 6.6574 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2828/3649 [======================>.......] - ETA: 3:50 - loss: 6.6590 - accuracy: 0.1057\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2829/3649 [======================>.......] - ETA: 3:50 - loss: 6.6593 - accuracy: 0.1057\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2830/3649 [======================>.......] - ETA: 3:49 - loss: 6.6595 - accuracy: 0.1057\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2831/3649 [======================>.......] - ETA: 3:49 - loss: 6.6609 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2832/3649 [======================>.......] - ETA: 3:49 - loss: 6.6603 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2833/3649 [======================>.......] - ETA: 3:48 - loss: 6.6615 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2834/3649 [======================>.......] - ETA: 3:48 - loss: 6.6604 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2835/3649 [======================>.......] - ETA: 3:48 - loss: 6.6610 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2836/3649 [======================>.......] - ETA: 3:47 - loss: 6.6618 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2837/3649 [======================>.......] - ETA: 3:47 - loss: 6.6619 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2838/3649 [======================>.......] - ETA: 3:47 - loss: 6.6608 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2839/3649 [======================>.......] - ETA: 3:47 - loss: 6.6604 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2840/3649 [======================>.......] - ETA: 3:46 - loss: 6.6618 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2841/3649 [======================>.......] - ETA: 3:46 - loss: 6.6616 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2842/3649 [======================>.......] - ETA: 3:46 - loss: 6.6615 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2843/3649 [======================>.......] - ETA: 3:45 - loss: 6.6604 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2844/3649 [======================>.......] - ETA: 3:45 - loss: 6.6594 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2845/3649 [======================>.......] - ETA: 3:45 - loss: 6.6593 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2846/3649 [======================>.......] - ETA: 3:44 - loss: 6.6607 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2847/3649 [======================>.......] - ETA: 3:44 - loss: 6.6607 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2848/3649 [======================>.......] - ETA: 3:44 - loss: 6.6596 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2849/3649 [======================>.......] - ETA: 3:44 - loss: 6.6591 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2850/3649 [======================>.......] - ETA: 3:43 - loss: 6.6603 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2851/3649 [======================>.......] - ETA: 3:43 - loss: 6.6603 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2852/3649 [======================>.......] - ETA: 3:43 - loss: 6.6596 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2853/3649 [======================>.......] - ETA: 3:42 - loss: 6.6588 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2854/3649 [======================>.......] - ETA: 3:42 - loss: 6.6605 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2855/3649 [======================>.......] - ETA: 3:42 - loss: 6.6623 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2856/3649 [======================>.......] - ETA: 3:41 - loss: 6.6608 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2857/3649 [======================>.......] - ETA: 3:41 - loss: 6.6606 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2858/3649 [======================>.......] - ETA: 3:41 - loss: 6.6606 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2859/3649 [======================>.......] - ETA: 3:40 - loss: 6.6602 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2860/3649 [======================>.......] - ETA: 3:40 - loss: 6.6584 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2861/3649 [======================>.......] - ETA: 3:40 - loss: 6.6598 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2862/3649 [======================>.......] - ETA: 3:40 - loss: 6.6596 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2863/3649 [======================>.......] - ETA: 3:39 - loss: 6.6610 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2864/3649 [======================>.......] - ETA: 3:39 - loss: 6.6602 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2865/3649 [======================>.......] - ETA: 3:39 - loss: 6.6608 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2866/3649 [======================>.......] - ETA: 3:38 - loss: 6.6624 - accuracy: 0.1054\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2867/3649 [======================>.......] - ETA: 3:38 - loss: 6.6624 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2868/3649 [======================>.......] - ETA: 3:38 - loss: 6.6614 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2869/3649 [======================>.......] - ETA: 3:37 - loss: 6.6629 - accuracy: 0.1053\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2870/3649 [======================>.......] - ETA: 3:37 - loss: 6.6623 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2871/3649 [======================>.......] - ETA: 3:37 - loss: 6.6628 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2872/3649 [======================>.......] - ETA: 3:37 - loss: 6.6626 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2873/3649 [======================>.......] - ETA: 3:36 - loss: 6.6624 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2874/3649 [======================>.......] - ETA: 3:36 - loss: 6.6617 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2875/3649 [======================>.......] - ETA: 3:36 - loss: 6.6618 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2876/3649 [======================>.......] - ETA: 3:35 - loss: 6.6610 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2877/3649 [======================>.......] - ETA: 3:35 - loss: 6.6623 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2878/3649 [======================>.......] - ETA: 3:35 - loss: 6.6620 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2879/3649 [======================>.......] - ETA: 3:34 - loss: 6.6634 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2880/3649 [======================>.......] - ETA: 3:34 - loss: 6.6625 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2881/3649 [======================>.......] - ETA: 3:34 - loss: 6.6613 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2882/3649 [======================>.......] - ETA: 3:34 - loss: 6.6604 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2883/3649 [======================>.......] - ETA: 3:33 - loss: 6.6594 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2884/3649 [======================>.......] - ETA: 3:33 - loss: 6.6607 - accuracy: 0.1047\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2885/3649 [======================>.......] - ETA: 3:33 - loss: 6.6602 - accuracy: 0.1047\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2886/3649 [======================>.......] - ETA: 3:32 - loss: 6.6612 - accuracy: 0.1046\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2887/3649 [======================>.......] - ETA: 3:32 - loss: 6.6593 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2888/3649 [======================>.......] - ETA: 3:32 - loss: 6.6605 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2889/3649 [======================>.......] - ETA: 3:31 - loss: 6.6592 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2890/3649 [======================>.......] - ETA: 3:31 - loss: 6.6581 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2891/3649 [======================>.......] - ETA: 3:31 - loss: 6.6595 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2892/3649 [======================>.......] - ETA: 3:31 - loss: 6.6576 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2893/3649 [======================>.......] - ETA: 3:30 - loss: 6.6587 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2894/3649 [======================>.......] - ETA: 3:30 - loss: 6.6587 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2895/3649 [======================>.......] - ETA: 3:30 - loss: 6.6587 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2896/3649 [======================>.......] - ETA: 3:29 - loss: 6.6577 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2897/3649 [======================>.......] - ETA: 3:29 - loss: 6.6589 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2898/3649 [======================>.......] - ETA: 3:29 - loss: 6.6588 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2899/3649 [======================>.......] - ETA: 3:28 - loss: 6.6577 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2900/3649 [======================>.......] - ETA: 3:28 - loss: 6.6579 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.65703\n",
            "2901/3649 [======================>.......] - ETA: 3:28 - loss: 6.6587 - accuracy: 0.1048\n",
            "Epoch 00001: loss improved from 6.65703 to 6.65668, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2902/3649 [======================>.......] - ETA: 3:28 - loss: 6.6567 - accuracy: 0.1051\n",
            "Epoch 00001: loss improved from 6.65668 to 6.65661, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2903/3649 [======================>.......] - ETA: 3:27 - loss: 6.6566 - accuracy: 0.1051\n",
            "Epoch 00001: loss did not improve from 6.65661\n",
            "2904/3649 [======================>.......] - ETA: 3:27 - loss: 6.6577 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65661\n",
            "2905/3649 [======================>.......] - ETA: 3:27 - loss: 6.6568 - accuracy: 0.1050\n",
            "Epoch 00001: loss improved from 6.65661 to 6.65601, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2906/3649 [======================>.......] - ETA: 3:27 - loss: 6.6560 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.65601\n",
            "2907/3649 [======================>.......] - ETA: 3:26 - loss: 6.6573 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65601\n",
            "2908/3649 [======================>.......] - ETA: 3:26 - loss: 6.6571 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65601\n",
            "2909/3649 [======================>.......] - ETA: 3:26 - loss: 6.6564 - accuracy: 0.1048\n",
            "Epoch 00001: loss improved from 6.65601 to 6.65564, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2910/3649 [======================>.......] - ETA: 3:25 - loss: 6.6556 - accuracy: 0.1048\n",
            "Epoch 00001: loss improved from 6.65564 to 6.65545, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2911/3649 [======================>.......] - ETA: 3:25 - loss: 6.6555 - accuracy: 0.1048\n",
            "Epoch 00001: loss improved from 6.65545 to 6.65543, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2912/3649 [======================>.......] - ETA: 3:25 - loss: 6.6554 - accuracy: 0.1047\n",
            "Epoch 00001: loss improved from 6.65543 to 6.65338, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2913/3649 [======================>.......] - ETA: 3:25 - loss: 6.6534 - accuracy: 0.1050\n",
            "Epoch 00001: loss improved from 6.65338 to 6.65323, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2914/3649 [======================>.......] - ETA: 3:24 - loss: 6.6532 - accuracy: 0.1050\n",
            "Epoch 00001: loss improved from 6.65323 to 6.65278, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2915/3649 [======================>.......] - ETA: 3:24 - loss: 6.6528 - accuracy: 0.1050WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.191240). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.65278\n",
            "2916/3649 [======================>.......] - ETA: 3:24 - loss: 6.6528 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.65278\n",
            "2917/3649 [======================>.......] - ETA: 3:24 - loss: 6.6530 - accuracy: 0.1049WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188320). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.65278 to 6.65242, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2918/3649 [======================>.......] - ETA: 3:23 - loss: 6.6524 - accuracy: 0.1049WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.190240). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.65242 to 6.65137, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2919/3649 [======================>.......] - ETA: 3:23 - loss: 6.6514 - accuracy: 0.1048WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302761). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.65137 to 6.65103, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2920/3649 [=======================>......] - ETA: 3:23 - loss: 6.6510 - accuracy: 0.1048WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.313228). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.65103\n",
            "2921/3649 [=======================>......] - ETA: 3:23 - loss: 6.6516 - accuracy: 0.1048WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302761). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.65103\n",
            "2922/3649 [=======================>......] - ETA: 3:22 - loss: 6.6513 - accuracy: 0.1047WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.190240). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.65103 to 6.64928, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2923/3649 [=======================>......] - ETA: 3:22 - loss: 6.6493 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.64928\n",
            "2924/3649 [=======================>......] - ETA: 3:22 - loss: 6.6505 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.64928\n",
            "2925/3649 [=======================>......] - ETA: 3:22 - loss: 6.6520 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.64928\n",
            "2926/3649 [=======================>......] - ETA: 3:21 - loss: 6.6517 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.64928\n",
            "2927/3649 [=======================>......] - ETA: 3:21 - loss: 6.6496 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.64928\n",
            "2928/3649 [=======================>......] - ETA: 3:21 - loss: 6.6495 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.64928\n",
            "2929/3649 [=======================>......] - ETA: 3:20 - loss: 6.6493 - accuracy: 0.1052\n",
            "Epoch 00001: loss improved from 6.64928 to 6.64789, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2930/3649 [=======================>......] - ETA: 3:20 - loss: 6.6479 - accuracy: 0.1051\n",
            "Epoch 00001: loss improved from 6.64789 to 6.64764, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2931/3649 [=======================>......] - ETA: 3:20 - loss: 6.6476 - accuracy: 0.1051\n",
            "Epoch 00001: loss improved from 6.64764 to 6.64672, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2932/3649 [=======================>......] - ETA: 3:20 - loss: 6.6467 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.64672\n",
            "2933/3649 [=======================>......] - ETA: 3:19 - loss: 6.6467 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.64672\n",
            "2934/3649 [=======================>......] - ETA: 3:19 - loss: 6.6478 - accuracy: 0.1050\n",
            "Epoch 00001: loss improved from 6.64672 to 6.64572, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2935/3649 [=======================>......] - ETA: 3:19 - loss: 6.6457 - accuracy: 0.1053\n",
            "Epoch 00001: loss improved from 6.64572 to 6.64572, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2936/3649 [=======================>......] - ETA: 3:19 - loss: 6.6457 - accuracy: 0.1052\n",
            "Epoch 00001: loss improved from 6.64572 to 6.64481, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2937/3649 [=======================>......] - ETA: 3:18 - loss: 6.6448 - accuracy: 0.1052\n",
            "Epoch 00001: loss improved from 6.64481 to 6.64456, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2938/3649 [=======================>......] - ETA: 3:18 - loss: 6.6446 - accuracy: 0.1052WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192054). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.64456 to 6.64279, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2939/3649 [=======================>......] - ETA: 3:18 - loss: 6.6428 - accuracy: 0.1051WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306437). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64279\n",
            "2940/3649 [=======================>......] - ETA: 3:18 - loss: 6.6435 - accuracy: 0.1051WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306437). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64279\n",
            "2941/3649 [=======================>......] - ETA: 3:17 - loss: 6.6431 - accuracy: 0.1051\n",
            "Epoch 00001: loss improved from 6.64279 to 6.64123, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2942/3649 [=======================>......] - ETA: 3:17 - loss: 6.6412 - accuracy: 0.1050\n",
            "Epoch 00001: loss improved from 6.64123 to 6.64016, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2943/3649 [=======================>......] - ETA: 3:17 - loss: 6.6402 - accuracy: 0.1050WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188299). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64016\n",
            "2944/3649 [=======================>......] - ETA: 3:17 - loss: 6.6410 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.64016\n",
            "2945/3649 [=======================>......] - ETA: 3:16 - loss: 6.6420 - accuracy: 0.1049WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188299). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.64016\n",
            "2946/3649 [=======================>......] - ETA: 3:16 - loss: 6.6405 - accuracy: 0.1049\n",
            "Epoch 00001: loss did not improve from 6.64016\n",
            "2947/3649 [=======================>......] - ETA: 3:16 - loss: 6.6402 - accuracy: 0.1049\n",
            "Epoch 00001: loss improved from 6.64016 to 6.63971, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2948/3649 [=======================>......] - ETA: 3:15 - loss: 6.6397 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.63971\n",
            "2949/3649 [=======================>......] - ETA: 3:15 - loss: 6.6399 - accuracy: 0.1048\n",
            "Epoch 00001: loss did not improve from 6.63971\n",
            "2950/3649 [=======================>......] - ETA: 3:15 - loss: 6.6400 - accuracy: 0.1047\n",
            "Epoch 00001: loss improved from 6.63971 to 6.63812, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2951/3649 [=======================>......] - ETA: 3:14 - loss: 6.6381 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.63812\n",
            "2952/3649 [=======================>......] - ETA: 3:14 - loss: 6.6383 - accuracy: 0.1050\n",
            "Epoch 00001: loss did not improve from 6.63812\n",
            "2953/3649 [=======================>......] - ETA: 3:14 - loss: 6.6398 - accuracy: 0.1050\n",
            "Epoch 00001: loss improved from 6.63812 to 6.63781, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2954/3649 [=======================>......] - ETA: 3:14 - loss: 6.6378 - accuracy: 0.1053\n",
            "Epoch 00001: loss improved from 6.63781 to 6.63765, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2955/3649 [=======================>......] - ETA: 3:13 - loss: 6.6376 - accuracy: 0.1052\n",
            "Epoch 00001: loss improved from 6.63765 to 6.63665, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2956/3649 [=======================>......] - ETA: 3:13 - loss: 6.6367 - accuracy: 0.1052\n",
            "Epoch 00001: loss did not improve from 6.63665\n",
            "2957/3649 [=======================>......] - ETA: 3:13 - loss: 6.6383 - accuracy: 0.1052\n",
            "Epoch 00001: loss improved from 6.63665 to 6.63640, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2958/3649 [=======================>......] - ETA: 3:13 - loss: 6.6364 - accuracy: 0.1055\n",
            "Epoch 00001: loss improved from 6.63640 to 6.63609, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2959/3649 [=======================>......] - ETA: 3:12 - loss: 6.6361 - accuracy: 0.1054\n",
            "Epoch 00001: loss improved from 6.63609 to 6.63418, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2960/3649 [=======================>......] - ETA: 3:12 - loss: 6.6342 - accuracy: 0.1057\n",
            "Epoch 00001: loss improved from 6.63418 to 6.63389, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2961/3649 [=======================>......] - ETA: 3:12 - loss: 6.6339 - accuracy: 0.1057WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.202607). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.63389 to 6.63237, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2962/3649 [=======================>......] - ETA: 3:12 - loss: 6.6324 - accuracy: 0.1057WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303298). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.63237 to 6.63129, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2963/3649 [=======================>......] - ETA: 3:12 - loss: 6.6313 - accuracy: 0.1060WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.308411). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.63129\n",
            "2964/3649 [=======================>......] - ETA: 3:11 - loss: 6.6316 - accuracy: 0.1059WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.308411). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.63129 to 6.63109, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2965/3649 [=======================>......] - ETA: 3:11 - loss: 6.6311 - accuracy: 0.1059WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303298). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.63109 to 6.63001, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2966/3649 [=======================>......] - ETA: 3:11 - loss: 6.6300 - accuracy: 0.1059WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303298). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.63001\n",
            "2967/3649 [=======================>......] - ETA: 3:11 - loss: 6.6314 - accuracy: 0.1058WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303298). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.63001 to 6.62955, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2968/3649 [=======================>......] - ETA: 3:10 - loss: 6.6296 - accuracy: 0.1058WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303298). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.62955 to 6.62941, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2969/3649 [=======================>......] - ETA: 3:10 - loss: 6.6294 - accuracy: 0.1058WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285776). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.62941 to 6.62789, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2970/3649 [=======================>......] - ETA: 3:10 - loss: 6.6279 - accuracy: 0.1057WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.285776). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.62789\n",
            "2971/3649 [=======================>......] - ETA: 3:09 - loss: 6.6283 - accuracy: 0.1057\n",
            "Epoch 00001: loss improved from 6.62789 to 6.62718, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2972/3649 [=======================>......] - ETA: 3:09 - loss: 6.6272 - accuracy: 0.1057\n",
            "Epoch 00001: loss improved from 6.62718 to 6.62627, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2973/3649 [=======================>......] - ETA: 3:09 - loss: 6.6263 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.62627\n",
            "2974/3649 [=======================>......] - ETA: 3:09 - loss: 6.6265 - accuracy: 0.1056\n",
            "Epoch 00001: loss improved from 6.62627 to 6.62615, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2975/3649 [=======================>......] - ETA: 3:08 - loss: 6.6262 - accuracy: 0.1055\n",
            "Epoch 00001: loss did not improve from 6.62615\n",
            "2976/3649 [=======================>......] - ETA: 3:08 - loss: 6.6262 - accuracy: 0.1055\n",
            "Epoch 00001: loss improved from 6.62615 to 6.62570, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2977/3649 [=======================>......] - ETA: 3:08 - loss: 6.6257 - accuracy: 0.1055\n",
            "Epoch 00001: loss improved from 6.62570 to 6.62366, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2978/3649 [=======================>......] - ETA: 3:08 - loss: 6.6237 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.62366\n",
            "2979/3649 [=======================>......] - ETA: 3:07 - loss: 6.6252 - accuracy: 0.1057\n",
            "Epoch 00001: loss did not improve from 6.62366\n",
            "2980/3649 [=======================>......] - ETA: 3:07 - loss: 6.6244 - accuracy: 0.1057\n",
            "Epoch 00001: loss improved from 6.62366 to 6.62325, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2981/3649 [=======================>......] - ETA: 3:07 - loss: 6.6232 - accuracy: 0.1057\n",
            "Epoch 00001: loss did not improve from 6.62325\n",
            "2982/3649 [=======================>......] - ETA: 3:07 - loss: 6.6233 - accuracy: 0.1056\n",
            "Epoch 00001: loss improved from 6.62325 to 6.62173, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2983/3649 [=======================>......] - ETA: 3:06 - loss: 6.6217 - accuracy: 0.1056\n",
            "Epoch 00001: loss improved from 6.62173 to 6.62122, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2984/3649 [=======================>......] - ETA: 3:06 - loss: 6.6212 - accuracy: 0.1056\n",
            "Epoch 00001: loss did not improve from 6.62122\n",
            "2985/3649 [=======================>......] - ETA: 3:06 - loss: 6.6214 - accuracy: 0.1055\n",
            "Epoch 00001: loss improved from 6.62122 to 6.61951, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2986/3649 [=======================>......] - ETA: 3:05 - loss: 6.6195 - accuracy: 0.1058\n",
            "Epoch 00001: loss did not improve from 6.61951\n",
            "2987/3649 [=======================>......] - ETA: 3:05 - loss: 6.6197 - accuracy: 0.1058\n",
            "Epoch 00001: loss improved from 6.61951 to 6.61784, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2988/3649 [=======================>......] - ETA: 3:05 - loss: 6.6178 - accuracy: 0.1061\n",
            "Epoch 00001: loss improved from 6.61784 to 6.61735, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2989/3649 [=======================>......] - ETA: 3:05 - loss: 6.6174 - accuracy: 0.1061\n",
            "Epoch 00001: loss improved from 6.61735 to 6.61695, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2990/3649 [=======================>......] - ETA: 3:04 - loss: 6.6169 - accuracy: 0.1060\n",
            "Epoch 00001: loss improved from 6.61695 to 6.61630, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2991/3649 [=======================>......] - ETA: 3:04 - loss: 6.6163 - accuracy: 0.1060\n",
            "Epoch 00001: loss improved from 6.61630 to 6.61435, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2992/3649 [=======================>......] - ETA: 3:04 - loss: 6.6144 - accuracy: 0.1063WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.237865). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.61435 to 6.61389, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2993/3649 [=======================>......] - ETA: 3:04 - loss: 6.6139 - accuracy: 0.1062WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.304598). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.61389\n",
            "2994/3649 [=======================>......] - ETA: 3:04 - loss: 6.6158 - accuracy: 0.1062WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.210905). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.61389 to 6.61385, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2995/3649 [=======================>......] - ETA: 3:03 - loss: 6.6138 - accuracy: 0.1065WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.210905). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.61385\n",
            "2996/3649 [=======================>......] - ETA: 3:03 - loss: 6.6140 - accuracy: 0.1065WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.210905). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.61385 to 6.61242, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "2997/3649 [=======================>......] - ETA: 3:03 - loss: 6.6124 - accuracy: 0.1068WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.210905). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.61242\n",
            "2998/3649 [=======================>......] - ETA: 3:02 - loss: 6.6134 - accuracy: 0.1067WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.206453). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.61242\n",
            "2999/3649 [=======================>......] - ETA: 3:02 - loss: 6.6131 - accuracy: 0.1067\n",
            "Epoch 00001: loss did not improve from 6.61242\n",
            "3000/3649 [=======================>......] - ETA: 3:02 - loss: 6.6131 - accuracy: 0.1067\n",
            "Epoch 00001: loss improved from 6.61242 to 6.61112, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3001/3649 [=======================>......] - ETA: 3:01 - loss: 6.6111 - accuracy: 0.1070\n",
            "Epoch 00001: loss improved from 6.61112 to 6.61110, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3002/3649 [=======================>......] - ETA: 3:01 - loss: 6.6111 - accuracy: 0.1069\n",
            "Epoch 00001: loss improved from 6.61110 to 6.60955, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3003/3649 [=======================>......] - ETA: 3:01 - loss: 6.6096 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.60955\n",
            "3004/3649 [=======================>......] - ETA: 3:01 - loss: 6.6096 - accuracy: 0.1072\n",
            "Epoch 00001: loss improved from 6.60955 to 6.60850, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3005/3649 [=======================>......] - ETA: 3:00 - loss: 6.6085 - accuracy: 0.1072\n",
            "Epoch 00001: loss improved from 6.60850 to 6.60793, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3006/3649 [=======================>......] - ETA: 3:00 - loss: 6.6079 - accuracy: 0.1071\n",
            "Epoch 00001: loss did not improve from 6.60793\n",
            "3007/3649 [=======================>......] - ETA: 3:00 - loss: 6.6081 - accuracy: 0.1071\n",
            "Epoch 00001: loss did not improve from 6.60793\n",
            "3008/3649 [=======================>......] - ETA: 3:00 - loss: 6.6093 - accuracy: 0.1070\n",
            "Epoch 00001: loss did not improve from 6.60793\n",
            "3009/3649 [=======================>......] - ETA: 2:59 - loss: 6.6082 - accuracy: 0.1070\n",
            "Epoch 00001: loss improved from 6.60793 to 6.60741, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3010/3649 [=======================>......] - ETA: 2:59 - loss: 6.6074 - accuracy: 0.1070\n",
            "Epoch 00001: loss did not improve from 6.60741\n",
            "3011/3649 [=======================>......] - ETA: 2:59 - loss: 6.6075 - accuracy: 0.1069\n",
            "Epoch 00001: loss improved from 6.60741 to 6.60633, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3012/3649 [=======================>......] - ETA: 2:58 - loss: 6.6063 - accuracy: 0.1069\n",
            "Epoch 00001: loss improved from 6.60633 to 6.60535, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3013/3649 [=======================>......] - ETA: 2:58 - loss: 6.6053 - accuracy: 0.1069\n",
            "Epoch 00001: loss did not improve from 6.60535\n",
            "3014/3649 [=======================>......] - ETA: 2:58 - loss: 6.6054 - accuracy: 0.1068\n",
            "Epoch 00001: loss did not improve from 6.60535\n",
            "3015/3649 [=======================>......] - ETA: 2:58 - loss: 6.6058 - accuracy: 0.1068\n",
            "Epoch 00001: loss improved from 6.60535 to 6.60387, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3016/3649 [=======================>......] - ETA: 2:57 - loss: 6.6039 - accuracy: 0.1071\n",
            "Epoch 00001: loss improved from 6.60387 to 6.60352, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3017/3649 [=======================>......] - ETA: 2:57 - loss: 6.6035 - accuracy: 0.1071\n",
            "Epoch 00001: loss improved from 6.60352 to 6.60229, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3018/3649 [=======================>......] - ETA: 2:57 - loss: 6.6023 - accuracy: 0.1070\n",
            "Epoch 00001: loss did not improve from 6.60229\n",
            "3019/3649 [=======================>......] - ETA: 2:57 - loss: 6.6037 - accuracy: 0.1070\n",
            "Epoch 00001: loss improved from 6.60229 to 6.60207, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3020/3649 [=======================>......] - ETA: 2:56 - loss: 6.6021 - accuracy: 0.1070\n",
            "Epoch 00001: loss improved from 6.60207 to 6.60189, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3021/3649 [=======================>......] - ETA: 2:56 - loss: 6.6019 - accuracy: 0.1069WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.245091). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.60189 to 6.60147, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3022/3649 [=======================>......] - ETA: 2:56 - loss: 6.6015 - accuracy: 0.1069WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306347). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.60147 to 6.60052, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3023/3649 [=======================>......] - ETA: 2:56 - loss: 6.6005 - accuracy: 0.1068WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306573). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.60052 to 6.59863, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3024/3649 [=======================>......] - ETA: 2:56 - loss: 6.5986 - accuracy: 0.1071WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306573). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59863 to 6.59832, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3025/3649 [=======================>......] - ETA: 2:55 - loss: 6.5983 - accuracy: 0.1071WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.313682). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59832 to 6.59674, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3026/3649 [=======================>......] - ETA: 2:55 - loss: 6.5967 - accuracy: 0.1071WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.313682). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59674 to 6.59650, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3027/3649 [=======================>......] - ETA: 2:55 - loss: 6.5965 - accuracy: 0.1070WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306573). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.59650\n",
            "3028/3649 [=======================>......] - ETA: 2:55 - loss: 6.5965 - accuracy: 0.1070WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.299788). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59650 to 6.59583, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3029/3649 [=======================>......] - ETA: 2:54 - loss: 6.5958 - accuracy: 0.1070WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.299788). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59583 to 6.59558, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3030/3649 [=======================>......] - ETA: 2:54 - loss: 6.5956 - accuracy: 0.1069WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303657). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59558 to 6.59505, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3031/3649 [=======================>......] - ETA: 2:54 - loss: 6.5951 - accuracy: 0.1069WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303657). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59505 to 6.59302, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3032/3649 [=======================>......] - ETA: 2:54 - loss: 6.5930 - accuracy: 0.1072WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.304128). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59302 to 6.59243, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3033/3649 [=======================>......] - ETA: 2:53 - loss: 6.5924 - accuracy: 0.1072WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298448). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.59243 to 6.59181, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3034/3649 [=======================>......] - ETA: 2:53 - loss: 6.5918 - accuracy: 0.1071WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303802). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3035/3649 [=======================>......] - ETA: 2:53 - loss: 6.5928 - accuracy: 0.1071WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298448). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3036/3649 [=======================>......] - ETA: 2:52 - loss: 6.5922 - accuracy: 0.1070WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.282780). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3037/3649 [=======================>......] - ETA: 2:52 - loss: 6.5942 - accuracy: 0.1070WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.207133). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3038/3649 [=======================>......] - ETA: 2:52 - loss: 6.5921 - accuracy: 0.1073WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.207133). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3039/3649 [=======================>......] - ETA: 2:52 - loss: 6.5939 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3040/3649 [=======================>......] - ETA: 2:51 - loss: 6.5934 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3041/3649 [========================>.....] - ETA: 2:51 - loss: 6.5923 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3042/3649 [========================>.....] - ETA: 2:51 - loss: 6.5939 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.59181\n",
            "3043/3649 [========================>.....] - ETA: 2:50 - loss: 6.5919 - accuracy: 0.1075\n",
            "Epoch 00001: loss improved from 6.59181 to 6.59139, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3044/3649 [========================>.....] - ETA: 2:50 - loss: 6.5914 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.59139\n",
            "3045/3649 [========================>.....] - ETA: 2:50 - loss: 6.5920 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.59139\n",
            "3046/3649 [========================>.....] - ETA: 2:50 - loss: 6.5925 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.59139\n",
            "3047/3649 [========================>.....] - ETA: 2:49 - loss: 6.5919 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.59139\n",
            "3048/3649 [========================>.....] - ETA: 2:49 - loss: 6.5925 - accuracy: 0.1073\n",
            "Epoch 00001: loss improved from 6.59139 to 6.59129, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3049/3649 [========================>.....] - ETA: 2:49 - loss: 6.5913 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.59129\n",
            "3050/3649 [========================>.....] - ETA: 2:48 - loss: 6.5915 - accuracy: 0.1072\n",
            "Epoch 00001: loss improved from 6.59129 to 6.59077, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3051/3649 [========================>.....] - ETA: 2:48 - loss: 6.5908 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.59077\n",
            "3052/3649 [========================>.....] - ETA: 2:48 - loss: 6.5916 - accuracy: 0.1071\n",
            "Epoch 00001: loss improved from 6.59077 to 6.58956, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3053/3649 [========================>.....] - ETA: 2:47 - loss: 6.5896 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.58956\n",
            "3054/3649 [========================>.....] - ETA: 2:47 - loss: 6.5911 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.58956\n",
            "3055/3649 [========================>.....] - ETA: 2:47 - loss: 6.5903 - accuracy: 0.1074\n",
            "Epoch 00001: loss improved from 6.58956 to 6.58885, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3056/3649 [========================>.....] - ETA: 2:47 - loss: 6.5889 - accuracy: 0.1077\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3057/3649 [========================>.....] - ETA: 2:46 - loss: 6.5902 - accuracy: 0.1076\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3058/3649 [========================>.....] - ETA: 2:46 - loss: 6.5916 - accuracy: 0.1076\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3059/3649 [========================>.....] - ETA: 2:46 - loss: 6.5918 - accuracy: 0.1076\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3060/3649 [========================>.....] - ETA: 2:45 - loss: 6.5930 - accuracy: 0.1075\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3061/3649 [========================>.....] - ETA: 2:45 - loss: 6.5929 - accuracy: 0.1075\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3062/3649 [========================>.....] - ETA: 2:45 - loss: 6.5940 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3063/3649 [========================>.....] - ETA: 2:45 - loss: 6.5952 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3064/3649 [========================>.....] - ETA: 2:44 - loss: 6.5943 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3065/3649 [========================>.....] - ETA: 2:44 - loss: 6.5942 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3066/3649 [========================>.....] - ETA: 2:44 - loss: 6.5944 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3067/3649 [========================>.....] - ETA: 2:43 - loss: 6.5939 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3068/3649 [========================>.....] - ETA: 2:43 - loss: 6.5938 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3069/3649 [========================>.....] - ETA: 2:43 - loss: 6.5950 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3070/3649 [========================>.....] - ETA: 2:42 - loss: 6.5955 - accuracy: 0.1072\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3071/3649 [========================>.....] - ETA: 2:42 - loss: 6.5950 - accuracy: 0.1071\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3072/3649 [========================>.....] - ETA: 2:42 - loss: 6.5951 - accuracy: 0.1071\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3073/3649 [========================>.....] - ETA: 2:42 - loss: 6.5947 - accuracy: 0.1071\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3074/3649 [========================>.....] - ETA: 2:41 - loss: 6.5927 - accuracy: 0.1074\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3075/3649 [========================>.....] - ETA: 2:41 - loss: 6.5920 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3076/3649 [========================>.....] - ETA: 2:41 - loss: 6.5910 - accuracy: 0.1073\n",
            "Epoch 00001: loss did not improve from 6.58885\n",
            "3077/3649 [========================>.....] - ETA: 2:40 - loss: 6.5896 - accuracy: 0.1076\n",
            "Epoch 00001: loss improved from 6.58885 to 6.58816, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3078/3649 [========================>.....] - ETA: 2:40 - loss: 6.5882 - accuracy: 0.1079\n",
            "Epoch 00001: loss improved from 6.58816 to 6.58712, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3079/3649 [========================>.....] - ETA: 2:40 - loss: 6.5871 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.58712 to 6.58612, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3080/3649 [========================>.....] - ETA: 2:40 - loss: 6.5861 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.58612 to 6.58508, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3081/3649 [========================>.....] - ETA: 2:39 - loss: 6.5851 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.58508 to 6.58403, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3082/3649 [========================>.....] - ETA: 2:39 - loss: 6.5840 - accuracy: 0.1077\n",
            "Epoch 00001: loss improved from 6.58403 to 6.58277, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3083/3649 [========================>.....] - ETA: 2:39 - loss: 6.5828 - accuracy: 0.1080WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192839). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.58277 to 6.58174, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3084/3649 [========================>.....] - ETA: 2:39 - loss: 6.5817 - accuracy: 0.1080WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.297962). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.58174 to 6.58030, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3085/3649 [========================>.....] - ETA: 2:38 - loss: 6.5803 - accuracy: 0.1083WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302609). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.58030 to 6.57917, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3086/3649 [========================>.....] - ETA: 2:38 - loss: 6.5792 - accuracy: 0.1082WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302609). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.57917 to 6.57806, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3087/3649 [========================>.....] - ETA: 2:38 - loss: 6.5781 - accuracy: 0.1082WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309337). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.57806 to 6.57666, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3088/3649 [========================>.....] - ETA: 2:38 - loss: 6.5767 - accuracy: 0.1082WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309337). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3089/3649 [========================>.....] - ETA: 2:37 - loss: 6.5786 - accuracy: 0.1081WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302609). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3090/3649 [========================>.....] - ETA: 2:37 - loss: 6.5785 - accuracy: 0.1081WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.300608). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3091/3649 [========================>.....] - ETA: 2:37 - loss: 6.5783 - accuracy: 0.1081WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.294344). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3092/3649 [========================>.....] - ETA: 2:37 - loss: 6.5774 - accuracy: 0.1080WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292492). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3093/3649 [========================>.....] - ETA: 2:36 - loss: 6.5788 - accuracy: 0.1080\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3094/3649 [========================>.....] - ETA: 2:36 - loss: 6.5772 - accuracy: 0.1080\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3095/3649 [========================>.....] - ETA: 2:36 - loss: 6.5785 - accuracy: 0.1079\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3096/3649 [========================>.....] - ETA: 2:35 - loss: 6.5779 - accuracy: 0.1079\n",
            "Epoch 00001: loss did not improve from 6.57666\n",
            "3097/3649 [========================>.....] - ETA: 2:35 - loss: 6.5767 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.57666 to 6.57616, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3098/3649 [========================>.....] - ETA: 2:35 - loss: 6.5762 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.57616 to 6.57547, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3099/3649 [========================>.....] - ETA: 2:35 - loss: 6.5755 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.57547 to 6.57429, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3100/3649 [========================>.....] - ETA: 2:34 - loss: 6.5743 - accuracy: 0.1081\n",
            "Epoch 00001: loss improved from 6.57429 to 6.57336, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3101/3649 [========================>.....] - ETA: 2:34 - loss: 6.5734 - accuracy: 0.1080\n",
            "Epoch 00001: loss did not improve from 6.57336\n",
            "3102/3649 [========================>.....] - ETA: 2:34 - loss: 6.5747 - accuracy: 0.1080\n",
            "Epoch 00001: loss did not improve from 6.57336\n",
            "3103/3649 [========================>.....] - ETA: 2:33 - loss: 6.5750 - accuracy: 0.1080\n",
            "Epoch 00001: loss improved from 6.57336 to 6.57326, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3104/3649 [========================>.....] - ETA: 2:33 - loss: 6.5733 - accuracy: 0.1079\n",
            "Epoch 00001: loss did not improve from 6.57326\n",
            "3105/3649 [========================>.....] - ETA: 2:33 - loss: 6.5734 - accuracy: 0.1079\n",
            "Epoch 00001: loss did not improve from 6.57326\n",
            "3106/3649 [========================>.....] - ETA: 2:33 - loss: 6.5734 - accuracy: 0.1079\n",
            "Epoch 00001: loss improved from 6.57326 to 6.57252, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3107/3649 [========================>.....] - ETA: 2:32 - loss: 6.5725 - accuracy: 0.1078\n",
            "Epoch 00001: loss improved from 6.57252 to 6.57173, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3108/3649 [========================>.....] - ETA: 2:32 - loss: 6.5717 - accuracy: 0.1081\n",
            "Epoch 00001: loss improved from 6.57173 to 6.57072, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3109/3649 [========================>.....] - ETA: 2:32 - loss: 6.5707 - accuracy: 0.1081\n",
            "Epoch 00001: loss improved from 6.57072 to 6.56984, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3110/3649 [========================>.....] - ETA: 2:32 - loss: 6.5698 - accuracy: 0.1080\n",
            "Epoch 00001: loss did not improve from 6.56984\n",
            "3111/3649 [========================>.....] - ETA: 2:31 - loss: 6.5713 - accuracy: 0.1080\n",
            "Epoch 00001: loss did not improve from 6.56984\n",
            "3112/3649 [========================>.....] - ETA: 2:31 - loss: 6.5701 - accuracy: 0.1083\n",
            "Epoch 00001: loss improved from 6.56984 to 6.56907, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3113/3649 [========================>.....] - ETA: 2:31 - loss: 6.5691 - accuracy: 0.1083\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3114/3649 [========================>.....] - ETA: 2:30 - loss: 6.5705 - accuracy: 0.1082\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3115/3649 [========================>.....] - ETA: 2:30 - loss: 6.5722 - accuracy: 0.1082\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3116/3649 [========================>.....] - ETA: 2:30 - loss: 6.5709 - accuracy: 0.1085\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3117/3649 [========================>.....] - ETA: 2:30 - loss: 6.5705 - accuracy: 0.1084\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3118/3649 [========================>.....] - ETA: 2:29 - loss: 6.5708 - accuracy: 0.1084\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3119/3649 [========================>.....] - ETA: 2:29 - loss: 6.5700 - accuracy: 0.1084\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3120/3649 [========================>.....] - ETA: 2:29 - loss: 6.5716 - accuracy: 0.1083\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3121/3649 [========================>.....] - ETA: 2:28 - loss: 6.5732 - accuracy: 0.1083\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3122/3649 [========================>.....] - ETA: 2:28 - loss: 6.5714 - accuracy: 0.1086\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3123/3649 [========================>.....] - ETA: 2:28 - loss: 6.5727 - accuracy: 0.1085\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3124/3649 [========================>.....] - ETA: 2:27 - loss: 6.5715 - accuracy: 0.1088\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3125/3649 [========================>.....] - ETA: 2:27 - loss: 6.5712 - accuracy: 0.1088\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3126/3649 [========================>.....] - ETA: 2:27 - loss: 6.5713 - accuracy: 0.1088\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3127/3649 [========================>.....] - ETA: 2:27 - loss: 6.5696 - accuracy: 0.1091\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3128/3649 [========================>.....] - ETA: 2:26 - loss: 6.5708 - accuracy: 0.1090\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3129/3649 [========================>.....] - ETA: 2:26 - loss: 6.5723 - accuracy: 0.1090\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3130/3649 [========================>.....] - ETA: 2:26 - loss: 6.5717 - accuracy: 0.1089\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3131/3649 [========================>.....] - ETA: 2:25 - loss: 6.5714 - accuracy: 0.1089\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3132/3649 [========================>.....] - ETA: 2:25 - loss: 6.5729 - accuracy: 0.1089\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3133/3649 [========================>.....] - ETA: 2:25 - loss: 6.5716 - accuracy: 0.1092\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3134/3649 [========================>.....] - ETA: 2:25 - loss: 6.5709 - accuracy: 0.1091\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3135/3649 [========================>.....] - ETA: 2:24 - loss: 6.5722 - accuracy: 0.1091\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3136/3649 [========================>.....] - ETA: 2:24 - loss: 6.5713 - accuracy: 0.1091\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3137/3649 [========================>.....] - ETA: 2:24 - loss: 6.5706 - accuracy: 0.1090\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3138/3649 [========================>.....] - ETA: 2:23 - loss: 6.5718 - accuracy: 0.1090\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3139/3649 [========================>.....] - ETA: 2:23 - loss: 6.5730 - accuracy: 0.1090\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3140/3649 [========================>.....] - ETA: 2:23 - loss: 6.5721 - accuracy: 0.1089\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3141/3649 [========================>.....] - ETA: 2:22 - loss: 6.5714 - accuracy: 0.1089\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3142/3649 [========================>.....] - ETA: 2:22 - loss: 6.5725 - accuracy: 0.1088\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3143/3649 [========================>.....] - ETA: 2:22 - loss: 6.5736 - accuracy: 0.1088\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3144/3649 [========================>.....] - ETA: 2:22 - loss: 6.5726 - accuracy: 0.1088\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3145/3649 [========================>.....] - ETA: 2:21 - loss: 6.5718 - accuracy: 0.1091\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3146/3649 [========================>.....] - ETA: 2:21 - loss: 6.5726 - accuracy: 0.1090\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3147/3649 [========================>.....] - ETA: 2:21 - loss: 6.5710 - accuracy: 0.1093\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3148/3649 [========================>.....] - ETA: 2:20 - loss: 6.5713 - accuracy: 0.1093\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3149/3649 [========================>.....] - ETA: 2:20 - loss: 6.5701 - accuracy: 0.1096\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3150/3649 [========================>.....] - ETA: 2:20 - loss: 6.5700 - accuracy: 0.1095\n",
            "Epoch 00001: loss did not improve from 6.56907\n",
            "3151/3649 [========================>.....] - ETA: 2:20 - loss: 6.5702 - accuracy: 0.1095\n",
            "Epoch 00001: loss improved from 6.56907 to 6.56860, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3152/3649 [========================>.....] - ETA: 2:19 - loss: 6.5686 - accuracy: 0.1098\n",
            "Epoch 00001: loss did not improve from 6.56860\n",
            "3153/3649 [========================>.....] - ETA: 2:19 - loss: 6.5697 - accuracy: 0.1097\n",
            "Epoch 00001: loss improved from 6.56860 to 6.56843, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3154/3649 [========================>.....] - ETA: 2:19 - loss: 6.5684 - accuracy: 0.1100\n",
            "Epoch 00001: loss improved from 6.56843 to 6.56713, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3155/3649 [========================>.....] - ETA: 2:18 - loss: 6.5671 - accuracy: 0.1103\n",
            "Epoch 00001: loss did not improve from 6.56713\n",
            "3156/3649 [========================>.....] - ETA: 2:18 - loss: 6.5684 - accuracy: 0.1103\n",
            "Epoch 00001: loss did not improve from 6.56713\n",
            "3157/3649 [========================>.....] - ETA: 2:18 - loss: 6.5686 - accuracy: 0.1102\n",
            "Epoch 00001: loss improved from 6.56713 to 6.56682, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3158/3649 [========================>.....] - ETA: 2:18 - loss: 6.5668 - accuracy: 0.1105\n",
            "Epoch 00001: loss did not improve from 6.56682\n",
            "3159/3649 [========================>.....] - ETA: 2:17 - loss: 6.5680 - accuracy: 0.1105\n",
            "Epoch 00001: loss did not improve from 6.56682\n",
            "3160/3649 [========================>.....] - ETA: 2:17 - loss: 6.5679 - accuracy: 0.1104\n",
            "Epoch 00001: loss improved from 6.56682 to 6.56643, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3161/3649 [========================>.....] - ETA: 2:17 - loss: 6.5664 - accuracy: 0.1107\n",
            "Epoch 00001: loss improved from 6.56643 to 6.56499, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3162/3649 [========================>.....] - ETA: 2:16 - loss: 6.5650 - accuracy: 0.1110\n",
            "Epoch 00001: loss did not improve from 6.56499\n",
            "3163/3649 [=========================>....] - ETA: 2:16 - loss: 6.5663 - accuracy: 0.1110\n",
            "Epoch 00001: loss improved from 6.56499 to 6.56471, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3164/3649 [=========================>....] - ETA: 2:16 - loss: 6.5647 - accuracy: 0.1113\n",
            "Epoch 00001: loss did not improve from 6.56471\n",
            "3165/3649 [=========================>....] - ETA: 2:16 - loss: 6.5653 - accuracy: 0.1112\n",
            "Epoch 00001: loss improved from 6.56471 to 6.56347, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3166/3649 [=========================>....] - ETA: 2:15 - loss: 6.5635 - accuracy: 0.1115\n",
            "Epoch 00001: loss improved from 6.56347 to 6.56301, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3167/3649 [=========================>....] - ETA: 2:15 - loss: 6.5630 - accuracy: 0.1115\n",
            "Epoch 00001: loss improved from 6.56301 to 6.56116, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3168/3649 [=========================>....] - ETA: 2:15 - loss: 6.5612 - accuracy: 0.1117\n",
            "Epoch 00001: loss did not improve from 6.56116\n",
            "3169/3649 [=========================>....] - ETA: 2:15 - loss: 6.5626 - accuracy: 0.1117\n",
            "Epoch 00001: loss improved from 6.56116 to 6.56095, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3170/3649 [=========================>....] - ETA: 2:14 - loss: 6.5610 - accuracy: 0.1120\n",
            "Epoch 00001: loss improved from 6.56095 to 6.55926, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3171/3649 [=========================>....] - ETA: 2:14 - loss: 6.5593 - accuracy: 0.1123\n",
            "Epoch 00001: loss did not improve from 6.55926\n",
            "3172/3649 [=========================>....] - ETA: 2:14 - loss: 6.5609 - accuracy: 0.1122\n",
            "Epoch 00001: loss did not improve from 6.55926\n",
            "3173/3649 [=========================>....] - ETA: 2:13 - loss: 6.5600 - accuracy: 0.1122\n",
            "Epoch 00001: loss improved from 6.55926 to 6.55826, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3174/3649 [=========================>....] - ETA: 2:13 - loss: 6.5583 - accuracy: 0.1125\n",
            "Epoch 00001: loss did not improve from 6.55826\n",
            "3175/3649 [=========================>....] - ETA: 2:13 - loss: 6.5597 - accuracy: 0.1124\n",
            "Epoch 00001: loss improved from 6.55826 to 6.55773, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3176/3649 [=========================>....] - ETA: 2:13 - loss: 6.5577 - accuracy: 0.1127\n",
            "Epoch 00001: loss did not improve from 6.55773\n",
            "3177/3649 [=========================>....] - ETA: 2:12 - loss: 6.5578 - accuracy: 0.1127\n",
            "Epoch 00001: loss improved from 6.55773 to 6.55587, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3178/3649 [=========================>....] - ETA: 2:12 - loss: 6.5559 - accuracy: 0.1130\n",
            "Epoch 00001: loss did not improve from 6.55587\n",
            "3179/3649 [=========================>....] - ETA: 2:12 - loss: 6.5571 - accuracy: 0.1129\n",
            "Epoch 00001: loss improved from 6.55587 to 6.55560, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3180/3649 [=========================>....] - ETA: 2:11 - loss: 6.5556 - accuracy: 0.1129\n",
            "Epoch 00001: loss did not improve from 6.55560\n",
            "3181/3649 [=========================>....] - ETA: 2:11 - loss: 6.5569 - accuracy: 0.1129\n",
            "Epoch 00001: loss did not improve from 6.55560\n",
            "3182/3649 [=========================>....] - ETA: 2:11 - loss: 6.5562 - accuracy: 0.1128\n",
            "Epoch 00001: loss improved from 6.55560 to 6.55507, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3183/3649 [=========================>....] - ETA: 2:11 - loss: 6.5551 - accuracy: 0.1128\n",
            "Epoch 00001: loss did not improve from 6.55507\n",
            "3184/3649 [=========================>....] - ETA: 2:10 - loss: 6.5564 - accuracy: 0.1128\n",
            "Epoch 00001: loss did not improve from 6.55507\n",
            "3185/3649 [=========================>....] - ETA: 2:10 - loss: 6.5569 - accuracy: 0.1127\n",
            "Epoch 00001: loss did not improve from 6.55507\n",
            "3186/3649 [=========================>....] - ETA: 2:10 - loss: 6.5554 - accuracy: 0.1127\n",
            "Epoch 00001: loss improved from 6.55507 to 6.55457, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3187/3649 [=========================>....] - ETA: 2:09 - loss: 6.5546 - accuracy: 0.1126\n",
            "Epoch 00001: loss did not improve from 6.55457\n",
            "3188/3649 [=========================>....] - ETA: 2:09 - loss: 6.5546 - accuracy: 0.1126\n",
            "Epoch 00001: loss improved from 6.55457 to 6.55277, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3189/3649 [=========================>....] - ETA: 2:09 - loss: 6.5528 - accuracy: 0.1129\n",
            "Epoch 00001: loss did not improve from 6.55277\n",
            "3190/3649 [=========================>....] - ETA: 2:09 - loss: 6.5538 - accuracy: 0.1129\n",
            "Epoch 00001: loss improved from 6.55277 to 6.55188, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3191/3649 [=========================>....] - ETA: 2:08 - loss: 6.5519 - accuracy: 0.1131\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3192/3649 [=========================>....] - ETA: 2:08 - loss: 6.5530 - accuracy: 0.1131\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3193/3649 [=========================>....] - ETA: 2:08 - loss: 6.5545 - accuracy: 0.1131\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3194/3649 [=========================>....] - ETA: 2:07 - loss: 6.5543 - accuracy: 0.1130\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3195/3649 [=========================>....] - ETA: 2:07 - loss: 6.5540 - accuracy: 0.1130\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3196/3649 [=========================>....] - ETA: 2:07 - loss: 6.5552 - accuracy: 0.1130\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3197/3649 [=========================>....] - ETA: 2:07 - loss: 6.5533 - accuracy: 0.1132\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3198/3649 [=========================>....] - ETA: 2:06 - loss: 6.5539 - accuracy: 0.1132\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3199/3649 [=========================>....] - ETA: 2:06 - loss: 6.5535 - accuracy: 0.1132\n",
            "Epoch 00001: loss did not improve from 6.55188\n",
            "3200/3649 [=========================>....] - ETA: 2:06 - loss: 6.5527 - accuracy: 0.1131\n",
            "Epoch 00001: loss improved from 6.55188 to 6.55138, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3201/3649 [=========================>....] - ETA: 2:05 - loss: 6.5514 - accuracy: 0.1134\n",
            "Epoch 00001: loss improved from 6.55138 to 6.55050, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3202/3649 [=========================>....] - ETA: 2:05 - loss: 6.5505 - accuracy: 0.1134\n",
            "Epoch 00001: loss improved from 6.55050 to 6.54945, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3203/3649 [=========================>....] - ETA: 2:05 - loss: 6.5494 - accuracy: 0.1133\n",
            "Epoch 00001: loss did not improve from 6.54945\n",
            "3204/3649 [=========================>....] - ETA: 2:05 - loss: 6.5505 - accuracy: 0.1133\n",
            "Epoch 00001: loss did not improve from 6.54945\n",
            "3205/3649 [=========================>....] - ETA: 2:04 - loss: 6.5517 - accuracy: 0.1133\n",
            "Epoch 00001: loss did not improve from 6.54945\n",
            "3206/3649 [=========================>....] - ETA: 2:04 - loss: 6.5503 - accuracy: 0.1135\n",
            "Epoch 00001: loss improved from 6.54945 to 6.54937, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3207/3649 [=========================>....] - ETA: 2:04 - loss: 6.5494 - accuracy: 0.1135\n",
            "Epoch 00001: loss improved from 6.54937 to 6.54884, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3208/3649 [=========================>....] - ETA: 2:04 - loss: 6.5488 - accuracy: 0.1135\n",
            "Epoch 00001: loss did not improve from 6.54884\n",
            "3209/3649 [=========================>....] - ETA: 2:03 - loss: 6.5491 - accuracy: 0.1134\n",
            "Epoch 00001: loss did not improve from 6.54884\n",
            "3210/3649 [=========================>....] - ETA: 2:03 - loss: 6.5503 - accuracy: 0.1134\n",
            "Epoch 00001: loss improved from 6.54884 to 6.54836, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3211/3649 [=========================>....] - ETA: 2:03 - loss: 6.5484 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54836\n",
            "3212/3649 [=========================>....] - ETA: 2:02 - loss: 6.5486 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54836\n",
            "3213/3649 [=========================>....] - ETA: 2:02 - loss: 6.5487 - accuracy: 0.1136\n",
            "Epoch 00001: loss improved from 6.54836 to 6.54717, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3214/3649 [=========================>....] - ETA: 2:02 - loss: 6.5472 - accuracy: 0.1139\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3215/3649 [=========================>....] - ETA: 2:01 - loss: 6.5476 - accuracy: 0.1138\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3216/3649 [=========================>....] - ETA: 2:01 - loss: 6.5490 - accuracy: 0.1138\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3217/3649 [=========================>....] - ETA: 2:01 - loss: 6.5504 - accuracy: 0.1138\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3218/3649 [=========================>....] - ETA: 2:01 - loss: 6.5498 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3219/3649 [=========================>....] - ETA: 2:00 - loss: 6.5510 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3220/3649 [=========================>....] - ETA: 2:00 - loss: 6.5511 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3221/3649 [=========================>....] - ETA: 2:00 - loss: 6.5503 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3222/3649 [=========================>....] - ETA: 1:59 - loss: 6.5494 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3223/3649 [=========================>....] - ETA: 1:59 - loss: 6.5485 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3224/3649 [=========================>....] - ETA: 1:59 - loss: 6.5497 - accuracy: 0.1135\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3225/3649 [=========================>....] - ETA: 1:59 - loss: 6.5493 - accuracy: 0.1135\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3226/3649 [=========================>....] - ETA: 1:58 - loss: 6.5482 - accuracy: 0.1135\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3227/3649 [=========================>....] - ETA: 1:58 - loss: 6.5483 - accuracy: 0.1134\n",
            "Epoch 00001: loss did not improve from 6.54717\n",
            "3228/3649 [=========================>....] - ETA: 1:58 - loss: 6.5483 - accuracy: 0.1134\n",
            "Epoch 00001: loss improved from 6.54717 to 6.54662, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3229/3649 [=========================>....] - ETA: 1:57 - loss: 6.5466 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3230/3649 [=========================>....] - ETA: 1:57 - loss: 6.5482 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3231/3649 [=========================>....] - ETA: 1:57 - loss: 6.5495 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3232/3649 [=========================>....] - ETA: 1:57 - loss: 6.5477 - accuracy: 0.1139\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3233/3649 [=========================>....] - ETA: 1:56 - loss: 6.5473 - accuracy: 0.1138\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3234/3649 [=========================>....] - ETA: 1:56 - loss: 6.5473 - accuracy: 0.1138\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3235/3649 [=========================>....] - ETA: 1:56 - loss: 6.5475 - accuracy: 0.1138\n",
            "Epoch 00001: loss did not improve from 6.54662\n",
            "3236/3649 [=========================>....] - ETA: 1:55 - loss: 6.5472 - accuracy: 0.1137\n",
            "Epoch 00001: loss improved from 6.54662 to 6.54641, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3237/3649 [=========================>....] - ETA: 1:55 - loss: 6.5464 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54641\n",
            "3238/3649 [=========================>....] - ETA: 1:55 - loss: 6.5476 - accuracy: 0.1137\n",
            "Epoch 00001: loss did not improve from 6.54641\n",
            "3239/3649 [=========================>....] - ETA: 1:55 - loss: 6.5476 - accuracy: 0.1136\n",
            "Epoch 00001: loss did not improve from 6.54641\n",
            "3240/3649 [=========================>....] - ETA: 1:54 - loss: 6.5470 - accuracy: 0.1136\n",
            "Epoch 00001: loss improved from 6.54641 to 6.54627, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3241/3649 [=========================>....] - ETA: 1:54 - loss: 6.5463 - accuracy: 0.1135\n",
            "Epoch 00001: loss improved from 6.54627 to 6.54620, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3242/3649 [=========================>....] - ETA: 1:54 - loss: 6.5462 - accuracy: 0.1135\n",
            "Epoch 00001: loss improved from 6.54620 to 6.54439, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3243/3649 [=========================>....] - ETA: 1:53 - loss: 6.5444 - accuracy: 0.1138\n",
            "Epoch 00001: loss improved from 6.54439 to 6.54386, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3244/3649 [=========================>....] - ETA: 1:53 - loss: 6.5439 - accuracy: 0.1137\n",
            "Epoch 00001: loss improved from 6.54386 to 6.54295, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3245/3649 [=========================>....] - ETA: 1:53 - loss: 6.5429 - accuracy: 0.1137\n",
            "Epoch 00001: loss improved from 6.54295 to 6.54220, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3246/3649 [=========================>....] - ETA: 1:53 - loss: 6.5422 - accuracy: 0.1137\n",
            "Epoch 00001: loss improved from 6.54220 to 6.54195, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3247/3649 [=========================>....] - ETA: 1:52 - loss: 6.5419 - accuracy: 0.1136WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.281480). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.54195 to 6.54194, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3248/3649 [=========================>....] - ETA: 1:52 - loss: 6.5419 - accuracy: 0.1136WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296270). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.54194 to 6.54147, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3249/3649 [=========================>....] - ETA: 1:52 - loss: 6.5415 - accuracy: 0.1136WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296270). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.54147 to 6.54074, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3250/3649 [=========================>....] - ETA: 1:52 - loss: 6.5407 - accuracy: 0.1138WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301141). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.54074 to 6.53942, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3251/3649 [=========================>....] - ETA: 1:51 - loss: 6.5394 - accuracy: 0.1138WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307248). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3252/3649 [=========================>....] - ETA: 1:51 - loss: 6.5409 - accuracy: 0.1138WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301141). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3253/3649 [=========================>....] - ETA: 1:51 - loss: 6.5421 - accuracy: 0.1137WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301141). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3254/3649 [=========================>....] - ETA: 1:51 - loss: 6.5403 - accuracy: 0.1140WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286351). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3255/3649 [=========================>....] - ETA: 1:50 - loss: 6.5417 - accuracy: 0.1140WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.272142). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3256/3649 [=========================>....] - ETA: 1:50 - loss: 6.5405 - accuracy: 0.1143\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3257/3649 [=========================>....] - ETA: 1:50 - loss: 6.5405 - accuracy: 0.1142\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3258/3649 [=========================>....] - ETA: 1:49 - loss: 6.5416 - accuracy: 0.1142\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3259/3649 [=========================>....] - ETA: 1:49 - loss: 6.5402 - accuracy: 0.1141\n",
            "Epoch 00001: loss did not improve from 6.53942\n",
            "3260/3649 [=========================>....] - ETA: 1:49 - loss: 6.5397 - accuracy: 0.1141\n",
            "Epoch 00001: loss improved from 6.53942 to 6.53876, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3261/3649 [=========================>....] - ETA: 1:49 - loss: 6.5388 - accuracy: 0.1144\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3262/3649 [=========================>....] - ETA: 1:48 - loss: 6.5391 - accuracy: 0.1143\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3263/3649 [=========================>....] - ETA: 1:48 - loss: 6.5406 - accuracy: 0.1143\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3264/3649 [=========================>....] - ETA: 1:48 - loss: 6.5396 - accuracy: 0.1143\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3265/3649 [=========================>....] - ETA: 1:47 - loss: 6.5398 - accuracy: 0.1142\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3266/3649 [=========================>....] - ETA: 1:47 - loss: 6.5411 - accuracy: 0.1142\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3267/3649 [=========================>....] - ETA: 1:47 - loss: 6.5425 - accuracy: 0.1142\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3268/3649 [=========================>....] - ETA: 1:46 - loss: 6.5426 - accuracy: 0.1141\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3269/3649 [=========================>....] - ETA: 1:46 - loss: 6.5425 - accuracy: 0.1141\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3270/3649 [=========================>....] - ETA: 1:46 - loss: 6.5412 - accuracy: 0.1144\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3271/3649 [=========================>....] - ETA: 1:46 - loss: 6.5410 - accuracy: 0.1143\n",
            "Epoch 00001: loss did not improve from 6.53876\n",
            "3272/3649 [=========================>....] - ETA: 1:45 - loss: 6.5396 - accuracy: 0.1143\n",
            "Epoch 00001: loss improved from 6.53876 to 6.53836, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3273/3649 [=========================>....] - ETA: 1:45 - loss: 6.5384 - accuracy: 0.1146\n",
            "Epoch 00001: loss improved from 6.53836 to 6.53725, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3274/3649 [=========================>....] - ETA: 1:45 - loss: 6.5372 - accuracy: 0.1148\n",
            "Epoch 00001: loss improved from 6.53725 to 6.53644, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3275/3649 [=========================>....] - ETA: 1:45 - loss: 6.5364 - accuracy: 0.1148\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3276/3649 [=========================>....] - ETA: 1:44 - loss: 6.5378 - accuracy: 0.1148\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3277/3649 [=========================>....] - ETA: 1:44 - loss: 6.5380 - accuracy: 0.1147\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3278/3649 [=========================>....] - ETA: 1:44 - loss: 6.5392 - accuracy: 0.1147\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3279/3649 [=========================>....] - ETA: 1:43 - loss: 6.5403 - accuracy: 0.1147\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3280/3649 [=========================>....] - ETA: 1:43 - loss: 6.5402 - accuracy: 0.1146\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3281/3649 [=========================>....] - ETA: 1:43 - loss: 6.5393 - accuracy: 0.1146\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3282/3649 [=========================>....] - ETA: 1:42 - loss: 6.5393 - accuracy: 0.1146\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3283/3649 [=========================>....] - ETA: 1:42 - loss: 6.5396 - accuracy: 0.1145\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3284/3649 [=========================>....] - ETA: 1:42 - loss: 6.5381 - accuracy: 0.1145\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3285/3649 [==========================>...] - ETA: 1:42 - loss: 6.5374 - accuracy: 0.1145\n",
            "Epoch 00001: loss did not improve from 6.53644\n",
            "3286/3649 [==========================>...] - ETA: 1:41 - loss: 6.5367 - accuracy: 0.1144\n",
            "Epoch 00001: loss improved from 6.53644 to 6.53581, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3287/3649 [==========================>...] - ETA: 1:41 - loss: 6.5358 - accuracy: 0.1144\n",
            "Epoch 00001: loss improved from 6.53581 to 6.53552, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3288/3649 [==========================>...] - ETA: 1:41 - loss: 6.5355 - accuracy: 0.1144\n",
            "Epoch 00001: loss improved from 6.53552 to 6.53537, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3289/3649 [==========================>...] - ETA: 1:41 - loss: 6.5354 - accuracy: 0.1143\n",
            "Epoch 00001: loss improved from 6.53537 to 6.53527, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3290/3649 [==========================>...] - ETA: 1:40 - loss: 6.5353 - accuracy: 0.1143\n",
            "Epoch 00001: loss improved from 6.53527 to 6.53470, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3291/3649 [==========================>...] - ETA: 1:40 - loss: 6.5347 - accuracy: 0.1143\n",
            "Epoch 00001: loss improved from 6.53470 to 6.53368, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3292/3649 [==========================>...] - ETA: 1:40 - loss: 6.5337 - accuracy: 0.1145\n",
            "Epoch 00001: loss improved from 6.53368 to 6.53186, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3293/3649 [==========================>...] - ETA: 1:40 - loss: 6.5319 - accuracy: 0.1148WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298112). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.53186 to 6.53185, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3294/3649 [==========================>...] - ETA: 1:39 - loss: 6.5319 - accuracy: 0.1148WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302689). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.53185 to 6.53082, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3295/3649 [==========================>...] - ETA: 1:39 - loss: 6.5308 - accuracy: 0.1147WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.308038). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.53082 to 6.52983, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3296/3649 [==========================>...] - ETA: 1:39 - loss: 6.5298 - accuracy: 0.1147WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.308038). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.52983\n",
            "3297/3649 [==========================>...] - ETA: 1:38 - loss: 6.5298 - accuracy: 0.1146WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.308038). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.52983 to 6.52921, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3298/3649 [==========================>...] - ETA: 1:38 - loss: 6.5292 - accuracy: 0.1146WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.308038). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.52921\n",
            "3299/3649 [==========================>...] - ETA: 1:38 - loss: 6.5306 - accuracy: 0.1146WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.305228). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.52921\n",
            "3300/3649 [==========================>...] - ETA: 1:38 - loss: 6.5299 - accuracy: 0.1145WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298112). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.52921 to 6.52833, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3301/3649 [==========================>...] - ETA: 1:37 - loss: 6.5283 - accuracy: 0.1148WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194237). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.52833 to 6.52801, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3302/3649 [==========================>...] - ETA: 1:37 - loss: 6.5280 - accuracy: 0.1148\n",
            "Epoch 00001: loss did not improve from 6.52801\n",
            "3303/3649 [==========================>...] - ETA: 1:37 - loss: 6.5296 - accuracy: 0.1147\n",
            "Epoch 00001: loss did not improve from 6.52801\n",
            "3304/3649 [==========================>...] - ETA: 1:36 - loss: 6.5289 - accuracy: 0.1147\n",
            "Epoch 00001: loss improved from 6.52801 to 6.52725, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3305/3649 [==========================>...] - ETA: 1:36 - loss: 6.5273 - accuracy: 0.1150\n",
            "Epoch 00001: loss did not improve from 6.52725\n",
            "3306/3649 [==========================>...] - ETA: 1:36 - loss: 6.5287 - accuracy: 0.1149\n",
            "Epoch 00001: loss improved from 6.52725 to 6.52709, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3307/3649 [==========================>...] - ETA: 1:36 - loss: 6.5271 - accuracy: 0.1152\n",
            "Epoch 00001: loss did not improve from 6.52709\n",
            "3308/3649 [==========================>...] - ETA: 1:35 - loss: 6.5285 - accuracy: 0.1152\n",
            "Epoch 00001: loss did not improve from 6.52709\n",
            "3309/3649 [==========================>...] - ETA: 1:35 - loss: 6.5283 - accuracy: 0.1151\n",
            "Epoch 00001: loss improved from 6.52709 to 6.52665, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3310/3649 [==========================>...] - ETA: 1:35 - loss: 6.5267 - accuracy: 0.1154\n",
            "Epoch 00001: loss improved from 6.52665 to 6.52607, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3311/3649 [==========================>...] - ETA: 1:35 - loss: 6.5261 - accuracy: 0.1154\n",
            "Epoch 00001: loss improved from 6.52607 to 6.52598, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3312/3649 [==========================>...] - ETA: 1:34 - loss: 6.5260 - accuracy: 0.1153\n",
            "Epoch 00001: loss did not improve from 6.52598\n",
            "3313/3649 [==========================>...] - ETA: 1:34 - loss: 6.5260 - accuracy: 0.1153\n",
            "Epoch 00001: loss improved from 6.52598 to 6.52535, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3314/3649 [==========================>...] - ETA: 1:34 - loss: 6.5253 - accuracy: 0.1153\n",
            "Epoch 00001: loss improved from 6.52535 to 6.52362, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3315/3649 [==========================>...] - ETA: 1:34 - loss: 6.5236 - accuracy: 0.1155\n",
            "Epoch 00001: loss improved from 6.52362 to 6.52323, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3316/3649 [==========================>...] - ETA: 1:33 - loss: 6.5232 - accuracy: 0.1155\n",
            "Epoch 00001: loss improved from 6.52323 to 6.52171, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3317/3649 [==========================>...] - ETA: 1:33 - loss: 6.5217 - accuracy: 0.1158WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.193825). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.52171 to 6.52111, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3318/3649 [==========================>...] - ETA: 1:33 - loss: 6.5211 - accuracy: 0.1157WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.290660). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3319/3649 [==========================>...] - ETA: 1:32 - loss: 6.5212 - accuracy: 0.1157WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.290660). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3320/3649 [==========================>...] - ETA: 1:32 - loss: 6.5227 - accuracy: 0.1157WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.290660). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3321/3649 [==========================>...] - ETA: 1:32 - loss: 6.5227 - accuracy: 0.1156\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3322/3649 [==========================>...] - ETA: 1:32 - loss: 6.5233 - accuracy: 0.1156\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3323/3649 [==========================>...] - ETA: 1:31 - loss: 6.5240 - accuracy: 0.1156\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3324/3649 [==========================>...] - ETA: 1:31 - loss: 6.5222 - accuracy: 0.1158\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3325/3649 [==========================>...] - ETA: 1:31 - loss: 6.5233 - accuracy: 0.1158\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3326/3649 [==========================>...] - ETA: 1:30 - loss: 6.5242 - accuracy: 0.1158\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3327/3649 [==========================>...] - ETA: 1:30 - loss: 6.5227 - accuracy: 0.1160\n",
            "Epoch 00001: loss did not improve from 6.52111\n",
            "3328/3649 [==========================>...] - ETA: 1:30 - loss: 6.5219 - accuracy: 0.1160\n",
            "Epoch 00001: loss improved from 6.52111 to 6.52051, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3329/3649 [==========================>...] - ETA: 1:30 - loss: 6.5205 - accuracy: 0.1163\n",
            "Epoch 00001: loss improved from 6.52051 to 6.52046, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3330/3649 [==========================>...] - ETA: 1:29 - loss: 6.5205 - accuracy: 0.1162\n",
            "Epoch 00001: loss did not improve from 6.52046\n",
            "3331/3649 [==========================>...] - ETA: 1:29 - loss: 6.5214 - accuracy: 0.1162\n",
            "Epoch 00001: loss improved from 6.52046 to 6.51964, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3332/3649 [==========================>...] - ETA: 1:29 - loss: 6.5196 - accuracy: 0.1164\n",
            "Epoch 00001: loss improved from 6.51964 to 6.51941, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3333/3649 [==========================>...] - ETA: 1:28 - loss: 6.5194 - accuracy: 0.1164\n",
            "Epoch 00001: loss improved from 6.51941 to 6.51784, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3334/3649 [==========================>...] - ETA: 1:28 - loss: 6.5178 - accuracy: 0.1167\n",
            "Epoch 00001: loss improved from 6.51784 to 6.51723, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3335/3649 [==========================>...] - ETA: 1:28 - loss: 6.5172 - accuracy: 0.1166\n",
            "Epoch 00001: loss improved from 6.51723 to 6.51554, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3336/3649 [==========================>...] - ETA: 1:28 - loss: 6.5155 - accuracy: 0.1169\n",
            "Epoch 00001: loss did not improve from 6.51554\n",
            "3337/3649 [==========================>...] - ETA: 1:27 - loss: 6.5156 - accuracy: 0.1169\n",
            "Epoch 00001: loss improved from 6.51554 to 6.51397, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3338/3649 [==========================>...] - ETA: 1:27 - loss: 6.5140 - accuracy: 0.1171\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3339/3649 [==========================>...] - ETA: 1:27 - loss: 6.5141 - accuracy: 0.1171\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3340/3649 [==========================>...] - ETA: 1:27 - loss: 6.5157 - accuracy: 0.1171\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3341/3649 [==========================>...] - ETA: 1:26 - loss: 6.5161 - accuracy: 0.1170\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3342/3649 [==========================>...] - ETA: 1:26 - loss: 6.5164 - accuracy: 0.1170\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3343/3649 [==========================>...] - ETA: 1:26 - loss: 6.5148 - accuracy: 0.1173\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3344/3649 [==========================>...] - ETA: 1:25 - loss: 6.5158 - accuracy: 0.1172\n",
            "Epoch 00001: loss did not improve from 6.51397\n",
            "3345/3649 [==========================>...] - ETA: 1:25 - loss: 6.5148 - accuracy: 0.1172\n",
            "Epoch 00001: loss improved from 6.51397 to 6.51374, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3346/3649 [==========================>...] - ETA: 1:25 - loss: 6.5137 - accuracy: 0.1172\n",
            "Epoch 00001: loss improved from 6.51374 to 6.51361, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3347/3649 [==========================>...] - ETA: 1:25 - loss: 6.5136 - accuracy: 0.1171\n",
            "Epoch 00001: loss improved from 6.51361 to 6.51318, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3348/3649 [==========================>...] - ETA: 1:24 - loss: 6.5132 - accuracy: 0.1171\n",
            "Epoch 00001: loss improved from 6.51318 to 6.51235, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3349/3649 [==========================>...] - ETA: 1:24 - loss: 6.5124 - accuracy: 0.1170\n",
            "Epoch 00001: loss improved from 6.51235 to 6.51186, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3350/3649 [==========================>...] - ETA: 1:24 - loss: 6.5119 - accuracy: 0.1170\n",
            "Epoch 00001: loss improved from 6.51186 to 6.51077, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3351/3649 [==========================>...] - ETA: 1:24 - loss: 6.5108 - accuracy: 0.1173WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.194911). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.51077 to 6.50904, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3352/3649 [==========================>...] - ETA: 1:23 - loss: 6.5090 - accuracy: 0.1175WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306213). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.50904 to 6.50815, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3353/3649 [==========================>...] - ETA: 1:23 - loss: 6.5082 - accuracy: 0.1175WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306675). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.50815 to 6.50640, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3354/3649 [==========================>...] - ETA: 1:23 - loss: 6.5064 - accuracy: 0.1178WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306675). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.50640 to 6.50529, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3355/3649 [==========================>...] - ETA: 1:23 - loss: 6.5053 - accuracy: 0.1180WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306675). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.50529 to 6.50372, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3356/3649 [==========================>...] - ETA: 1:22 - loss: 6.5037 - accuracy: 0.1183WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309579). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3357/3649 [==========================>...] - ETA: 1:22 - loss: 6.5055 - accuracy: 0.1183WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306675). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3358/3649 [==========================>...] - ETA: 1:22 - loss: 6.5074 - accuracy: 0.1182WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.306213). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3359/3649 [==========================>...] - ETA: 1:21 - loss: 6.5082 - accuracy: 0.1182WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.303814). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3360/3649 [==========================>...] - ETA: 1:21 - loss: 6.5072 - accuracy: 0.1182WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298682). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3361/3649 [==========================>...] - ETA: 1:21 - loss: 6.5055 - accuracy: 0.1184\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3362/3649 [==========================>...] - ETA: 1:21 - loss: 6.5063 - accuracy: 0.1184\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3363/3649 [==========================>...] - ETA: 1:20 - loss: 6.5046 - accuracy: 0.1186\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3364/3649 [==========================>...] - ETA: 1:20 - loss: 6.5044 - accuracy: 0.1186\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3365/3649 [==========================>...] - ETA: 1:20 - loss: 6.5061 - accuracy: 0.1186\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3366/3649 [==========================>...] - ETA: 1:19 - loss: 6.5075 - accuracy: 0.1185\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3367/3649 [==========================>...] - ETA: 1:19 - loss: 6.5073 - accuracy: 0.1185\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3368/3649 [==========================>...] - ETA: 1:19 - loss: 6.5055 - accuracy: 0.1188\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3369/3649 [==========================>...] - ETA: 1:19 - loss: 6.5064 - accuracy: 0.1187\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3370/3649 [==========================>...] - ETA: 1:18 - loss: 6.5054 - accuracy: 0.1187\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3371/3649 [==========================>...] - ETA: 1:18 - loss: 6.5043 - accuracy: 0.1187\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3372/3649 [==========================>...] - ETA: 1:18 - loss: 6.5046 - accuracy: 0.1186\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3373/3649 [==========================>...] - ETA: 1:17 - loss: 6.5043 - accuracy: 0.1186\n",
            "Epoch 00001: loss did not improve from 6.50372\n",
            "3374/3649 [==========================>...] - ETA: 1:17 - loss: 6.5042 - accuracy: 0.1186\n",
            "Epoch 00001: loss improved from 6.50372 to 6.50335, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3375/3649 [==========================>...] - ETA: 1:17 - loss: 6.5034 - accuracy: 0.1185\n",
            "Epoch 00001: loss improved from 6.50335 to 6.50266, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3376/3649 [==========================>...] - ETA: 1:17 - loss: 6.5027 - accuracy: 0.1185\n",
            "Epoch 00001: loss improved from 6.50266 to 6.50156, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3377/3649 [==========================>...] - ETA: 1:16 - loss: 6.5016 - accuracy: 0.1187\n",
            "Epoch 00001: loss improved from 6.50156 to 6.49979, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3378/3649 [==========================>...] - ETA: 1:16 - loss: 6.4998 - accuracy: 0.1190\n",
            "Epoch 00001: loss improved from 6.49979 to 6.49881, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3379/3649 [==========================>...] - ETA: 1:16 - loss: 6.4988 - accuracy: 0.1190\n",
            "Epoch 00001: loss improved from 6.49881 to 6.49706, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3380/3649 [==========================>...] - ETA: 1:15 - loss: 6.4971 - accuracy: 0.1192WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.188773). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.49706 to 6.49658, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3381/3649 [==========================>...] - ETA: 1:15 - loss: 6.4966 - accuracy: 0.1192WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.297799). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.49658 to 6.49502, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3382/3649 [==========================>...] - ETA: 1:15 - loss: 6.4950 - accuracy: 0.1195WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.297799). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.49502 to 6.49481, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3383/3649 [==========================>...] - ETA: 1:15 - loss: 6.4948 - accuracy: 0.1194WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301648). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.49481 to 6.49414, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3384/3649 [==========================>...] - ETA: 1:14 - loss: 6.4941 - accuracy: 0.1194WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307711). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.49414 to 6.49385, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3385/3649 [==========================>...] - ETA: 1:14 - loss: 6.4939 - accuracy: 0.1194WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307711). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.49385 to 6.49316, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3386/3649 [==========================>...] - ETA: 1:14 - loss: 6.4932 - accuracy: 0.1193WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301648). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.49316\n",
            "3387/3649 [==========================>...] - ETA: 1:14 - loss: 6.4948 - accuracy: 0.1193WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301648). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.49316\n",
            "3388/3649 [==========================>...] - ETA: 1:13 - loss: 6.4965 - accuracy: 0.1192WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296822). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.49316\n",
            "3389/3649 [==========================>...] - ETA: 1:13 - loss: 6.4966 - accuracy: 0.1192WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.199333). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.49316\n",
            "3390/3649 [==========================>...] - ETA: 1:13 - loss: 6.4948 - accuracy: 0.1195\n",
            "Epoch 00001: loss did not improve from 6.49316\n",
            "3391/3649 [==========================>...] - ETA: 1:12 - loss: 6.4957 - accuracy: 0.1194\n",
            "Epoch 00001: loss did not improve from 6.49316\n",
            "3392/3649 [==========================>...] - ETA: 1:12 - loss: 6.4941 - accuracy: 0.1197\n",
            "Epoch 00001: loss improved from 6.49316 to 6.49246, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3393/3649 [==========================>...] - ETA: 1:12 - loss: 6.4925 - accuracy: 0.1200\n",
            "Epoch 00001: loss improved from 6.49246 to 6.49209, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3394/3649 [==========================>...] - ETA: 1:12 - loss: 6.4921 - accuracy: 0.1199\n",
            "Epoch 00001: loss improved from 6.49209 to 6.49172, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3395/3649 [==========================>...] - ETA: 1:11 - loss: 6.4917 - accuracy: 0.1199\n",
            "Epoch 00001: loss did not improve from 6.49172\n",
            "3396/3649 [==========================>...] - ETA: 1:11 - loss: 6.4918 - accuracy: 0.1198\n",
            "Epoch 00001: loss improved from 6.49172 to 6.49010, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3397/3649 [==========================>...] - ETA: 1:11 - loss: 6.4901 - accuracy: 0.1201\n",
            "Epoch 00001: loss did not improve from 6.49010\n",
            "3398/3649 [==========================>...] - ETA: 1:10 - loss: 6.4913 - accuracy: 0.1201\n",
            "Epoch 00001: loss did not improve from 6.49010\n",
            "3399/3649 [==========================>...] - ETA: 1:10 - loss: 6.4915 - accuracy: 0.1200\n",
            "Epoch 00001: loss did not improve from 6.49010\n",
            "3400/3649 [==========================>...] - ETA: 1:10 - loss: 6.4915 - accuracy: 0.1200\n",
            "Epoch 00001: loss improved from 6.49010 to 6.48979, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3401/3649 [==========================>...] - ETA: 1:10 - loss: 6.4898 - accuracy: 0.1203\n",
            "Epoch 00001: loss did not improve from 6.48979\n",
            "3402/3649 [==========================>...] - ETA: 1:09 - loss: 6.4906 - accuracy: 0.1202\n",
            "Epoch 00001: loss improved from 6.48979 to 6.48883, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3403/3649 [==========================>...] - ETA: 1:09 - loss: 6.4888 - accuracy: 0.1205\n",
            "Epoch 00001: loss improved from 6.48883 to 6.48779, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3404/3649 [==========================>...] - ETA: 1:09 - loss: 6.4878 - accuracy: 0.1204\n",
            "Epoch 00001: loss did not improve from 6.48779\n",
            "3405/3649 [==========================>...] - ETA: 1:08 - loss: 6.4878 - accuracy: 0.1204\n",
            "Epoch 00001: loss did not improve from 6.48779\n",
            "3406/3649 [===========================>..] - ETA: 1:08 - loss: 6.4894 - accuracy: 0.1204\n",
            "Epoch 00001: loss did not improve from 6.48779\n",
            "3407/3649 [===========================>..] - ETA: 1:08 - loss: 6.4890 - accuracy: 0.1203\n",
            "Epoch 00001: loss did not improve from 6.48779\n",
            "3408/3649 [===========================>..] - ETA: 1:08 - loss: 6.4880 - accuracy: 0.1203\n",
            "Epoch 00001: loss improved from 6.48779 to 6.48628, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3409/3649 [===========================>..] - ETA: 1:07 - loss: 6.4863 - accuracy: 0.1206\n",
            "Epoch 00001: loss did not improve from 6.48628\n",
            "3410/3649 [===========================>..] - ETA: 1:07 - loss: 6.4864 - accuracy: 0.1205\n",
            "Epoch 00001: loss improved from 6.48628 to 6.48579, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3411/3649 [===========================>..] - ETA: 1:07 - loss: 6.4858 - accuracy: 0.1205\n",
            "Epoch 00001: loss improved from 6.48579 to 6.48400, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3412/3649 [===========================>..] - ETA: 1:06 - loss: 6.4840 - accuracy: 0.1208\n",
            "Epoch 00001: loss improved from 6.48400 to 6.48327, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3413/3649 [===========================>..] - ETA: 1:06 - loss: 6.4833 - accuracy: 0.1207\n",
            "Epoch 00001: loss improved from 6.48327 to 6.48152, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3414/3649 [===========================>..] - ETA: 1:06 - loss: 6.4815 - accuracy: 0.1210\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3415/3649 [===========================>..] - ETA: 1:06 - loss: 6.4831 - accuracy: 0.1209\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3416/3649 [===========================>..] - ETA: 1:05 - loss: 6.4829 - accuracy: 0.1209\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3417/3649 [===========================>..] - ETA: 1:05 - loss: 6.4844 - accuracy: 0.1209\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3418/3649 [===========================>..] - ETA: 1:05 - loss: 6.4859 - accuracy: 0.1208\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3419/3649 [===========================>..] - ETA: 1:05 - loss: 6.4853 - accuracy: 0.1208\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3420/3649 [===========================>..] - ETA: 1:04 - loss: 6.4843 - accuracy: 0.1208\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3421/3649 [===========================>..] - ETA: 1:04 - loss: 6.4850 - accuracy: 0.1207\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3422/3649 [===========================>..] - ETA: 1:04 - loss: 6.4834 - accuracy: 0.1210\n",
            "Epoch 00001: loss did not improve from 6.48152\n",
            "3423/3649 [===========================>..] - ETA: 1:03 - loss: 6.4831 - accuracy: 0.1209\n",
            "Epoch 00001: loss improved from 6.48152 to 6.48150, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3424/3649 [===========================>..] - ETA: 1:03 - loss: 6.4815 - accuracy: 0.1212\n",
            "Epoch 00001: loss improved from 6.48150 to 6.48126, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3425/3649 [===========================>..] - ETA: 1:03 - loss: 6.4813 - accuracy: 0.1212\n",
            "Epoch 00001: loss did not improve from 6.48126\n",
            "3426/3649 [===========================>..] - ETA: 1:03 - loss: 6.4817 - accuracy: 0.1211\n",
            "Epoch 00001: loss improved from 6.48126 to 6.48023, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3427/3649 [===========================>..] - ETA: 1:02 - loss: 6.4802 - accuracy: 0.1211\n",
            "Epoch 00001: loss improved from 6.48023 to 6.47954, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3428/3649 [===========================>..] - ETA: 1:02 - loss: 6.4795 - accuracy: 0.1211\n",
            "Epoch 00001: loss improved from 6.47954 to 6.47776, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3429/3649 [===========================>..] - ETA: 1:02 - loss: 6.4778 - accuracy: 0.1213\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3430/3649 [===========================>..] - ETA: 1:01 - loss: 6.4786 - accuracy: 0.1213\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3431/3649 [===========================>..] - ETA: 1:01 - loss: 6.4779 - accuracy: 0.1212\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3432/3649 [===========================>..] - ETA: 1:01 - loss: 6.4781 - accuracy: 0.1212\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3433/3649 [===========================>..] - ETA: 1:01 - loss: 6.4778 - accuracy: 0.1212\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3434/3649 [===========================>..] - ETA: 1:00 - loss: 6.4792 - accuracy: 0.1211\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3435/3649 [===========================>..] - ETA: 1:00 - loss: 6.4804 - accuracy: 0.1211\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3436/3649 [===========================>..] - ETA: 1:00 - loss: 6.4796 - accuracy: 0.1211\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3437/3649 [===========================>..] - ETA: 59s - loss: 6.4778 - accuracy: 0.1213 \n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3438/3649 [===========================>..] - ETA: 59s - loss: 6.4783 - accuracy: 0.1213\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3439/3649 [===========================>..] - ETA: 59s - loss: 6.4798 - accuracy: 0.1213\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3440/3649 [===========================>..] - ETA: 59s - loss: 6.4804 - accuracy: 0.1212\n",
            "Epoch 00001: loss did not improve from 6.47776\n",
            "3441/3649 [===========================>..] - ETA: 58s - loss: 6.4794 - accuracy: 0.1212\n",
            "Epoch 00001: loss improved from 6.47776 to 6.47765, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3442/3649 [===========================>..] - ETA: 58s - loss: 6.4777 - accuracy: 0.1214\n",
            "Epoch 00001: loss improved from 6.47765 to 6.47739, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3443/3649 [===========================>..] - ETA: 58s - loss: 6.4774 - accuracy: 0.1214\n",
            "Epoch 00001: loss improved from 6.47739 to 6.47573, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3444/3649 [===========================>..] - ETA: 57s - loss: 6.4757 - accuracy: 0.1217\n",
            "Epoch 00001: loss improved from 6.47573 to 6.47552, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3445/3649 [===========================>..] - ETA: 57s - loss: 6.4755 - accuracy: 0.1216\n",
            "Epoch 00001: loss improved from 6.47552 to 6.47451, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3446/3649 [===========================>..] - ETA: 57s - loss: 6.4745 - accuracy: 0.1216\n",
            "Epoch 00001: loss improved from 6.47451 to 6.47421, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3447/3649 [===========================>..] - ETA: 57s - loss: 6.4742 - accuracy: 0.1216\n",
            "Epoch 00001: loss improved from 6.47421 to 6.47242, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3448/3649 [===========================>..] - ETA: 56s - loss: 6.4724 - accuracy: 0.1218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292622). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.47242 to 6.47216, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3449/3649 [===========================>..] - ETA: 56s - loss: 6.4722 - accuracy: 0.1218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301080). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.47216\n",
            "3450/3649 [===========================>..] - ETA: 56s - loss: 6.4724 - accuracy: 0.1217WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301080). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.47216 to 6.47070, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3451/3649 [===========================>..] - ETA: 56s - loss: 6.4707 - accuracy: 0.1220WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301080). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.47070 to 6.47056, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3452/3649 [===========================>..] - ETA: 55s - loss: 6.4706 - accuracy: 0.1220WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.317417). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.47056 to 6.46961, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3453/3649 [===========================>..] - ETA: 55s - loss: 6.4696 - accuracy: 0.1219WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309584). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46961 to 6.46858, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3454/3649 [===========================>..] - ETA: 55s - loss: 6.4686 - accuracy: 0.1219WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309584). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46858 to 6.46810, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3455/3649 [===========================>..] - ETA: 54s - loss: 6.4681 - accuracy: 0.1219WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.309584). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46810 to 6.46671, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3456/3649 [===========================>..] - ETA: 54s - loss: 6.4667 - accuracy: 0.1218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.299780). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46671 to 6.46629, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3457/3649 [===========================>..] - ETA: 54s - loss: 6.4663 - accuracy: 0.1218WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.294435). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.46629\n",
            "3458/3649 [===========================>..] - ETA: 54s - loss: 6.4671 - accuracy: 0.1217WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.289693). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46629 to 6.46532, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3459/3649 [===========================>..] - ETA: 53s - loss: 6.4653 - accuracy: 0.1220WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.254766). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46532 to 6.46436, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3460/3649 [===========================>..] - ETA: 53s - loss: 6.4644 - accuracy: 0.1223WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46436 to 6.46285, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3461/3649 [===========================>..] - ETA: 53s - loss: 6.4628 - accuracy: 0.1225WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298534). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46285 to 6.46204, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3462/3649 [===========================>..] - ETA: 53s - loss: 6.4620 - accuracy: 0.1228WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298534). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46204 to 6.46050, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3463/3649 [===========================>..] - ETA: 52s - loss: 6.4605 - accuracy: 0.1230WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298534). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.46050\n",
            "3464/3649 [===========================>..] - ETA: 52s - loss: 6.4609 - accuracy: 0.1230WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.46050 to 6.45927, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3465/3649 [===========================>..] - ETA: 52s - loss: 6.4593 - accuracy: 0.1232WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298534). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.45927 to 6.45855, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3466/3649 [===========================>..] - ETA: 51s - loss: 6.4585 - accuracy: 0.1232WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.305877). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.45855 to 6.45800, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3467/3649 [===========================>..] - ETA: 51s - loss: 6.4580 - accuracy: 0.1232WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307808). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.45800\n",
            "3468/3649 [===========================>..] - ETA: 51s - loss: 6.4582 - accuracy: 0.1231WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307808). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.45800\n",
            "3469/3649 [===========================>..] - ETA: 51s - loss: 6.4598 - accuracy: 0.1231WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.307808). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.45800\n",
            "3470/3649 [===========================>..] - ETA: 50s - loss: 6.4581 - accuracy: 0.1233WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.301404). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.45800 to 6.45754, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3471/3649 [===========================>..] - ETA: 50s - loss: 6.4575 - accuracy: 0.1233WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192261). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.45754\n",
            "3472/3649 [===========================>..] - ETA: 50s - loss: 6.4576 - accuracy: 0.1233\n",
            "Epoch 00001: loss improved from 6.45754 to 6.45595, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3473/3649 [===========================>..] - ETA: 49s - loss: 6.4560 - accuracy: 0.1235\n",
            "Epoch 00001: loss improved from 6.45595 to 6.45485, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3474/3649 [===========================>..] - ETA: 49s - loss: 6.4548 - accuracy: 0.1235WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192261). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.45485 to 6.45358, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3475/3649 [===========================>..] - ETA: 49s - loss: 6.4536 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.45358\n",
            "3476/3649 [===========================>..] - ETA: 49s - loss: 6.4538 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.45358\n",
            "3477/3649 [===========================>..] - ETA: 48s - loss: 6.4536 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.45358\n",
            "3478/3649 [===========================>..] - ETA: 48s - loss: 6.4539 - accuracy: 0.1236\n",
            "Epoch 00001: loss improved from 6.45358 to 6.45285, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3479/3649 [===========================>..] - ETA: 48s - loss: 6.4528 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.45285\n",
            "3480/3649 [===========================>..] - ETA: 48s - loss: 6.4546 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.45285\n",
            "3481/3649 [===========================>..] - ETA: 47s - loss: 6.4529 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.45285\n",
            "3482/3649 [===========================>..] - ETA: 47s - loss: 6.4544 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.45285\n",
            "3483/3649 [===========================>..] - ETA: 47s - loss: 6.4539 - accuracy: 0.1237\n",
            "Epoch 00001: loss improved from 6.45285 to 6.45278, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3484/3649 [===========================>..] - ETA: 46s - loss: 6.4528 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3485/3649 [===========================>..] - ETA: 46s - loss: 6.4544 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3486/3649 [===========================>..] - ETA: 46s - loss: 6.4558 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3487/3649 [===========================>..] - ETA: 45s - loss: 6.4571 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3488/3649 [===========================>..] - ETA: 45s - loss: 6.4576 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3489/3649 [===========================>..] - ETA: 45s - loss: 6.4579 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3490/3649 [===========================>..] - ETA: 45s - loss: 6.4590 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3491/3649 [===========================>..] - ETA: 44s - loss: 6.4603 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3492/3649 [===========================>..] - ETA: 44s - loss: 6.4615 - accuracy: 0.1234\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3493/3649 [===========================>..] - ETA: 44s - loss: 6.4601 - accuracy: 0.1234\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3494/3649 [===========================>..] - ETA: 43s - loss: 6.4614 - accuracy: 0.1234\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3495/3649 [===========================>..] - ETA: 43s - loss: 6.4611 - accuracy: 0.1233\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3496/3649 [===========================>..] - ETA: 43s - loss: 6.4614 - accuracy: 0.1233\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3497/3649 [===========================>..] - ETA: 43s - loss: 6.4596 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3498/3649 [===========================>..] - ETA: 42s - loss: 6.4611 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3499/3649 [===========================>..] - ETA: 42s - loss: 6.4622 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3500/3649 [===========================>..] - ETA: 42s - loss: 6.4619 - accuracy: 0.1234\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3501/3649 [===========================>..] - ETA: 41s - loss: 6.4612 - accuracy: 0.1234\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3502/3649 [===========================>..] - ETA: 41s - loss: 6.4601 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3503/3649 [===========================>..] - ETA: 41s - loss: 6.4587 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3504/3649 [===========================>..] - ETA: 41s - loss: 6.4577 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3505/3649 [===========================>..] - ETA: 40s - loss: 6.4566 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3506/3649 [===========================>..] - ETA: 40s - loss: 6.4555 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3507/3649 [===========================>..] - ETA: 40s - loss: 6.4544 - accuracy: 0.1240\n",
            "Epoch 00001: loss did not improve from 6.45278\n",
            "3508/3649 [===========================>..] - ETA: 39s - loss: 6.4532 - accuracy: 0.1240\n",
            "Epoch 00001: loss improved from 6.45278 to 6.45214, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3509/3649 [===========================>..] - ETA: 39s - loss: 6.4521 - accuracy: 0.1243\n",
            "Epoch 00001: loss improved from 6.45214 to 6.45084, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3510/3649 [===========================>..] - ETA: 39s - loss: 6.4508 - accuracy: 0.1245\n",
            "Epoch 00001: loss improved from 6.45084 to 6.44975, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3511/3649 [===========================>..] - ETA: 39s - loss: 6.4497 - accuracy: 0.1245\n",
            "Epoch 00001: loss improved from 6.44975 to 6.44877, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3512/3649 [===========================>..] - ETA: 38s - loss: 6.4488 - accuracy: 0.1244\n",
            "Epoch 00001: loss improved from 6.44877 to 6.44759, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3513/3649 [===========================>..] - ETA: 38s - loss: 6.4476 - accuracy: 0.1244\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3514/3649 [===========================>..] - ETA: 38s - loss: 6.4487 - accuracy: 0.1244\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3515/3649 [===========================>..] - ETA: 37s - loss: 6.4498 - accuracy: 0.1243\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3516/3649 [===========================>..] - ETA: 37s - loss: 6.4500 - accuracy: 0.1243\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3517/3649 [===========================>..] - ETA: 37s - loss: 6.4509 - accuracy: 0.1243\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3518/3649 [===========================>..] - ETA: 37s - loss: 6.4508 - accuracy: 0.1242\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3519/3649 [===========================>..] - ETA: 36s - loss: 6.4499 - accuracy: 0.1242\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3520/3649 [===========================>..] - ETA: 36s - loss: 6.4503 - accuracy: 0.1241\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3521/3649 [===========================>..] - ETA: 36s - loss: 6.4498 - accuracy: 0.1241\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3522/3649 [===========================>..] - ETA: 35s - loss: 6.4488 - accuracy: 0.1241\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3523/3649 [===========================>..] - ETA: 35s - loss: 6.4488 - accuracy: 0.1240\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3524/3649 [===========================>..] - ETA: 35s - loss: 6.4484 - accuracy: 0.1240\n",
            "Epoch 00001: loss did not improve from 6.44759\n",
            "3525/3649 [===========================>..] - ETA: 35s - loss: 6.4476 - accuracy: 0.1240\n",
            "Epoch 00001: loss improved from 6.44759 to 6.44688, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3526/3649 [===========================>..] - ETA: 34s - loss: 6.4469 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.44688\n",
            "3527/3649 [===========================>..] - ETA: 34s - loss: 6.4470 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.44688\n",
            "3528/3649 [============================>.] - ETA: 34s - loss: 6.4481 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.44688\n",
            "3529/3649 [============================>.] - ETA: 33s - loss: 6.4472 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44688\n",
            "3530/3649 [============================>.] - ETA: 33s - loss: 6.4473 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44688\n",
            "3531/3649 [============================>.] - ETA: 33s - loss: 6.4470 - accuracy: 0.1238\n",
            "Epoch 00001: loss improved from 6.44688 to 6.44661, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3532/3649 [============================>.] - ETA: 33s - loss: 6.4466 - accuracy: 0.1237\n",
            "Epoch 00001: loss improved from 6.44661 to 6.44613, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3533/3649 [============================>.] - ETA: 32s - loss: 6.4461 - accuracy: 0.1237\n",
            "Epoch 00001: loss improved from 6.44613 to 6.44548, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3534/3649 [============================>.] - ETA: 32s - loss: 6.4455 - accuracy: 0.1237\n",
            "Epoch 00001: loss improved from 6.44548 to 6.44515, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3535/3649 [============================>.] - ETA: 32s - loss: 6.4452 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44515\n",
            "3536/3649 [============================>.] - ETA: 32s - loss: 6.4466 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44515\n",
            "3537/3649 [============================>.] - ETA: 31s - loss: 6.4457 - accuracy: 0.1236\n",
            "Epoch 00001: loss improved from 6.44515 to 6.44478, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3538/3649 [============================>.] - ETA: 31s - loss: 6.4448 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.44478\n",
            "3539/3649 [============================>.] - ETA: 31s - loss: 6.4449 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.44478\n",
            "3540/3649 [============================>.] - ETA: 30s - loss: 6.4463 - accuracy: 0.1234\n",
            "Epoch 00001: loss improved from 6.44478 to 6.44468, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3541/3649 [============================>.] - ETA: 30s - loss: 6.4447 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44468\n",
            "3542/3649 [============================>.] - ETA: 30s - loss: 6.4453 - accuracy: 0.1237\n",
            "Epoch 00001: loss improved from 6.44468 to 6.44412, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3543/3649 [============================>.] - ETA: 30s - loss: 6.4441 - accuracy: 0.1239\n",
            "Epoch 00001: loss improved from 6.44412 to 6.44408, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3544/3649 [============================>.] - ETA: 29s - loss: 6.4441 - accuracy: 0.1239\n",
            "Epoch 00001: loss improved from 6.44408 to 6.44332, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3545/3649 [============================>.] - ETA: 29s - loss: 6.4433 - accuracy: 0.1238\n",
            "Epoch 00001: loss improved from 6.44332 to 6.44274, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3546/3649 [============================>.] - ETA: 29s - loss: 6.4427 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3547/3649 [============================>.] - ETA: 28s - loss: 6.4441 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3548/3649 [============================>.] - ETA: 28s - loss: 6.4432 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3549/3649 [============================>.] - ETA: 28s - loss: 6.4445 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3550/3649 [============================>.] - ETA: 28s - loss: 6.4457 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3551/3649 [============================>.] - ETA: 27s - loss: 6.4444 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3552/3649 [============================>.] - ETA: 27s - loss: 6.4446 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3553/3649 [============================>.] - ETA: 27s - loss: 6.4437 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3554/3649 [============================>.] - ETA: 26s - loss: 6.4437 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3555/3649 [============================>.] - ETA: 26s - loss: 6.4448 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3556/3649 [============================>.] - ETA: 26s - loss: 6.4437 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3557/3649 [============================>.] - ETA: 26s - loss: 6.4440 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3558/3649 [============================>.] - ETA: 25s - loss: 6.4452 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3559/3649 [============================>.] - ETA: 25s - loss: 6.4464 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3560/3649 [============================>.] - ETA: 25s - loss: 6.4454 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3561/3649 [============================>.] - ETA: 24s - loss: 6.4445 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3562/3649 [============================>.] - ETA: 24s - loss: 6.4446 - accuracy: 0.1235\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3563/3649 [============================>.] - ETA: 24s - loss: 6.4430 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3564/3649 [============================>.] - ETA: 24s - loss: 6.4442 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3565/3649 [============================>.] - ETA: 23s - loss: 6.4436 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3566/3649 [============================>.] - ETA: 23s - loss: 6.4448 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3567/3649 [============================>.] - ETA: 23s - loss: 6.4461 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3568/3649 [============================>.] - ETA: 22s - loss: 6.4462 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3569/3649 [============================>.] - ETA: 22s - loss: 6.4454 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3570/3649 [============================>.] - ETA: 22s - loss: 6.4444 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3571/3649 [============================>.] - ETA: 22s - loss: 6.4455 - accuracy: 0.1238\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3572/3649 [============================>.] - ETA: 21s - loss: 6.4458 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3573/3649 [============================>.] - ETA: 21s - loss: 6.4462 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3574/3649 [============================>.] - ETA: 21s - loss: 6.4462 - accuracy: 0.1237\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3575/3649 [============================>.] - ETA: 20s - loss: 6.4458 - accuracy: 0.1236\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3576/3649 [============================>.] - ETA: 20s - loss: 6.4446 - accuracy: 0.1239\n",
            "Epoch 00001: loss did not improve from 6.44274\n",
            "3577/3649 [============================>.] - ETA: 20s - loss: 6.4431 - accuracy: 0.1241\n",
            "Epoch 00001: loss improved from 6.44274 to 6.44165, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3578/3649 [============================>.] - ETA: 20s - loss: 6.4417 - accuracy: 0.1244\n",
            "Epoch 00001: loss improved from 6.44165 to 6.44016, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3579/3649 [============================>.] - ETA: 19s - loss: 6.4402 - accuracy: 0.1246\n",
            "Epoch 00001: loss improved from 6.44016 to 6.43866, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3580/3649 [============================>.] - ETA: 19s - loss: 6.4387 - accuracy: 0.1249\n",
            "Epoch 00001: loss improved from 6.43866 to 6.43714, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3581/3649 [============================>.] - ETA: 19s - loss: 6.4371 - accuracy: 0.1251\n",
            "Epoch 00001: loss improved from 6.43714 to 6.43559, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3582/3649 [============================>.] - ETA: 18s - loss: 6.4356 - accuracy: 0.1253\n",
            "Epoch 00001: loss improved from 6.43559 to 6.43398, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3583/3649 [============================>.] - ETA: 18s - loss: 6.4340 - accuracy: 0.1256WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.189562). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.43398 to 6.43231, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3584/3649 [============================>.] - ETA: 18s - loss: 6.4323 - accuracy: 0.1258WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.43231\n",
            "3585/3649 [============================>.] - ETA: 18s - loss: 6.4348 - accuracy: 0.1258WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.43231\n",
            "3586/3649 [============================>.] - ETA: 17s - loss: 6.4357 - accuracy: 0.1258WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.43231\n",
            "3587/3649 [============================>.] - ETA: 17s - loss: 6.4364 - accuracy: 0.1257WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.43231\n",
            "3588/3649 [============================>.] - ETA: 17s - loss: 6.4348 - accuracy: 0.1260WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286852). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.43231\n",
            "3589/3649 [============================>.] - ETA: 16s - loss: 6.4331 - accuracy: 0.1262\n",
            "Epoch 00001: loss improved from 6.43231 to 6.43152, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3590/3649 [============================>.] - ETA: 16s - loss: 6.4315 - accuracy: 0.1265\n",
            "Epoch 00001: loss improved from 6.43152 to 6.42988, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3591/3649 [============================>.] - ETA: 16s - loss: 6.4299 - accuracy: 0.1267\n",
            "Epoch 00001: loss improved from 6.42988 to 6.42820, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3592/3649 [============================>.] - ETA: 16s - loss: 6.4282 - accuracy: 0.1269\n",
            "Epoch 00001: loss improved from 6.42820 to 6.42649, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3593/3649 [============================>.] - ETA: 15s - loss: 6.4265 - accuracy: 0.1272\n",
            "Epoch 00001: loss improved from 6.42649 to 6.42476, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3594/3649 [============================>.] - ETA: 15s - loss: 6.4248 - accuracy: 0.1274\n",
            "Epoch 00001: loss improved from 6.42476 to 6.42302, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3595/3649 [============================>.] - ETA: 15s - loss: 6.4230 - accuracy: 0.1277\n",
            "Epoch 00001: loss improved from 6.42302 to 6.42128, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3596/3649 [============================>.] - ETA: 15s - loss: 6.4213 - accuracy: 0.1279WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296004). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.42128 to 6.41953, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3597/3649 [============================>.] - ETA: 14s - loss: 6.4195 - accuracy: 0.1282WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296004). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.41953 to 6.41778, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3598/3649 [============================>.] - ETA: 14s - loss: 6.4178 - accuracy: 0.1284WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296915). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.41778 to 6.41602, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3599/3649 [============================>.] - ETA: 14s - loss: 6.4160 - accuracy: 0.1286WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298871). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.41602 to 6.41590, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3600/3649 [============================>.] - ETA: 13s - loss: 6.4159 - accuracy: 0.1286WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.300730). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.41590 to 6.41587, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3601/3649 [============================>.] - ETA: 13s - loss: 6.4159 - accuracy: 0.1286WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298871). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3602/3649 [============================>.] - ETA: 13s - loss: 6.4165 - accuracy: 0.1285WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.298871). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3603/3649 [============================>.] - ETA: 13s - loss: 6.4182 - accuracy: 0.1285WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.296180). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3604/3649 [============================>.] - ETA: 12s - loss: 6.4186 - accuracy: 0.1285WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.293998). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3605/3649 [============================>.] - ETA: 12s - loss: 6.4187 - accuracy: 0.1284WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.287756). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3606/3649 [============================>.] - ETA: 12s - loss: 6.4181 - accuracy: 0.1284\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3607/3649 [============================>.] - ETA: 11s - loss: 6.4181 - accuracy: 0.1284\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3608/3649 [============================>.] - ETA: 11s - loss: 6.4182 - accuracy: 0.1283\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3609/3649 [============================>.] - ETA: 11s - loss: 6.4167 - accuracy: 0.1286\n",
            "Epoch 00001: loss did not improve from 6.41587\n",
            "3610/3649 [============================>.] - ETA: 11s - loss: 6.4170 - accuracy: 0.1285\n",
            "Epoch 00001: loss improved from 6.41587 to 6.41577, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3611/3649 [============================>.] - ETA: 10s - loss: 6.4158 - accuracy: 0.1288\n",
            "Epoch 00001: loss improved from 6.41577 to 6.41487, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3612/3649 [============================>.] - ETA: 10s - loss: 6.4149 - accuracy: 0.1287\n",
            "Epoch 00001: loss did not improve from 6.41487\n",
            "3613/3649 [============================>.] - ETA: 10s - loss: 6.4161 - accuracy: 0.1287\n",
            "Epoch 00001: loss did not improve from 6.41487\n",
            "3614/3649 [============================>.] - ETA: 9s - loss: 6.4149 - accuracy: 0.1289 \n",
            "Epoch 00001: loss improved from 6.41487 to 6.41373, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3615/3649 [============================>.] - ETA: 9s - loss: 6.4137 - accuracy: 0.1292\n",
            "Epoch 00001: loss did not improve from 6.41373\n",
            "3616/3649 [============================>.] - ETA: 9s - loss: 6.4138 - accuracy: 0.1291\n",
            "Epoch 00001: loss did not improve from 6.41373\n",
            "3617/3649 [============================>.] - ETA: 9s - loss: 6.4141 - accuracy: 0.1291\n",
            "Epoch 00001: loss improved from 6.41373 to 6.41300, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3618/3649 [============================>.] - ETA: 8s - loss: 6.4130 - accuracy: 0.1291\n",
            "Epoch 00001: loss improved from 6.41300 to 6.41209, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3619/3649 [============================>.] - ETA: 8s - loss: 6.4121 - accuracy: 0.1290\n",
            "Epoch 00001: loss did not improve from 6.41209\n",
            "3620/3649 [============================>.] - ETA: 8s - loss: 6.4121 - accuracy: 0.1290\n",
            "Epoch 00001: loss improved from 6.41209 to 6.41167, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3621/3649 [============================>.] - ETA: 7s - loss: 6.4117 - accuracy: 0.1290\n",
            "Epoch 00001: loss improved from 6.41167 to 6.41037, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3622/3649 [============================>.] - ETA: 7s - loss: 6.4104 - accuracy: 0.1292\n",
            "Epoch 00001: loss improved from 6.41037 to 6.40888, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3623/3649 [============================>.] - ETA: 7s - loss: 6.4089 - accuracy: 0.1295\n",
            "Epoch 00001: loss improved from 6.40888 to 6.40731, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3624/3649 [============================>.] - ETA: 7s - loss: 6.4073 - accuracy: 0.1297\n",
            "Epoch 00001: loss improved from 6.40731 to 6.40577, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3625/3649 [============================>.] - ETA: 6s - loss: 6.4058 - accuracy: 0.1299\n",
            "Epoch 00001: loss improved from 6.40577 to 6.40424, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3626/3649 [============================>.] - ETA: 6s - loss: 6.4042 - accuracy: 0.1302WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271888). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.40424 to 6.40272, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3627/3649 [============================>.] - ETA: 6s - loss: 6.4027 - accuracy: 0.1304WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292192). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.40272 to 6.40118, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3628/3649 [============================>.] - ETA: 5s - loss: 6.4012 - accuracy: 0.1307WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292282). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.40118 to 6.39962, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3629/3649 [============================>.] - ETA: 5s - loss: 6.3996 - accuracy: 0.1309WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292282). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.39962 to 6.39804, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3630/3649 [============================>.] - ETA: 5s - loss: 6.3980 - accuracy: 0.1311WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302242). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3631/3649 [============================>.] - ETA: 5s - loss: 6.3998 - accuracy: 0.1311WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.302242). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3632/3649 [============================>.] - ETA: 4s - loss: 6.3993 - accuracy: 0.1311WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.292282). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3633/3649 [============================>.] - ETA: 4s - loss: 6.4010 - accuracy: 0.1310WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.286031). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3634/3649 [============================>.] - ETA: 4s - loss: 6.4027 - accuracy: 0.1310WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.271888). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3635/3649 [============================>.] - ETA: 3s - loss: 6.4029 - accuracy: 0.1309\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3636/3649 [============================>.] - ETA: 3s - loss: 6.4044 - accuracy: 0.1309\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3637/3649 [============================>.] - ETA: 3s - loss: 6.4029 - accuracy: 0.1312\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3638/3649 [============================>.] - ETA: 3s - loss: 6.4013 - accuracy: 0.1314\n",
            "Epoch 00001: loss did not improve from 6.39804\n",
            "3639/3649 [============================>.] - ETA: 2s - loss: 6.3997 - accuracy: 0.1316\n",
            "Epoch 00001: loss improved from 6.39804 to 6.39798, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3640/3649 [============================>.] - ETA: 2s - loss: 6.3980 - accuracy: 0.1319\n",
            "Epoch 00001: loss improved from 6.39798 to 6.39631, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3641/3649 [============================>.] - ETA: 2s - loss: 6.3963 - accuracy: 0.1321\n",
            "Epoch 00001: loss improved from 6.39631 to 6.39464, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3642/3649 [============================>.] - ETA: 1s - loss: 6.3946 - accuracy: 0.1323\n",
            "Epoch 00001: loss improved from 6.39464 to 6.39297, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3643/3649 [============================>.] - ETA: 1s - loss: 6.3930 - accuracy: 0.1326\n",
            "Epoch 00001: loss improved from 6.39297 to 6.39131, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3644/3649 [============================>.] - ETA: 1s - loss: 6.3913 - accuracy: 0.1328\n",
            "Epoch 00001: loss improved from 6.39131 to 6.38965, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3645/3649 [============================>.] - ETA: 1s - loss: 6.3896 - accuracy: 0.1331WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.192407). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.38965 to 6.38797, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3646/3649 [============================>.] - ETA: 0s - loss: 6.3880 - accuracy: 0.1333WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.312403). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.38797 to 6.38629, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3647/3649 [============================>.] - ETA: 0s - loss: 6.3863 - accuracy: 0.1335WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.312403). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss improved from 6.38629 to 6.38460, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "3648/3649 [============================>.] - ETA: 0s - loss: 6.3846 - accuracy: 0.1338WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.312403). Check your callbacks.\n",
            "\n",
            "Epoch 00001: loss did not improve from 6.38460\n",
            "3649/3649 [==============================] - ETA: 0s - loss: 6.3860 - accuracy: 0.1337WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.312403). Check your callbacks.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy\n",
            "3649/3649 [==============================] - 1038s 285ms/step - loss: 6.3860 - accuracy: 0.1337\n",
            "Epoch 2/2\n",
            "\n",
            "Epoch 00002: loss did not improve from 6.38460\n",
            "   1/3649 [..............................] - ETA: 0s - loss: 8.4997 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss did not improve from 6.38460\n",
            "   2/3649 [..............................] - ETA: 6:04 - loss: 8.9885 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss did not improve from 6.38460\n",
            "   3/3649 [..............................] - ETA: 7:57 - loss: 8.1511 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss did not improve from 6.38460\n",
            "   4/3649 [..............................] - ETA: 8:55 - loss: 6.6425 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss improved from 6.38460 to 6.35569, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "   5/3649 [..............................] - ETA: 10:28 - loss: 6.3557 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss improved from 6.35569 to 6.08404, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "   6/3649 [..............................] - ETA: 14:47 - loss: 6.0840 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss did not improve from 6.08404\n",
            "   7/3649 [..............................] - ETA: 14:27 - loss: 6.7113 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss did not improve from 6.08404\n",
            "   8/3649 [..............................] - ETA: 14:10 - loss: 6.6042 - accuracy: 0.0000e+00\n",
            "Epoch 00002: loss improved from 6.08404 to 5.97679, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "   9/3649 [..............................] - ETA: 14:29 - loss: 5.9768 - accuracy: 0.1111    \n",
            "Epoch 00002: loss improved from 5.97679 to 5.93191, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "  10/3649 [..............................] - ETA: 16:37 - loss: 5.9319 - accuracy: 0.1000\n",
            "Epoch 00002: loss improved from 5.93191 to 5.53268, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "  11/3649 [..............................] - ETA: 17:54 - loss: 5.5327 - accuracy: 0.1818\n",
            "Epoch 00002: loss improved from 5.53268 to 5.21547, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "  12/3649 [..............................] - ETA: 18:54 - loss: 5.2155 - accuracy: 0.1667\n",
            "Epoch 00002: loss improved from 5.21547 to 4.93138, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "  13/3649 [..............................] - ETA: 19:52 - loss: 4.9314 - accuracy: 0.2308\n",
            "Epoch 00002: loss did not improve from 4.93138\n",
            "  14/3649 [..............................] - ETA: 19:19 - loss: 5.0423 - accuracy: 0.2143\n",
            "Epoch 00002: loss did not improve from 4.93138\n",
            "  15/3649 [..............................] - ETA: 18:50 - loss: 5.0543 - accuracy: 0.2000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.189126). Check your callbacks.\n",
            "\n",
            "Epoch 00002: loss did not improve from 4.93138\n",
            "  16/3649 [..............................] - ETA: 18:24 - loss: 5.3993 - accuracy: 0.1875\n",
            "Epoch 00002: loss did not improve from 4.93138\n",
            "  17/3649 [..............................] - ETA: 18:05 - loss: 5.1289 - accuracy: 0.2353\n",
            "Epoch 00002: loss did not improve from 4.93138\n",
            "  18/3649 [..............................] - ETA: 17:43 - loss: 5.0937 - accuracy: 0.2222\n",
            "Epoch 00002: loss improved from 4.93138 to 4.92882, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "  19/3649 [..............................] - ETA: 17:38 - loss: 4.9288 - accuracy: 0.2105\n",
            "Epoch 00002: loss improved from 4.92882 to 4.76775, saving model to /content/drive/My Drive/CookingWithBert/models/word_rnn_weights.hdf5\n",
            "  20/3649 [..............................] - ETA: 18:30 - loss: 4.7677 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  21/3649 [..............................] - ETA: 18:12 - loss: 4.8227 - accuracy: 0.2381\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  22/3649 [..............................] - ETA: 17:55 - loss: 5.0885 - accuracy: 0.2273\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  23/3649 [..............................] - ETA: 17:39 - loss: 4.9127 - accuracy: 0.2609\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  24/3649 [..............................] - ETA: 17:24 - loss: 4.9242 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  25/3649 [..............................] - ETA: 17:10 - loss: 4.9468 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  26/3649 [..............................] - ETA: 16:58 - loss: 5.0018 - accuracy: 0.2308\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  27/3649 [..............................] - ETA: 16:45 - loss: 4.8356 - accuracy: 0.2593\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  28/3649 [..............................] - ETA: 16:34 - loss: 4.7980 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  29/3649 [..............................] - ETA: 16:24 - loss: 4.9913 - accuracy: 0.2414\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  30/3649 [..............................] - ETA: 16:15 - loss: 4.8385 - accuracy: 0.2667\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  31/3649 [..............................] - ETA: 16:06 - loss: 4.8581 - accuracy: 0.2581\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  32/3649 [..............................] - ETA: 15:58 - loss: 4.8908 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  33/3649 [..............................] - ETA: 15:49 - loss: 4.8467 - accuracy: 0.2424\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  34/3649 [..............................] - ETA: 15:41 - loss: 5.0415 - accuracy: 0.2353\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  35/3649 [..............................] - ETA: 15:35 - loss: 5.0586 - accuracy: 0.2286\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  36/3649 [..............................] - ETA: 15:29 - loss: 5.2485 - accuracy: 0.2222\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  37/3649 [..............................] - ETA: 15:22 - loss: 5.2531 - accuracy: 0.2162\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  38/3649 [..............................] - ETA: 15:16 - loss: 5.4198 - accuracy: 0.2105\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  39/3649 [..............................] - ETA: 15:10 - loss: 5.4171 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  40/3649 [..............................] - ETA: 15:05 - loss: 5.4254 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  41/3649 [..............................] - ETA: 15:00 - loss: 5.4348 - accuracy: 0.1951\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  42/3649 [..............................] - ETA: 14:55 - loss: 5.3122 - accuracy: 0.2143\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  43/3649 [..............................] - ETA: 14:50 - loss: 5.3628 - accuracy: 0.2093\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  44/3649 [..............................] - ETA: 14:45 - loss: 5.3426 - accuracy: 0.2045\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  45/3649 [..............................] - ETA: 14:41 - loss: 5.3557 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  46/3649 [..............................] - ETA: 14:37 - loss: 5.2441 - accuracy: 0.2174\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  47/3649 [..............................] - ETA: 14:33 - loss: 5.3649 - accuracy: 0.2128\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  48/3649 [..............................] - ETA: 14:29 - loss: 5.3430 - accuracy: 0.2083\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  49/3649 [..............................] - ETA: 14:25 - loss: 5.4488 - accuracy: 0.2041\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  50/3649 [..............................] - ETA: 14:21 - loss: 5.5380 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  51/3649 [..............................] - ETA: 14:18 - loss: 5.4462 - accuracy: 0.2157\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  52/3649 [..............................] - ETA: 14:14 - loss: 5.4493 - accuracy: 0.2115\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  53/3649 [..............................] - ETA: 14:11 - loss: 5.4826 - accuracy: 0.2075\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  54/3649 [..............................] - ETA: 14:08 - loss: 5.4958 - accuracy: 0.2037\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  55/3649 [..............................] - ETA: 14:05 - loss: 5.5108 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  56/3649 [..............................] - ETA: 14:02 - loss: 5.4722 - accuracy: 0.2143\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  57/3649 [..............................] - ETA: 13:59 - loss: 5.3787 - accuracy: 0.2281\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  58/3649 [..............................] - ETA: 13:57 - loss: 5.3824 - accuracy: 0.2241\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  59/3649 [..............................] - ETA: 13:54 - loss: 5.3494 - accuracy: 0.2203\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  60/3649 [..............................] - ETA: 13:51 - loss: 5.4318 - accuracy: 0.2167\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  61/3649 [..............................] - ETA: 13:49 - loss: 5.3466 - accuracy: 0.2295\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  62/3649 [..............................] - ETA: 13:48 - loss: 5.3474 - accuracy: 0.2258\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  63/3649 [..............................] - ETA: 13:46 - loss: 5.3269 - accuracy: 0.2222\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  64/3649 [..............................] - ETA: 13:44 - loss: 5.3141 - accuracy: 0.2188\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  65/3649 [..............................] - ETA: 13:42 - loss: 5.3289 - accuracy: 0.2154\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  66/3649 [..............................] - ETA: 13:40 - loss: 5.3416 - accuracy: 0.2121\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  67/3649 [..............................] - ETA: 13:38 - loss: 5.3746 - accuracy: 0.2090\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  68/3649 [..............................] - ETA: 13:36 - loss: 5.3596 - accuracy: 0.2059\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  69/3649 [..............................] - ETA: 13:34 - loss: 5.3610 - accuracy: 0.2029\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  70/3649 [..............................] - ETA: 13:32 - loss: 5.2871 - accuracy: 0.2143\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  71/3649 [..............................] - ETA: 13:30 - loss: 5.2848 - accuracy: 0.2113\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  72/3649 [..............................] - ETA: 13:28 - loss: 5.3173 - accuracy: 0.2083\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  73/3649 [..............................] - ETA: 13:27 - loss: 5.2550 - accuracy: 0.2192\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  74/3649 [..............................] - ETA: 13:25 - loss: 5.2539 - accuracy: 0.2162\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  75/3649 [..............................] - ETA: 13:23 - loss: 5.3158 - accuracy: 0.2133\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  76/3649 [..............................] - ETA: 13:22 - loss: 5.3158 - accuracy: 0.2105\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  77/3649 [..............................] - ETA: 13:20 - loss: 5.2577 - accuracy: 0.2208\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  78/3649 [..............................] - ETA: 13:18 - loss: 5.2509 - accuracy: 0.2179\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  79/3649 [..............................] - ETA: 13:17 - loss: 5.2582 - accuracy: 0.2152\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  80/3649 [..............................] - ETA: 13:15 - loss: 5.2039 - accuracy: 0.2250\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  81/3649 [..............................] - ETA: 13:14 - loss: 5.2230 - accuracy: 0.2222\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  82/3649 [..............................] - ETA: 13:12 - loss: 5.2273 - accuracy: 0.2195\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  83/3649 [..............................] - ETA: 13:11 - loss: 5.2243 - accuracy: 0.2169\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  84/3649 [..............................] - ETA: 13:09 - loss: 5.2017 - accuracy: 0.2143\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  85/3649 [..............................] - ETA: 13:08 - loss: 5.1774 - accuracy: 0.2118\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  86/3649 [..............................] - ETA: 13:07 - loss: 5.1833 - accuracy: 0.2093\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  87/3649 [..............................] - ETA: 13:05 - loss: 5.1259 - accuracy: 0.2184\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  88/3649 [..............................] - ETA: 13:04 - loss: 5.1303 - accuracy: 0.2159\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  89/3649 [..............................] - ETA: 13:03 - loss: 5.1172 - accuracy: 0.2135\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  90/3649 [..............................] - ETA: 13:02 - loss: 5.1411 - accuracy: 0.2111\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  91/3649 [..............................] - ETA: 13:00 - loss: 5.1313 - accuracy: 0.2088\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  92/3649 [..............................] - ETA: 12:59 - loss: 5.1304 - accuracy: 0.2065\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  93/3649 [..............................] - ETA: 12:58 - loss: 5.2029 - accuracy: 0.2043\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  94/3649 [..............................] - ETA: 12:57 - loss: 5.1856 - accuracy: 0.2021\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  95/3649 [..............................] - ETA: 12:56 - loss: 5.1983 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  96/3649 [..............................] - ETA: 12:55 - loss: 5.1460 - accuracy: 0.2083\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  97/3649 [..............................] - ETA: 12:54 - loss: 5.1491 - accuracy: 0.2062\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  98/3649 [..............................] - ETA: 12:53 - loss: 5.1478 - accuracy: 0.2041\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "  99/3649 [..............................] - ETA: 12:52 - loss: 5.1625 - accuracy: 0.2020\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 100/3649 [..............................] - ETA: 12:51 - loss: 5.2057 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 101/3649 [..............................] - ETA: 12:49 - loss: 5.1560 - accuracy: 0.2079\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 102/3649 [..............................] - ETA: 12:48 - loss: 5.2289 - accuracy: 0.2059\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 103/3649 [..............................] - ETA: 12:48 - loss: 5.2479 - accuracy: 0.2039\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 104/3649 [..............................] - ETA: 12:47 - loss: 5.2077 - accuracy: 0.2115\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 105/3649 [..............................] - ETA: 12:46 - loss: 5.2246 - accuracy: 0.2095\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 106/3649 [..............................] - ETA: 12:45 - loss: 5.2751 - accuracy: 0.2075\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 107/3649 [..............................] - ETA: 12:44 - loss: 5.2648 - accuracy: 0.2056\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 108/3649 [..............................] - ETA: 12:43 - loss: 5.2742 - accuracy: 0.2037\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 109/3649 [..............................] - ETA: 12:42 - loss: 5.2759 - accuracy: 0.2018\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 110/3649 [..............................] - ETA: 12:41 - loss: 5.2591 - accuracy: 0.2091\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 111/3649 [..............................] - ETA: 12:40 - loss: 5.2922 - accuracy: 0.2072\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 112/3649 [..............................] - ETA: 12:40 - loss: 5.2462 - accuracy: 0.2143\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 113/3649 [..............................] - ETA: 12:39 - loss: 5.2457 - accuracy: 0.2124\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 114/3649 [..............................] - ETA: 12:38 - loss: 5.3032 - accuracy: 0.2105\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 115/3649 [..............................] - ETA: 12:37 - loss: 5.2928 - accuracy: 0.2087\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 116/3649 [..............................] - ETA: 12:36 - loss: 5.2985 - accuracy: 0.2069\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 117/3649 [..............................] - ETA: 12:35 - loss: 5.2948 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 118/3649 [..............................] - ETA: 12:34 - loss: 5.2942 - accuracy: 0.2034\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 119/3649 [..............................] - ETA: 12:34 - loss: 5.3090 - accuracy: 0.2017\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 120/3649 [..............................] - ETA: 12:33 - loss: 5.2908 - accuracy: 0.2083\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 121/3649 [..............................] - ETA: 12:32 - loss: 5.3285 - accuracy: 0.2066\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 122/3649 [>.............................] - ETA: 12:31 - loss: 5.2937 - accuracy: 0.2049\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 123/3649 [>.............................] - ETA: 12:31 - loss: 5.2946 - accuracy: 0.2033\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 124/3649 [>.............................] - ETA: 12:30 - loss: 5.2812 - accuracy: 0.2016\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 125/3649 [>.............................] - ETA: 12:29 - loss: 5.2861 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 126/3649 [>.............................] - ETA: 12:28 - loss: 5.2459 - accuracy: 0.2063\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 127/3649 [>.............................] - ETA: 12:28 - loss: 5.2630 - accuracy: 0.2047\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 128/3649 [>.............................] - ETA: 12:27 - loss: 5.2740 - accuracy: 0.2031\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 129/3649 [>.............................] - ETA: 12:26 - loss: 5.3219 - accuracy: 0.2016\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 130/3649 [>.............................] - ETA: 12:26 - loss: 5.3079 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 131/3649 [>.............................] - ETA: 12:25 - loss: 5.2918 - accuracy: 0.1985\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 132/3649 [>.............................] - ETA: 12:24 - loss: 5.2553 - accuracy: 0.2045\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 133/3649 [>.............................] - ETA: 12:24 - loss: 5.2188 - accuracy: 0.2105\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 134/3649 [>.............................] - ETA: 12:23 - loss: 5.2131 - accuracy: 0.2090\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 135/3649 [>.............................] - ETA: 12:22 - loss: 5.2086 - accuracy: 0.2074\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 136/3649 [>.............................] - ETA: 12:22 - loss: 5.2052 - accuracy: 0.2059\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 137/3649 [>.............................] - ETA: 12:21 - loss: 5.2018 - accuracy: 0.2044\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 138/3649 [>.............................] - ETA: 12:20 - loss: 5.1903 - accuracy: 0.2029\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 139/3649 [>.............................] - ETA: 12:20 - loss: 5.1879 - accuracy: 0.2014\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 140/3649 [>.............................] - ETA: 12:19 - loss: 5.1750 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 141/3649 [>.............................] - ETA: 12:18 - loss: 5.1689 - accuracy: 0.1986\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 142/3649 [>.............................] - ETA: 12:18 - loss: 5.1627 - accuracy: 0.1972\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 143/3649 [>.............................] - ETA: 12:17 - loss: 5.1466 - accuracy: 0.1958\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 144/3649 [>.............................] - ETA: 12:16 - loss: 5.1945 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 145/3649 [>.............................] - ETA: 12:16 - loss: 5.2374 - accuracy: 0.1931\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 146/3649 [>.............................] - ETA: 12:15 - loss: 5.2532 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 147/3649 [>.............................] - ETA: 12:15 - loss: 5.2507 - accuracy: 0.1905\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 148/3649 [>.............................] - ETA: 12:14 - loss: 5.2323 - accuracy: 0.1892\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 149/3649 [>.............................] - ETA: 12:13 - loss: 5.2433 - accuracy: 0.1879\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 150/3649 [>.............................] - ETA: 12:13 - loss: 5.2378 - accuracy: 0.1867\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 151/3649 [>.............................] - ETA: 12:12 - loss: 5.2174 - accuracy: 0.1854\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 152/3649 [>.............................] - ETA: 12:12 - loss: 5.1969 - accuracy: 0.1908\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 153/3649 [>.............................] - ETA: 12:11 - loss: 5.2403 - accuracy: 0.1895\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 154/3649 [>.............................] - ETA: 12:11 - loss: 5.2171 - accuracy: 0.1948\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 155/3649 [>.............................] - ETA: 12:10 - loss: 5.2608 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 156/3649 [>.............................] - ETA: 12:10 - loss: 5.2678 - accuracy: 0.1923\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 157/3649 [>.............................] - ETA: 12:09 - loss: 5.2637 - accuracy: 0.1911\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 158/3649 [>.............................] - ETA: 12:09 - loss: 5.2722 - accuracy: 0.1899\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 159/3649 [>.............................] - ETA: 12:08 - loss: 5.3093 - accuracy: 0.1887\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 160/3649 [>.............................] - ETA: 12:08 - loss: 5.2782 - accuracy: 0.1937\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 161/3649 [>.............................] - ETA: 12:07 - loss: 5.3149 - accuracy: 0.1925\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 162/3649 [>.............................] - ETA: 12:07 - loss: 5.3491 - accuracy: 0.1914\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 163/3649 [>.............................] - ETA: 12:06 - loss: 5.3787 - accuracy: 0.1902\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 164/3649 [>.............................] - ETA: 12:06 - loss: 5.3690 - accuracy: 0.1890\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 165/3649 [>.............................] - ETA: 12:05 - loss: 5.3978 - accuracy: 0.1879\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 166/3649 [>.............................] - ETA: 12:05 - loss: 5.4297 - accuracy: 0.1867\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 167/3649 [>.............................] - ETA: 12:04 - loss: 5.4157 - accuracy: 0.1856\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 168/3649 [>.............................] - ETA: 12:04 - loss: 5.4021 - accuracy: 0.1845\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 169/3649 [>.............................] - ETA: 12:03 - loss: 5.3939 - accuracy: 0.1834\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 170/3649 [>.............................] - ETA: 12:03 - loss: 5.3924 - accuracy: 0.1824\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 171/3649 [>.............................] - ETA: 12:02 - loss: 5.3827 - accuracy: 0.1813\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 172/3649 [>.............................] - ETA: 12:02 - loss: 5.3706 - accuracy: 0.1802\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 173/3649 [>.............................] - ETA: 12:01 - loss: 5.3980 - accuracy: 0.1792\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 174/3649 [>.............................] - ETA: 12:01 - loss: 5.4259 - accuracy: 0.1782\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 175/3649 [>.............................] - ETA: 12:00 - loss: 5.4336 - accuracy: 0.1771\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 176/3649 [>.............................] - ETA: 12:00 - loss: 5.4444 - accuracy: 0.1761\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 177/3649 [>.............................] - ETA: 11:59 - loss: 5.4503 - accuracy: 0.1751\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 178/3649 [>.............................] - ETA: 11:59 - loss: 5.4789 - accuracy: 0.1742\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 179/3649 [>.............................] - ETA: 11:58 - loss: 5.4507 - accuracy: 0.1788\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 180/3649 [>.............................] - ETA: 11:58 - loss: 5.4553 - accuracy: 0.1778\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 181/3649 [>.............................] - ETA: 11:58 - loss: 5.4467 - accuracy: 0.1768\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 182/3649 [>.............................] - ETA: 11:57 - loss: 5.4553 - accuracy: 0.1758\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 183/3649 [>.............................] - ETA: 11:57 - loss: 5.4795 - accuracy: 0.1749\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 184/3649 [>.............................] - ETA: 11:56 - loss: 5.4824 - accuracy: 0.1739\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 185/3649 [>.............................] - ETA: 11:56 - loss: 5.4540 - accuracy: 0.1784\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 186/3649 [>.............................] - ETA: 11:55 - loss: 5.4835 - accuracy: 0.1774\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 187/3649 [>.............................] - ETA: 11:55 - loss: 5.5099 - accuracy: 0.1765\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 188/3649 [>.............................] - ETA: 11:54 - loss: 5.5350 - accuracy: 0.1755\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 189/3649 [>.............................] - ETA: 11:54 - loss: 5.5285 - accuracy: 0.1746\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 190/3649 [>.............................] - ETA: 11:54 - loss: 5.5448 - accuracy: 0.1737\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 191/3649 [>.............................] - ETA: 11:53 - loss: 5.5174 - accuracy: 0.1780\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 192/3649 [>.............................] - ETA: 11:53 - loss: 5.5243 - accuracy: 0.1771\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 193/3649 [>.............................] - ETA: 11:52 - loss: 5.5316 - accuracy: 0.1762\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 194/3649 [>.............................] - ETA: 11:52 - loss: 5.5251 - accuracy: 0.1804\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 195/3649 [>.............................] - ETA: 11:52 - loss: 5.5246 - accuracy: 0.1795\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 196/3649 [>.............................] - ETA: 11:51 - loss: 5.4975 - accuracy: 0.1837\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 197/3649 [>.............................] - ETA: 11:51 - loss: 5.5030 - accuracy: 0.1827\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 198/3649 [>.............................] - ETA: 11:50 - loss: 5.5042 - accuracy: 0.1818\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 199/3649 [>.............................] - ETA: 11:50 - loss: 5.5304 - accuracy: 0.1809\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 200/3649 [>.............................] - ETA: 11:50 - loss: 5.5381 - accuracy: 0.1800\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 201/3649 [>.............................] - ETA: 11:49 - loss: 5.5362 - accuracy: 0.1791\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 202/3649 [>.............................] - ETA: 11:49 - loss: 5.5095 - accuracy: 0.1832\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 203/3649 [>.............................] - ETA: 11:48 - loss: 5.5101 - accuracy: 0.1823\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 204/3649 [>.............................] - ETA: 11:48 - loss: 5.5107 - accuracy: 0.1814\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 205/3649 [>.............................] - ETA: 11:48 - loss: 5.5192 - accuracy: 0.1805\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 206/3649 [>.............................] - ETA: 11:47 - loss: 5.5176 - accuracy: 0.1796\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 207/3649 [>.............................] - ETA: 11:47 - loss: 5.5081 - accuracy: 0.1787\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 208/3649 [>.............................] - ETA: 11:46 - loss: 5.5107 - accuracy: 0.1779\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 209/3649 [>.............................] - ETA: 11:46 - loss: 5.5095 - accuracy: 0.1770\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 210/3649 [>.............................] - ETA: 11:46 - loss: 5.4843 - accuracy: 0.1810\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 211/3649 [>.............................] - ETA: 11:45 - loss: 5.4860 - accuracy: 0.1801\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 212/3649 [>.............................] - ETA: 11:45 - loss: 5.4743 - accuracy: 0.1792\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 213/3649 [>.............................] - ETA: 11:44 - loss: 5.4807 - accuracy: 0.1784\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 214/3649 [>.............................] - ETA: 11:44 - loss: 5.4901 - accuracy: 0.1776\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 215/3649 [>.............................] - ETA: 11:44 - loss: 5.4938 - accuracy: 0.1767\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 216/3649 [>.............................] - ETA: 11:43 - loss: 5.4884 - accuracy: 0.1759\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 217/3649 [>.............................] - ETA: 11:43 - loss: 5.4741 - accuracy: 0.1797\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 218/3649 [>.............................] - ETA: 11:42 - loss: 5.4821 - accuracy: 0.1789\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 219/3649 [>.............................] - ETA: 11:42 - loss: 5.4888 - accuracy: 0.1781\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 220/3649 [>.............................] - ETA: 11:42 - loss: 5.4836 - accuracy: 0.1773\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 221/3649 [>.............................] - ETA: 11:41 - loss: 5.4596 - accuracy: 0.1810\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 222/3649 [>.............................] - ETA: 11:41 - loss: 5.4843 - accuracy: 0.1802\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 223/3649 [>.............................] - ETA: 11:40 - loss: 5.5085 - accuracy: 0.1794\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 224/3649 [>.............................] - ETA: 11:40 - loss: 5.5081 - accuracy: 0.1786\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 225/3649 [>.............................] - ETA: 11:40 - loss: 5.4929 - accuracy: 0.1822\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 226/3649 [>.............................] - ETA: 11:39 - loss: 5.4930 - accuracy: 0.1814\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 227/3649 [>.............................] - ETA: 11:39 - loss: 5.4943 - accuracy: 0.1806\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 228/3649 [>.............................] - ETA: 11:39 - loss: 5.4808 - accuracy: 0.1842\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 229/3649 [>.............................] - ETA: 11:38 - loss: 5.5089 - accuracy: 0.1834\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 230/3649 [>.............................] - ETA: 11:38 - loss: 5.5001 - accuracy: 0.1826\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 231/3649 [>.............................] - ETA: 11:38 - loss: 5.5300 - accuracy: 0.1818\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 232/3649 [>.............................] - ETA: 11:37 - loss: 5.5290 - accuracy: 0.1810\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 233/3649 [>.............................] - ETA: 11:37 - loss: 5.5144 - accuracy: 0.1845\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 234/3649 [>.............................] - ETA: 11:36 - loss: 5.5079 - accuracy: 0.1838\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 235/3649 [>.............................] - ETA: 11:36 - loss: 5.4920 - accuracy: 0.1872\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 236/3649 [>.............................] - ETA: 11:36 - loss: 5.4873 - accuracy: 0.1864\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 237/3649 [>.............................] - ETA: 11:35 - loss: 5.5105 - accuracy: 0.1857\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 238/3649 [>.............................] - ETA: 11:35 - loss: 5.5096 - accuracy: 0.1849\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 239/3649 [>.............................] - ETA: 11:35 - loss: 5.5127 - accuracy: 0.1841\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 240/3649 [>.............................] - ETA: 11:34 - loss: 5.5005 - accuracy: 0.1833\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 241/3649 [>.............................] - ETA: 11:34 - loss: 5.4954 - accuracy: 0.1826\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 242/3649 [>.............................] - ETA: 11:34 - loss: 5.5184 - accuracy: 0.1818\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 243/3649 [>.............................] - ETA: 11:33 - loss: 5.5214 - accuracy: 0.1811\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 244/3649 [=>............................] - ETA: 11:33 - loss: 5.4995 - accuracy: 0.1844\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 245/3649 [=>............................] - ETA: 11:33 - loss: 5.5129 - accuracy: 0.1837\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 246/3649 [=>............................] - ETA: 11:32 - loss: 5.5042 - accuracy: 0.1829\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 247/3649 [=>............................] - ETA: 11:32 - loss: 5.5279 - accuracy: 0.1822\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 248/3649 [=>............................] - ETA: 11:32 - loss: 5.5528 - accuracy: 0.1815\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 249/3649 [=>............................] - ETA: 11:31 - loss: 5.5776 - accuracy: 0.1807\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 250/3649 [=>............................] - ETA: 11:31 - loss: 5.5825 - accuracy: 0.1800\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 251/3649 [=>............................] - ETA: 11:31 - loss: 5.6065 - accuracy: 0.1793\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 252/3649 [=>............................] - ETA: 11:30 - loss: 5.5983 - accuracy: 0.1786\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 253/3649 [=>............................] - ETA: 11:30 - loss: 5.5883 - accuracy: 0.1779\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 254/3649 [=>............................] - ETA: 11:30 - loss: 5.5681 - accuracy: 0.1811\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 255/3649 [=>............................] - ETA: 11:29 - loss: 5.5474 - accuracy: 0.1843\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 256/3649 [=>............................] - ETA: 11:29 - loss: 5.5400 - accuracy: 0.1836\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 257/3649 [=>............................] - ETA: 11:29 - loss: 5.5331 - accuracy: 0.1829\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 258/3649 [=>............................] - ETA: 11:28 - loss: 5.5253 - accuracy: 0.1822\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 259/3649 [=>............................] - ETA: 11:28 - loss: 5.5171 - accuracy: 0.1815\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 260/3649 [=>............................] - ETA: 11:28 - loss: 5.5034 - accuracy: 0.1808\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 261/3649 [=>............................] - ETA: 11:27 - loss: 5.4954 - accuracy: 0.1801\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 262/3649 [=>............................] - ETA: 11:27 - loss: 5.4802 - accuracy: 0.1832\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 263/3649 [=>............................] - ETA: 11:27 - loss: 5.4691 - accuracy: 0.1825\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 264/3649 [=>............................] - ETA: 11:26 - loss: 5.4592 - accuracy: 0.1818\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 265/3649 [=>............................] - ETA: 11:26 - loss: 5.4443 - accuracy: 0.1849\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 266/3649 [=>............................] - ETA: 11:26 - loss: 5.4578 - accuracy: 0.1842\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 267/3649 [=>............................] - ETA: 11:25 - loss: 5.4741 - accuracy: 0.1835\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 268/3649 [=>............................] - ETA: 11:25 - loss: 5.4831 - accuracy: 0.1828\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 269/3649 [=>............................] - ETA: 11:25 - loss: 5.4826 - accuracy: 0.1822\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 270/3649 [=>............................] - ETA: 11:24 - loss: 5.4696 - accuracy: 0.1852\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 271/3649 [=>............................] - ETA: 11:24 - loss: 5.4882 - accuracy: 0.1845\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 272/3649 [=>............................] - ETA: 11:24 - loss: 5.4708 - accuracy: 0.1875\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 273/3649 [=>............................] - ETA: 11:24 - loss: 5.4779 - accuracy: 0.1868\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 274/3649 [=>............................] - ETA: 11:23 - loss: 5.4735 - accuracy: 0.1861\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 275/3649 [=>............................] - ETA: 11:23 - loss: 5.4595 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 276/3649 [=>............................] - ETA: 11:23 - loss: 5.4461 - accuracy: 0.1920\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 277/3649 [=>............................] - ETA: 11:22 - loss: 5.4438 - accuracy: 0.1913\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 278/3649 [=>............................] - ETA: 11:22 - loss: 5.4359 - accuracy: 0.1906\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 279/3649 [=>............................] - ETA: 11:22 - loss: 5.4247 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 280/3649 [=>............................] - ETA: 11:21 - loss: 5.4198 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 281/3649 [=>............................] - ETA: 11:21 - loss: 5.4058 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 282/3649 [=>............................] - ETA: 11:21 - loss: 5.4117 - accuracy: 0.1915\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 283/3649 [=>............................] - ETA: 11:20 - loss: 5.4338 - accuracy: 0.1908\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 284/3649 [=>............................] - ETA: 11:20 - loss: 5.4176 - accuracy: 0.1937\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 285/3649 [=>............................] - ETA: 11:20 - loss: 5.4359 - accuracy: 0.1930\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 286/3649 [=>............................] - ETA: 11:19 - loss: 5.4572 - accuracy: 0.1923\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 287/3649 [=>............................] - ETA: 11:19 - loss: 5.4425 - accuracy: 0.1951\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 288/3649 [=>............................] - ETA: 11:19 - loss: 5.4300 - accuracy: 0.1979\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 289/3649 [=>............................] - ETA: 11:19 - loss: 5.4261 - accuracy: 0.1972\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 290/3649 [=>............................] - ETA: 11:18 - loss: 5.4125 - accuracy: 0.1966\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 291/3649 [=>............................] - ETA: 11:18 - loss: 5.4178 - accuracy: 0.1959\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 292/3649 [=>............................] - ETA: 11:18 - loss: 5.4333 - accuracy: 0.1952\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 293/3649 [=>............................] - ETA: 11:17 - loss: 5.4512 - accuracy: 0.1945\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 294/3649 [=>............................] - ETA: 11:17 - loss: 5.4672 - accuracy: 0.1939\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 295/3649 [=>............................] - ETA: 11:17 - loss: 5.4590 - accuracy: 0.1932\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 296/3649 [=>............................] - ETA: 11:17 - loss: 5.4659 - accuracy: 0.1926\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 297/3649 [=>............................] - ETA: 11:16 - loss: 5.4565 - accuracy: 0.1953\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 298/3649 [=>............................] - ETA: 11:16 - loss: 5.4561 - accuracy: 0.1946\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 299/3649 [=>............................] - ETA: 11:16 - loss: 5.4412 - accuracy: 0.1973\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 300/3649 [=>............................] - ETA: 11:15 - loss: 5.4600 - accuracy: 0.1967\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 301/3649 [=>............................] - ETA: 11:15 - loss: 5.4717 - accuracy: 0.1960\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 302/3649 [=>............................] - ETA: 11:15 - loss: 5.4774 - accuracy: 0.1954\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 303/3649 [=>............................] - ETA: 11:14 - loss: 5.4888 - accuracy: 0.1947\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 304/3649 [=>............................] - ETA: 11:14 - loss: 5.4764 - accuracy: 0.1974\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 305/3649 [=>............................] - ETA: 11:14 - loss: 5.4787 - accuracy: 0.1967\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 306/3649 [=>............................] - ETA: 11:14 - loss: 5.4968 - accuracy: 0.1961\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 307/3649 [=>............................] - ETA: 11:13 - loss: 5.4833 - accuracy: 0.1987\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 308/3649 [=>............................] - ETA: 11:13 - loss: 5.4807 - accuracy: 0.1981\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 309/3649 [=>............................] - ETA: 11:13 - loss: 5.4851 - accuracy: 0.1974\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 310/3649 [=>............................] - ETA: 11:12 - loss: 5.4720 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 311/3649 [=>............................] - ETA: 11:12 - loss: 5.4651 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 312/3649 [=>............................] - ETA: 11:12 - loss: 5.4814 - accuracy: 0.1987\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 313/3649 [=>............................] - ETA: 11:11 - loss: 5.4798 - accuracy: 0.1981\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 314/3649 [=>............................] - ETA: 11:11 - loss: 5.4645 - accuracy: 0.2006\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 315/3649 [=>............................] - ETA: 11:11 - loss: 5.4725 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 316/3649 [=>............................] - ETA: 11:11 - loss: 5.4587 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 317/3649 [=>............................] - ETA: 11:10 - loss: 5.4734 - accuracy: 0.2019\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 318/3649 [=>............................] - ETA: 11:10 - loss: 5.4777 - accuracy: 0.2013\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 319/3649 [=>............................] - ETA: 11:10 - loss: 5.4756 - accuracy: 0.2006\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 320/3649 [=>............................] - ETA: 11:09 - loss: 5.4993 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 321/3649 [=>............................] - ETA: 11:09 - loss: 5.5050 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 322/3649 [=>............................] - ETA: 11:09 - loss: 5.4911 - accuracy: 0.2019\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 323/3649 [=>............................] - ETA: 11:09 - loss: 5.4776 - accuracy: 0.2043\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 324/3649 [=>............................] - ETA: 11:08 - loss: 5.4700 - accuracy: 0.2037\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 325/3649 [=>............................] - ETA: 11:08 - loss: 5.4717 - accuracy: 0.2031\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 326/3649 [=>............................] - ETA: 11:08 - loss: 5.4730 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 327/3649 [=>............................] - ETA: 11:07 - loss: 5.4766 - accuracy: 0.2018\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 328/3649 [=>............................] - ETA: 11:07 - loss: 5.4975 - accuracy: 0.2012\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 329/3649 [=>............................] - ETA: 11:07 - loss: 5.5162 - accuracy: 0.2006\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 330/3649 [=>............................] - ETA: 11:07 - loss: 5.5124 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 331/3649 [=>............................] - ETA: 11:06 - loss: 5.5131 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 332/3649 [=>............................] - ETA: 11:06 - loss: 5.5299 - accuracy: 0.1988\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 333/3649 [=>............................] - ETA: 11:06 - loss: 5.5322 - accuracy: 0.1982\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 334/3649 [=>............................] - ETA: 11:06 - loss: 5.5398 - accuracy: 0.1976\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 335/3649 [=>............................] - ETA: 11:05 - loss: 5.5573 - accuracy: 0.1970\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 336/3649 [=>............................] - ETA: 11:05 - loss: 5.5715 - accuracy: 0.1964\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 337/3649 [=>............................] - ETA: 11:05 - loss: 5.5879 - accuracy: 0.1958\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 338/3649 [=>............................] - ETA: 11:04 - loss: 5.5882 - accuracy: 0.1953\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 339/3649 [=>............................] - ETA: 11:04 - loss: 5.6034 - accuracy: 0.1947\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 340/3649 [=>............................] - ETA: 11:04 - loss: 5.6052 - accuracy: 0.1941\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 341/3649 [=>............................] - ETA: 11:04 - loss: 5.5901 - accuracy: 0.1965\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 342/3649 [=>............................] - ETA: 11:03 - loss: 5.5909 - accuracy: 0.1959\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 343/3649 [=>............................] - ETA: 11:03 - loss: 5.5808 - accuracy: 0.1983\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 344/3649 [=>............................] - ETA: 11:03 - loss: 5.5846 - accuracy: 0.1977\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 345/3649 [=>............................] - ETA: 11:03 - loss: 5.5999 - accuracy: 0.1971\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 346/3649 [=>............................] - ETA: 11:02 - loss: 5.5969 - accuracy: 0.1965\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 347/3649 [=>............................] - ETA: 11:02 - loss: 5.5994 - accuracy: 0.1960\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 348/3649 [=>............................] - ETA: 11:02 - loss: 5.6125 - accuracy: 0.1954\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 349/3649 [=>............................] - ETA: 11:01 - loss: 5.6143 - accuracy: 0.1948\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 350/3649 [=>............................] - ETA: 11:01 - loss: 5.6243 - accuracy: 0.1943\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 351/3649 [=>............................] - ETA: 11:01 - loss: 5.6267 - accuracy: 0.1937\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 352/3649 [=>............................] - ETA: 11:01 - loss: 5.6294 - accuracy: 0.1932\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 353/3649 [=>............................] - ETA: 11:00 - loss: 5.6352 - accuracy: 0.1926\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 354/3649 [=>............................] - ETA: 11:00 - loss: 5.6348 - accuracy: 0.1921\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 355/3649 [=>............................] - ETA: 11:00 - loss: 5.6492 - accuracy: 0.1915\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 356/3649 [=>............................] - ETA: 11:00 - loss: 5.6636 - accuracy: 0.1910\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 357/3649 [=>............................] - ETA: 10:59 - loss: 5.6777 - accuracy: 0.1905\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 358/3649 [=>............................] - ETA: 10:59 - loss: 5.6745 - accuracy: 0.1899\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 359/3649 [=>............................] - ETA: 10:59 - loss: 5.6636 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 360/3649 [=>............................] - ETA: 10:58 - loss: 5.6777 - accuracy: 0.1917\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 361/3649 [=>............................] - ETA: 10:58 - loss: 5.6749 - accuracy: 0.1911\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 362/3649 [=>............................] - ETA: 10:58 - loss: 5.6652 - accuracy: 0.1934\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 363/3649 [=>............................] - ETA: 10:58 - loss: 5.6693 - accuracy: 0.1928\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 364/3649 [=>............................] - ETA: 10:57 - loss: 5.6819 - accuracy: 0.1923\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 365/3649 [==>...........................] - ETA: 10:57 - loss: 5.6899 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 366/3649 [==>...........................] - ETA: 10:57 - loss: 5.6902 - accuracy: 0.1913\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 367/3649 [==>...........................] - ETA: 10:57 - loss: 5.6762 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 368/3649 [==>...........................] - ETA: 10:56 - loss: 5.6790 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 369/3649 [==>...........................] - ETA: 10:56 - loss: 5.6866 - accuracy: 0.1924\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 370/3649 [==>...........................] - ETA: 10:56 - loss: 5.6878 - accuracy: 0.1919\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 371/3649 [==>...........................] - ETA: 10:56 - loss: 5.6793 - accuracy: 0.1914\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 372/3649 [==>...........................] - ETA: 10:55 - loss: 5.6909 - accuracy: 0.1909\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 373/3649 [==>...........................] - ETA: 10:55 - loss: 5.6985 - accuracy: 0.1903\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 374/3649 [==>...........................] - ETA: 10:55 - loss: 5.7102 - accuracy: 0.1898\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 375/3649 [==>...........................] - ETA: 10:55 - loss: 5.7138 - accuracy: 0.1893\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 376/3649 [==>...........................] - ETA: 10:54 - loss: 5.7257 - accuracy: 0.1888\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 377/3649 [==>...........................] - ETA: 10:54 - loss: 5.7294 - accuracy: 0.1883\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 378/3649 [==>...........................] - ETA: 10:54 - loss: 5.7306 - accuracy: 0.1878\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 379/3649 [==>...........................] - ETA: 10:54 - loss: 5.7435 - accuracy: 0.1873\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 380/3649 [==>...........................] - ETA: 10:53 - loss: 5.7294 - accuracy: 0.1895\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 381/3649 [==>...........................] - ETA: 10:53 - loss: 5.7327 - accuracy: 0.1890\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 382/3649 [==>...........................] - ETA: 10:53 - loss: 5.7464 - accuracy: 0.1885\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 383/3649 [==>...........................] - ETA: 10:53 - loss: 5.7495 - accuracy: 0.1880\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 384/3649 [==>...........................] - ETA: 10:52 - loss: 5.7458 - accuracy: 0.1875\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 385/3649 [==>...........................] - ETA: 10:52 - loss: 5.7339 - accuracy: 0.1896\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 386/3649 [==>...........................] - ETA: 10:52 - loss: 5.7460 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 387/3649 [==>...........................] - ETA: 10:52 - loss: 5.7585 - accuracy: 0.1886\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 388/3649 [==>...........................] - ETA: 10:51 - loss: 5.7713 - accuracy: 0.1881\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 389/3649 [==>...........................] - ETA: 10:51 - loss: 5.7574 - accuracy: 0.1902\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 390/3649 [==>...........................] - ETA: 10:51 - loss: 5.7567 - accuracy: 0.1897\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 391/3649 [==>...........................] - ETA: 10:51 - loss: 5.7449 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 392/3649 [==>...........................] - ETA: 10:50 - loss: 5.7429 - accuracy: 0.1913\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 393/3649 [==>...........................] - ETA: 10:50 - loss: 5.7297 - accuracy: 0.1934\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 394/3649 [==>...........................] - ETA: 10:50 - loss: 5.7275 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 395/3649 [==>...........................] - ETA: 10:49 - loss: 5.7188 - accuracy: 0.1924\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 396/3649 [==>...........................] - ETA: 10:49 - loss: 5.7148 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 397/3649 [==>...........................] - ETA: 10:49 - loss: 5.7057 - accuracy: 0.1940\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 398/3649 [==>...........................] - ETA: 10:49 - loss: 5.7052 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 399/3649 [==>...........................] - ETA: 10:48 - loss: 5.7018 - accuracy: 0.1930\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 400/3649 [==>...........................] - ETA: 10:48 - loss: 5.7143 - accuracy: 0.1925\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 401/3649 [==>...........................] - ETA: 10:48 - loss: 5.7151 - accuracy: 0.1920\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 402/3649 [==>...........................] - ETA: 10:48 - loss: 5.7238 - accuracy: 0.1915\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 403/3649 [==>...........................] - ETA: 10:47 - loss: 5.7206 - accuracy: 0.1911\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 404/3649 [==>...........................] - ETA: 10:47 - loss: 5.7229 - accuracy: 0.1906\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 405/3649 [==>...........................] - ETA: 10:47 - loss: 5.7103 - accuracy: 0.1926\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 406/3649 [==>...........................] - ETA: 10:47 - loss: 5.7124 - accuracy: 0.1921\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 407/3649 [==>...........................] - ETA: 10:47 - loss: 5.6994 - accuracy: 0.1941\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 408/3649 [==>...........................] - ETA: 10:46 - loss: 5.6941 - accuracy: 0.1936\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 409/3649 [==>...........................] - ETA: 10:46 - loss: 5.6904 - accuracy: 0.1932\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 410/3649 [==>...........................] - ETA: 10:46 - loss: 5.6853 - accuracy: 0.1927\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 411/3649 [==>...........................] - ETA: 10:46 - loss: 5.6880 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 412/3649 [==>...........................] - ETA: 10:45 - loss: 5.6823 - accuracy: 0.1917\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 413/3649 [==>...........................] - ETA: 10:45 - loss: 5.6952 - accuracy: 0.1913\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 414/3649 [==>...........................] - ETA: 10:45 - loss: 5.6840 - accuracy: 0.1932\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 415/3649 [==>...........................] - ETA: 10:45 - loss: 5.6829 - accuracy: 0.1928\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 416/3649 [==>...........................] - ETA: 10:44 - loss: 5.6702 - accuracy: 0.1947\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 417/3649 [==>...........................] - ETA: 10:44 - loss: 5.6597 - accuracy: 0.1966\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 418/3649 [==>...........................] - ETA: 10:44 - loss: 5.6590 - accuracy: 0.1962\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 419/3649 [==>...........................] - ETA: 10:44 - loss: 5.6591 - accuracy: 0.1957\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 420/3649 [==>...........................] - ETA: 10:43 - loss: 5.6601 - accuracy: 0.1952\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 421/3649 [==>...........................] - ETA: 10:43 - loss: 5.6622 - accuracy: 0.1948\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 422/3649 [==>...........................] - ETA: 10:43 - loss: 5.6709 - accuracy: 0.1943\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 423/3649 [==>...........................] - ETA: 10:43 - loss: 5.6750 - accuracy: 0.1939\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 424/3649 [==>...........................] - ETA: 10:42 - loss: 5.6632 - accuracy: 0.1958\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 425/3649 [==>...........................] - ETA: 10:42 - loss: 5.6767 - accuracy: 0.1953\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 426/3649 [==>...........................] - ETA: 10:42 - loss: 5.6729 - accuracy: 0.1948\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 427/3649 [==>...........................] - ETA: 10:42 - loss: 5.6868 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 428/3649 [==>...........................] - ETA: 10:41 - loss: 5.6889 - accuracy: 0.1939\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 429/3649 [==>...........................] - ETA: 10:41 - loss: 5.6918 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 430/3649 [==>...........................] - ETA: 10:41 - loss: 5.6792 - accuracy: 0.1953\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 431/3649 [==>...........................] - ETA: 10:41 - loss: 5.6789 - accuracy: 0.1949\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 432/3649 [==>...........................] - ETA: 10:40 - loss: 5.6764 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 433/3649 [==>...........................] - ETA: 10:40 - loss: 5.6649 - accuracy: 0.1963\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 434/3649 [==>...........................] - ETA: 10:40 - loss: 5.6772 - accuracy: 0.1959\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 435/3649 [==>...........................] - ETA: 10:40 - loss: 5.6898 - accuracy: 0.1954\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 436/3649 [==>...........................] - ETA: 10:40 - loss: 5.6865 - accuracy: 0.1950\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 437/3649 [==>...........................] - ETA: 10:39 - loss: 5.6858 - accuracy: 0.1945\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 438/3649 [==>...........................] - ETA: 10:39 - loss: 5.6836 - accuracy: 0.1941\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 439/3649 [==>...........................] - ETA: 10:39 - loss: 5.6769 - accuracy: 0.1936\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 440/3649 [==>...........................] - ETA: 10:39 - loss: 5.6899 - accuracy: 0.1932\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 441/3649 [==>...........................] - ETA: 10:38 - loss: 5.6901 - accuracy: 0.1927\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 442/3649 [==>...........................] - ETA: 10:38 - loss: 5.6836 - accuracy: 0.1923\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 443/3649 [==>...........................] - ETA: 10:38 - loss: 5.6848 - accuracy: 0.1919\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 444/3649 [==>...........................] - ETA: 10:38 - loss: 5.6968 - accuracy: 0.1914\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 445/3649 [==>...........................] - ETA: 10:37 - loss: 5.6848 - accuracy: 0.1933\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 446/3649 [==>...........................] - ETA: 10:37 - loss: 5.6822 - accuracy: 0.1928\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 447/3649 [==>...........................] - ETA: 10:37 - loss: 5.6934 - accuracy: 0.1924\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 448/3649 [==>...........................] - ETA: 10:37 - loss: 5.6820 - accuracy: 0.1942\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 449/3649 [==>...........................] - ETA: 10:36 - loss: 5.6776 - accuracy: 0.1938\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 450/3649 [==>...........................] - ETA: 10:36 - loss: 5.6765 - accuracy: 0.1933\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 451/3649 [==>...........................] - ETA: 10:36 - loss: 5.6754 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 452/3649 [==>...........................] - ETA: 10:36 - loss: 5.6755 - accuracy: 0.1925\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 453/3649 [==>...........................] - ETA: 10:35 - loss: 5.6755 - accuracy: 0.1921\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 454/3649 [==>...........................] - ETA: 10:35 - loss: 5.6718 - accuracy: 0.1916\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 455/3649 [==>...........................] - ETA: 10:35 - loss: 5.6820 - accuracy: 0.1912\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 456/3649 [==>...........................] - ETA: 10:35 - loss: 5.6752 - accuracy: 0.1908\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 457/3649 [==>...........................] - ETA: 10:34 - loss: 5.6767 - accuracy: 0.1904\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 458/3649 [==>...........................] - ETA: 10:34 - loss: 5.6727 - accuracy: 0.1900\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 459/3649 [==>...........................] - ETA: 10:34 - loss: 5.6747 - accuracy: 0.1895\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 460/3649 [==>...........................] - ETA: 10:34 - loss: 5.6730 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 461/3649 [==>...........................] - ETA: 10:34 - loss: 5.6614 - accuracy: 0.1909\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 462/3649 [==>...........................] - ETA: 10:33 - loss: 5.6610 - accuracy: 0.1905\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 463/3649 [==>...........................] - ETA: 10:33 - loss: 5.6494 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 464/3649 [==>...........................] - ETA: 10:33 - loss: 5.6622 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 465/3649 [==>...........................] - ETA: 10:33 - loss: 5.6760 - accuracy: 0.1914\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 466/3649 [==>...........................] - ETA: 10:32 - loss: 5.6892 - accuracy: 0.1910\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 467/3649 [==>...........................] - ETA: 10:32 - loss: 5.6887 - accuracy: 0.1906\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 468/3649 [==>...........................] - ETA: 10:32 - loss: 5.6890 - accuracy: 0.1902\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 469/3649 [==>...........................] - ETA: 10:32 - loss: 5.6853 - accuracy: 0.1898\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 470/3649 [==>...........................] - ETA: 10:31 - loss: 5.6906 - accuracy: 0.1894\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 471/3649 [==>...........................] - ETA: 10:31 - loss: 5.6798 - accuracy: 0.1911\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 472/3649 [==>...........................] - ETA: 10:31 - loss: 5.6750 - accuracy: 0.1907\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 473/3649 [==>...........................] - ETA: 10:31 - loss: 5.6834 - accuracy: 0.1903\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 474/3649 [==>...........................] - ETA: 10:31 - loss: 5.6836 - accuracy: 0.1899\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 475/3649 [==>...........................] - ETA: 10:30 - loss: 5.6814 - accuracy: 0.1895\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 476/3649 [==>...........................] - ETA: 10:30 - loss: 5.6933 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 477/3649 [==>...........................] - ETA: 10:30 - loss: 5.6896 - accuracy: 0.1887\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 478/3649 [==>...........................] - ETA: 10:30 - loss: 5.7007 - accuracy: 0.1883\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 479/3649 [==>...........................] - ETA: 10:29 - loss: 5.7117 - accuracy: 0.1879\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 480/3649 [==>...........................] - ETA: 10:29 - loss: 5.7227 - accuracy: 0.1875\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 481/3649 [==>...........................] - ETA: 10:29 - loss: 5.7122 - accuracy: 0.1892\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 482/3649 [==>...........................] - ETA: 10:29 - loss: 5.7080 - accuracy: 0.1888\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 483/3649 [==>...........................] - ETA: 10:28 - loss: 5.7041 - accuracy: 0.1905\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 484/3649 [==>...........................] - ETA: 10:28 - loss: 5.7063 - accuracy: 0.1901\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 485/3649 [==>...........................] - ETA: 10:28 - loss: 5.7007 - accuracy: 0.1897\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 486/3649 [==>...........................] - ETA: 10:28 - loss: 5.7100 - accuracy: 0.1893\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 487/3649 [===>..........................] - ETA: 10:27 - loss: 5.7080 - accuracy: 0.1889\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 488/3649 [===>..........................] - ETA: 10:27 - loss: 5.7184 - accuracy: 0.1885\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 489/3649 [===>..........................] - ETA: 10:27 - loss: 5.7111 - accuracy: 0.1881\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 490/3649 [===>..........................] - ETA: 10:27 - loss: 5.7205 - accuracy: 0.1878\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 491/3649 [===>..........................] - ETA: 10:27 - loss: 5.7306 - accuracy: 0.1874\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 492/3649 [===>..........................] - ETA: 10:26 - loss: 5.7264 - accuracy: 0.1870\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 493/3649 [===>..........................] - ETA: 10:26 - loss: 5.7336 - accuracy: 0.1866\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 494/3649 [===>..........................] - ETA: 10:26 - loss: 5.7425 - accuracy: 0.1862\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 495/3649 [===>..........................] - ETA: 10:26 - loss: 5.7331 - accuracy: 0.1879\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 496/3649 [===>..........................] - ETA: 10:25 - loss: 5.7379 - accuracy: 0.1875\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 497/3649 [===>..........................] - ETA: 10:25 - loss: 5.7278 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 498/3649 [===>..........................] - ETA: 10:25 - loss: 5.7295 - accuracy: 0.1888\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 499/3649 [===>..........................] - ETA: 10:25 - loss: 5.7325 - accuracy: 0.1884\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 500/3649 [===>..........................] - ETA: 10:24 - loss: 5.7246 - accuracy: 0.1900\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 501/3649 [===>..........................] - ETA: 10:24 - loss: 5.7337 - accuracy: 0.1896\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 502/3649 [===>..........................] - ETA: 10:24 - loss: 5.7234 - accuracy: 0.1912\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 503/3649 [===>..........................] - ETA: 10:24 - loss: 5.7219 - accuracy: 0.1909\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 504/3649 [===>..........................] - ETA: 10:24 - loss: 5.7127 - accuracy: 0.1925\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 505/3649 [===>..........................] - ETA: 10:23 - loss: 5.7090 - accuracy: 0.1921\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 506/3649 [===>..........................] - ETA: 10:23 - loss: 5.7128 - accuracy: 0.1917\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 507/3649 [===>..........................] - ETA: 10:23 - loss: 5.7186 - accuracy: 0.1913\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 508/3649 [===>..........................] - ETA: 10:23 - loss: 5.7168 - accuracy: 0.1909\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 509/3649 [===>..........................] - ETA: 10:22 - loss: 5.7203 - accuracy: 0.1906\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 510/3649 [===>..........................] - ETA: 10:22 - loss: 5.7209 - accuracy: 0.1902\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 511/3649 [===>..........................] - ETA: 10:22 - loss: 5.7262 - accuracy: 0.1898\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 512/3649 [===>..........................] - ETA: 10:22 - loss: 5.7231 - accuracy: 0.1895\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 513/3649 [===>..........................] - ETA: 10:21 - loss: 5.7241 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 514/3649 [===>..........................] - ETA: 10:21 - loss: 5.7270 - accuracy: 0.1887\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 515/3649 [===>..........................] - ETA: 10:21 - loss: 5.7164 - accuracy: 0.1903\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 516/3649 [===>..........................] - ETA: 10:21 - loss: 5.7119 - accuracy: 0.1899\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 517/3649 [===>..........................] - ETA: 10:21 - loss: 5.7062 - accuracy: 0.1896\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 518/3649 [===>..........................] - ETA: 10:20 - loss: 5.7162 - accuracy: 0.1892\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 519/3649 [===>..........................] - ETA: 10:20 - loss: 5.7113 - accuracy: 0.1908\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 520/3649 [===>..........................] - ETA: 10:20 - loss: 5.7077 - accuracy: 0.1904\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 521/3649 [===>..........................] - ETA: 10:20 - loss: 5.7095 - accuracy: 0.1900\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 522/3649 [===>..........................] - ETA: 10:19 - loss: 5.7134 - accuracy: 0.1897\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 523/3649 [===>..........................] - ETA: 10:19 - loss: 5.7144 - accuracy: 0.1893\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 524/3649 [===>..........................] - ETA: 10:19 - loss: 5.7048 - accuracy: 0.1908\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 525/3649 [===>..........................] - ETA: 10:19 - loss: 5.7014 - accuracy: 0.1905\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 526/3649 [===>..........................] - ETA: 10:19 - loss: 5.7028 - accuracy: 0.1901\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 527/3649 [===>..........................] - ETA: 10:18 - loss: 5.7035 - accuracy: 0.1898\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 528/3649 [===>..........................] - ETA: 10:18 - loss: 5.6976 - accuracy: 0.1894\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 529/3649 [===>..........................] - ETA: 10:18 - loss: 5.6917 - accuracy: 0.1890\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 530/3649 [===>..........................] - ETA: 10:18 - loss: 5.6928 - accuracy: 0.1887\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 531/3649 [===>..........................] - ETA: 10:17 - loss: 5.7054 - accuracy: 0.1883\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 532/3649 [===>..........................] - ETA: 10:17 - loss: 5.6986 - accuracy: 0.1898\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 533/3649 [===>..........................] - ETA: 10:17 - loss: 5.6884 - accuracy: 0.1914\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 534/3649 [===>..........................] - ETA: 10:17 - loss: 5.7016 - accuracy: 0.1910\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 535/3649 [===>..........................] - ETA: 10:17 - loss: 5.6974 - accuracy: 0.1907\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 536/3649 [===>..........................] - ETA: 10:16 - loss: 5.6999 - accuracy: 0.1903\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 537/3649 [===>..........................] - ETA: 10:16 - loss: 5.6938 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 538/3649 [===>..........................] - ETA: 10:16 - loss: 5.6875 - accuracy: 0.1933\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 539/3649 [===>..........................] - ETA: 10:16 - loss: 5.6907 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 540/3649 [===>..........................] - ETA: 10:15 - loss: 5.6826 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 541/3649 [===>..........................] - ETA: 10:15 - loss: 5.6726 - accuracy: 0.1959\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 542/3649 [===>..........................] - ETA: 10:15 - loss: 5.6711 - accuracy: 0.1956\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 543/3649 [===>..........................] - ETA: 10:15 - loss: 5.6829 - accuracy: 0.1952\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 544/3649 [===>..........................] - ETA: 10:14 - loss: 5.6788 - accuracy: 0.1949\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 545/3649 [===>..........................] - ETA: 10:14 - loss: 5.6809 - accuracy: 0.1945\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 546/3649 [===>..........................] - ETA: 10:14 - loss: 5.6713 - accuracy: 0.1960\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 547/3649 [===>..........................] - ETA: 10:14 - loss: 5.6623 - accuracy: 0.1974\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 548/3649 [===>..........................] - ETA: 10:14 - loss: 5.6651 - accuracy: 0.1971\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 549/3649 [===>..........................] - ETA: 10:13 - loss: 5.6620 - accuracy: 0.1967\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 550/3649 [===>..........................] - ETA: 10:13 - loss: 5.6730 - accuracy: 0.1964\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 551/3649 [===>..........................] - ETA: 10:13 - loss: 5.6761 - accuracy: 0.1960\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 552/3649 [===>..........................] - ETA: 10:13 - loss: 5.6693 - accuracy: 0.1957\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 553/3649 [===>..........................] - ETA: 10:12 - loss: 5.6724 - accuracy: 0.1953\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 554/3649 [===>..........................] - ETA: 10:12 - loss: 5.6832 - accuracy: 0.1949\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 555/3649 [===>..........................] - ETA: 10:12 - loss: 5.6771 - accuracy: 0.1946\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 556/3649 [===>..........................] - ETA: 10:12 - loss: 5.6803 - accuracy: 0.1942\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 557/3649 [===>..........................] - ETA: 10:12 - loss: 5.6799 - accuracy: 0.1939\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 558/3649 [===>..........................] - ETA: 10:11 - loss: 5.6821 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 559/3649 [===>..........................] - ETA: 10:11 - loss: 5.6767 - accuracy: 0.1932\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 560/3649 [===>..........................] - ETA: 10:11 - loss: 5.6839 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 561/3649 [===>..........................] - ETA: 10:11 - loss: 5.6935 - accuracy: 0.1925\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 562/3649 [===>..........................] - ETA: 10:10 - loss: 5.6891 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 563/3649 [===>..........................] - ETA: 10:10 - loss: 5.6983 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 564/3649 [===>..........................] - ETA: 10:10 - loss: 5.6975 - accuracy: 0.1915\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 565/3649 [===>..........................] - ETA: 10:10 - loss: 5.6960 - accuracy: 0.1912\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 566/3649 [===>..........................] - ETA: 10:10 - loss: 5.6929 - accuracy: 0.1908\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 567/3649 [===>..........................] - ETA: 10:09 - loss: 5.7027 - accuracy: 0.1905\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 568/3649 [===>..........................] - ETA: 10:09 - loss: 5.7048 - accuracy: 0.1901\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 569/3649 [===>..........................] - ETA: 10:09 - loss: 5.6987 - accuracy: 0.1898\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 570/3649 [===>..........................] - ETA: 10:09 - loss: 5.7062 - accuracy: 0.1895\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 571/3649 [===>..........................] - ETA: 10:09 - loss: 5.7108 - accuracy: 0.1891\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 572/3649 [===>..........................] - ETA: 10:08 - loss: 5.7186 - accuracy: 0.1888\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 573/3649 [===>..........................] - ETA: 10:08 - loss: 5.7263 - accuracy: 0.1885\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 574/3649 [===>..........................] - ETA: 10:08 - loss: 5.7189 - accuracy: 0.1899\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 575/3649 [===>..........................] - ETA: 10:08 - loss: 5.7134 - accuracy: 0.1913\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 576/3649 [===>..........................] - ETA: 10:07 - loss: 5.7108 - accuracy: 0.1910\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 577/3649 [===>..........................] - ETA: 10:07 - loss: 5.7194 - accuracy: 0.1906\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 578/3649 [===>..........................] - ETA: 10:07 - loss: 5.7172 - accuracy: 0.1903\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 579/3649 [===>..........................] - ETA: 10:07 - loss: 5.7113 - accuracy: 0.1917\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 580/3649 [===>..........................] - ETA: 10:07 - loss: 5.7123 - accuracy: 0.1914\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 581/3649 [===>..........................] - ETA: 10:06 - loss: 5.7135 - accuracy: 0.1910\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 582/3649 [===>..........................] - ETA: 10:06 - loss: 5.7134 - accuracy: 0.1907\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 583/3649 [===>..........................] - ETA: 10:06 - loss: 5.7078 - accuracy: 0.1921\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 584/3649 [===>..........................] - ETA: 10:06 - loss: 5.7086 - accuracy: 0.1918\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 585/3649 [===>..........................] - ETA: 10:05 - loss: 5.7107 - accuracy: 0.1915\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 586/3649 [===>..........................] - ETA: 10:05 - loss: 5.7023 - accuracy: 0.1928\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 587/3649 [===>..........................] - ETA: 10:05 - loss: 5.6984 - accuracy: 0.1942\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 588/3649 [===>..........................] - ETA: 10:05 - loss: 5.7018 - accuracy: 0.1939\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 589/3649 [===>..........................] - ETA: 10:05 - loss: 5.6927 - accuracy: 0.1952\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 590/3649 [===>..........................] - ETA: 10:04 - loss: 5.6835 - accuracy: 0.1966\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 591/3649 [===>..........................] - ETA: 10:04 - loss: 5.6877 - accuracy: 0.1963\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 592/3649 [===>..........................] - ETA: 10:04 - loss: 5.6898 - accuracy: 0.1959\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 593/3649 [===>..........................] - ETA: 10:04 - loss: 5.6862 - accuracy: 0.1956\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 594/3649 [===>..........................] - ETA: 10:03 - loss: 5.6870 - accuracy: 0.1953\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 595/3649 [===>..........................] - ETA: 10:03 - loss: 5.6823 - accuracy: 0.1950\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 596/3649 [===>..........................] - ETA: 10:03 - loss: 5.6853 - accuracy: 0.1946\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 597/3649 [===>..........................] - ETA: 10:03 - loss: 5.6817 - accuracy: 0.1943\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 598/3649 [===>..........................] - ETA: 10:03 - loss: 5.6793 - accuracy: 0.1940\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 599/3649 [===>..........................] - ETA: 10:02 - loss: 5.6817 - accuracy: 0.1937\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 600/3649 [===>..........................] - ETA: 10:02 - loss: 5.6730 - accuracy: 0.1950\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 601/3649 [===>..........................] - ETA: 10:02 - loss: 5.6741 - accuracy: 0.1947\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 602/3649 [===>..........................] - ETA: 10:02 - loss: 5.6732 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 603/3649 [===>..........................] - ETA: 10:02 - loss: 5.6725 - accuracy: 0.1940\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 604/3649 [===>..........................] - ETA: 10:01 - loss: 5.6855 - accuracy: 0.1937\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 605/3649 [===>..........................] - ETA: 10:01 - loss: 5.6983 - accuracy: 0.1934\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 606/3649 [===>..........................] - ETA: 10:01 - loss: 5.7044 - accuracy: 0.1931\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 607/3649 [===>..........................] - ETA: 10:01 - loss: 5.6964 - accuracy: 0.1944\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 608/3649 [===>..........................] - ETA: 10:00 - loss: 5.6960 - accuracy: 0.1941\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 609/3649 [====>.........................] - ETA: 10:00 - loss: 5.6970 - accuracy: 0.1938\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 610/3649 [====>.........................] - ETA: 10:00 - loss: 5.7086 - accuracy: 0.1934\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 611/3649 [====>.........................] - ETA: 10:00 - loss: 5.7086 - accuracy: 0.1931\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 612/3649 [====>.........................] - ETA: 10:00 - loss: 5.7099 - accuracy: 0.1928\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 613/3649 [====>.........................] - ETA: 9:59 - loss: 5.7046 - accuracy: 0.1925 \n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 614/3649 [====>.........................] - ETA: 9:59 - loss: 5.7131 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 615/3649 [====>.........................] - ETA: 9:59 - loss: 5.7226 - accuracy: 0.1919\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 616/3649 [====>.........................] - ETA: 9:59 - loss: 5.7225 - accuracy: 0.1916\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 617/3649 [====>.........................] - ETA: 9:58 - loss: 5.7134 - accuracy: 0.1929\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 618/3649 [====>.........................] - ETA: 9:58 - loss: 5.7113 - accuracy: 0.1926\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 619/3649 [====>.........................] - ETA: 9:58 - loss: 5.7067 - accuracy: 0.1922\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 620/3649 [====>.........................] - ETA: 9:58 - loss: 5.6985 - accuracy: 0.1935\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 621/3649 [====>.........................] - ETA: 9:58 - loss: 5.6899 - accuracy: 0.1948\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 622/3649 [====>.........................] - ETA: 9:57 - loss: 5.6855 - accuracy: 0.1945\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 623/3649 [====>.........................] - ETA: 9:57 - loss: 5.6811 - accuracy: 0.1942\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 624/3649 [====>.........................] - ETA: 9:57 - loss: 5.6762 - accuracy: 0.1939\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 625/3649 [====>.........................] - ETA: 9:57 - loss: 5.6710 - accuracy: 0.1936\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 626/3649 [====>.........................] - ETA: 9:56 - loss: 5.6650 - accuracy: 0.1949\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 627/3649 [====>.........................] - ETA: 9:56 - loss: 5.6592 - accuracy: 0.1962\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 628/3649 [====>.........................] - ETA: 9:56 - loss: 5.6526 - accuracy: 0.1975\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 629/3649 [====>.........................] - ETA: 9:56 - loss: 5.6468 - accuracy: 0.1971\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 630/3649 [====>.........................] - ETA: 9:56 - loss: 5.6411 - accuracy: 0.1968\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 631/3649 [====>.........................] - ETA: 9:55 - loss: 5.6330 - accuracy: 0.1981\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 632/3649 [====>.........................] - ETA: 9:55 - loss: 5.6407 - accuracy: 0.1978\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 633/3649 [====>.........................] - ETA: 9:55 - loss: 5.6423 - accuracy: 0.1975\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 634/3649 [====>.........................] - ETA: 9:55 - loss: 5.6404 - accuracy: 0.1972\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 635/3649 [====>.........................] - ETA: 9:55 - loss: 5.6336 - accuracy: 0.1984\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 636/3649 [====>.........................] - ETA: 9:54 - loss: 5.6407 - accuracy: 0.1981\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 637/3649 [====>.........................] - ETA: 9:54 - loss: 5.6383 - accuracy: 0.1978\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 638/3649 [====>.........................] - ETA: 9:54 - loss: 5.6314 - accuracy: 0.1991\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 639/3649 [====>.........................] - ETA: 9:54 - loss: 5.6334 - accuracy: 0.1987\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 640/3649 [====>.........................] - ETA: 9:53 - loss: 5.6313 - accuracy: 0.1984\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 641/3649 [====>.........................] - ETA: 9:53 - loss: 5.6253 - accuracy: 0.1997\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 642/3649 [====>.........................] - ETA: 9:53 - loss: 5.6205 - accuracy: 0.2009\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 643/3649 [====>.........................] - ETA: 9:53 - loss: 5.6171 - accuracy: 0.2006\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 644/3649 [====>.........................] - ETA: 9:53 - loss: 5.6120 - accuracy: 0.2003\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 645/3649 [====>.........................] - ETA: 9:52 - loss: 5.6077 - accuracy: 0.2000\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 646/3649 [====>.........................] - ETA: 9:52 - loss: 5.6085 - accuracy: 0.1997\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 647/3649 [====>.........................] - ETA: 9:52 - loss: 5.6043 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 648/3649 [====>.........................] - ETA: 9:52 - loss: 5.6030 - accuracy: 0.1991\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 649/3649 [====>.........................] - ETA: 9:52 - loss: 5.6039 - accuracy: 0.1988\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 650/3649 [====>.........................] - ETA: 9:51 - loss: 5.6004 - accuracy: 0.1985\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 651/3649 [====>.........................] - ETA: 9:51 - loss: 5.5959 - accuracy: 0.1982\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 652/3649 [====>.........................] - ETA: 9:51 - loss: 5.5922 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 653/3649 [====>.........................] - ETA: 9:51 - loss: 5.5898 - accuracy: 0.1991\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 654/3649 [====>.........................] - ETA: 9:50 - loss: 5.5871 - accuracy: 0.1988\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 655/3649 [====>.........................] - ETA: 9:50 - loss: 5.5872 - accuracy: 0.1985\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 656/3649 [====>.........................] - ETA: 9:50 - loss: 5.5847 - accuracy: 0.1982\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 657/3649 [====>.........................] - ETA: 9:50 - loss: 5.5789 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 658/3649 [====>.........................] - ETA: 9:50 - loss: 5.5772 - accuracy: 0.1991\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 659/3649 [====>.........................] - ETA: 9:49 - loss: 5.5745 - accuracy: 0.1988\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 660/3649 [====>.........................] - ETA: 9:49 - loss: 5.5708 - accuracy: 0.1985\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 661/3649 [====>.........................] - ETA: 9:49 - loss: 5.5706 - accuracy: 0.1982\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 662/3649 [====>.........................] - ETA: 9:49 - loss: 5.5687 - accuracy: 0.1979\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 663/3649 [====>.........................] - ETA: 9:49 - loss: 5.5685 - accuracy: 0.1976\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 664/3649 [====>.........................] - ETA: 9:48 - loss: 5.5707 - accuracy: 0.1973\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 665/3649 [====>.........................] - ETA: 9:48 - loss: 5.5666 - accuracy: 0.1970\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 666/3649 [====>.........................] - ETA: 9:48 - loss: 5.5771 - accuracy: 0.1967\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 667/3649 [====>.........................] - ETA: 9:48 - loss: 5.5813 - accuracy: 0.1964\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 668/3649 [====>.........................] - ETA: 9:47 - loss: 5.5739 - accuracy: 0.1976\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 669/3649 [====>.........................] - ETA: 9:47 - loss: 5.5710 - accuracy: 0.1973\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 670/3649 [====>.........................] - ETA: 9:47 - loss: 5.5646 - accuracy: 0.1985\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 671/3649 [====>.........................] - ETA: 9:47 - loss: 5.5746 - accuracy: 0.1982\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 672/3649 [====>.........................] - ETA: 9:47 - loss: 5.5703 - accuracy: 0.1994\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 673/3649 [====>.........................] - ETA: 9:46 - loss: 5.5628 - accuracy: 0.2006\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 674/3649 [====>.........................] - ETA: 9:46 - loss: 5.5632 - accuracy: 0.2003\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 675/3649 [====>.........................] - ETA: 9:46 - loss: 5.5564 - accuracy: 0.2015\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 676/3649 [====>.........................] - ETA: 9:46 - loss: 5.5573 - accuracy: 0.2012\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 677/3649 [====>.........................] - ETA: 9:46 - loss: 5.5502 - accuracy: 0.2024\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 678/3649 [====>.........................] - ETA: 9:45 - loss: 5.5453 - accuracy: 0.2021\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 679/3649 [====>.........................] - ETA: 9:45 - loss: 5.5451 - accuracy: 0.2018\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 680/3649 [====>.........................] - ETA: 9:45 - loss: 5.5560 - accuracy: 0.2015\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 681/3649 [====>.........................] - ETA: 9:45 - loss: 5.5548 - accuracy: 0.2012\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 682/3649 [====>.........................] - ETA: 9:44 - loss: 5.5568 - accuracy: 0.2009\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 683/3649 [====>.........................] - ETA: 9:44 - loss: 5.5659 - accuracy: 0.2006\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 684/3649 [====>.........................] - ETA: 9:44 - loss: 5.5623 - accuracy: 0.2003\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 685/3649 [====>.........................] - ETA: 9:44 - loss: 5.5561 - accuracy: 0.2015\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 686/3649 [====>.........................] - ETA: 9:44 - loss: 5.5537 - accuracy: 0.2012\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 687/3649 [====>.........................] - ETA: 9:43 - loss: 5.5545 - accuracy: 0.2009\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 688/3649 [====>.........................] - ETA: 9:43 - loss: 5.5480 - accuracy: 0.2020\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 689/3649 [====>.........................] - ETA: 9:43 - loss: 5.5491 - accuracy: 0.2017\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 690/3649 [====>.........................] - ETA: 9:43 - loss: 5.5462 - accuracy: 0.2029\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 691/3649 [====>.........................] - ETA: 9:43 - loss: 5.5455 - accuracy: 0.2026\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 692/3649 [====>.........................] - ETA: 9:42 - loss: 5.5550 - accuracy: 0.2023\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 693/3649 [====>.........................] - ETA: 9:42 - loss: 5.5484 - accuracy: 0.2035\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 694/3649 [====>.........................] - ETA: 9:42 - loss: 5.5470 - accuracy: 0.2032\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 695/3649 [====>.........................] - ETA: 9:42 - loss: 5.5452 - accuracy: 0.2029\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 696/3649 [====>.........................] - ETA: 9:42 - loss: 5.5401 - accuracy: 0.2026\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 697/3649 [====>.........................] - ETA: 9:41 - loss: 5.5398 - accuracy: 0.2023\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 698/3649 [====>.........................] - ETA: 9:41 - loss: 5.5389 - accuracy: 0.2020\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 699/3649 [====>.........................] - ETA: 9:41 - loss: 5.5312 - accuracy: 0.2031\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 700/3649 [====>.........................] - ETA: 9:41 - loss: 5.5277 - accuracy: 0.2029\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 701/3649 [====>.........................] - ETA: 9:41 - loss: 5.5296 - accuracy: 0.2026\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 702/3649 [====>.........................] - ETA: 9:40 - loss: 5.5304 - accuracy: 0.2023\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 703/3649 [====>.........................] - ETA: 9:40 - loss: 5.5307 - accuracy: 0.2020\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 704/3649 [====>.........................] - ETA: 9:40 - loss: 5.5267 - accuracy: 0.2017\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 705/3649 [====>.........................] - ETA: 9:40 - loss: 5.5285 - accuracy: 0.2014\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 706/3649 [====>.........................] - ETA: 9:40 - loss: 5.5210 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 707/3649 [====>.........................] - ETA: 9:40 - loss: 5.5236 - accuracy: 0.2023\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 708/3649 [====>.........................] - ETA: 9:41 - loss: 5.5223 - accuracy: 0.2020\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 709/3649 [====>.........................] - ETA: 9:41 - loss: 5.5146 - accuracy: 0.2031\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 710/3649 [====>.........................] - ETA: 9:41 - loss: 5.5242 - accuracy: 0.2028\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 711/3649 [====>.........................] - ETA: 9:41 - loss: 5.5220 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 712/3649 [====>.........................] - ETA: 9:41 - loss: 5.5151 - accuracy: 0.2037\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 713/3649 [====>.........................] - ETA: 9:40 - loss: 5.5143 - accuracy: 0.2034\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 714/3649 [====>.........................] - ETA: 9:40 - loss: 5.5126 - accuracy: 0.2031\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 715/3649 [====>.........................] - ETA: 9:40 - loss: 5.5071 - accuracy: 0.2028\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 716/3649 [====>.........................] - ETA: 9:40 - loss: 5.5079 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 717/3649 [====>.........................] - ETA: 9:40 - loss: 5.5110 - accuracy: 0.2022\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 718/3649 [====>.........................] - ETA: 9:39 - loss: 5.5034 - accuracy: 0.2033\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 719/3649 [====>.........................] - ETA: 9:39 - loss: 5.4993 - accuracy: 0.2031\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 720/3649 [====>.........................] - ETA: 9:39 - loss: 5.5079 - accuracy: 0.2028\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 721/3649 [====>.........................] - ETA: 9:39 - loss: 5.5052 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 722/3649 [====>.........................] - ETA: 9:38 - loss: 5.5047 - accuracy: 0.2022\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 723/3649 [====>.........................] - ETA: 9:38 - loss: 5.5135 - accuracy: 0.2019\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 724/3649 [====>.........................] - ETA: 9:38 - loss: 5.5114 - accuracy: 0.2017\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 725/3649 [====>.........................] - ETA: 9:38 - loss: 5.5098 - accuracy: 0.2014\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 726/3649 [====>.........................] - ETA: 9:38 - loss: 5.5025 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 727/3649 [====>.........................] - ETA: 9:37 - loss: 5.5035 - accuracy: 0.2022\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 728/3649 [====>.........................] - ETA: 9:37 - loss: 5.5036 - accuracy: 0.2019\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 729/3649 [====>.........................] - ETA: 9:37 - loss: 5.5048 - accuracy: 0.2016\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 730/3649 [=====>........................] - ETA: 9:37 - loss: 5.5141 - accuracy: 0.2014\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 731/3649 [=====>........................] - ETA: 9:37 - loss: 5.5067 - accuracy: 0.2025\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 732/3649 [=====>........................] - ETA: 9:36 - loss: 5.5022 - accuracy: 0.2036\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 733/3649 [=====>........................] - ETA: 9:36 - loss: 5.5021 - accuracy: 0.2033\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 734/3649 [=====>........................] - ETA: 9:36 - loss: 5.4947 - accuracy: 0.2044\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 735/3649 [=====>........................] - ETA: 9:36 - loss: 5.4944 - accuracy: 0.2041\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 736/3649 [=====>........................] - ETA: 9:36 - loss: 5.4939 - accuracy: 0.2038\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 737/3649 [=====>........................] - ETA: 9:35 - loss: 5.4999 - accuracy: 0.2035\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 738/3649 [=====>........................] - ETA: 9:35 - loss: 5.4934 - accuracy: 0.2046\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 739/3649 [=====>........................] - ETA: 9:35 - loss: 5.5020 - accuracy: 0.2043\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 740/3649 [=====>........................] - ETA: 9:35 - loss: 5.5093 - accuracy: 0.2041\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 741/3649 [=====>........................] - ETA: 9:35 - loss: 5.5020 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 742/3649 [=====>........................] - ETA: 9:34 - loss: 5.5047 - accuracy: 0.2049\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 743/3649 [=====>........................] - ETA: 9:34 - loss: 5.5080 - accuracy: 0.2046\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 744/3649 [=====>........................] - ETA: 9:34 - loss: 5.5166 - accuracy: 0.2043\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 745/3649 [=====>........................] - ETA: 9:34 - loss: 5.5093 - accuracy: 0.2054\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 746/3649 [=====>........................] - ETA: 9:34 - loss: 5.5091 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 747/3649 [=====>........................] - ETA: 9:33 - loss: 5.5093 - accuracy: 0.2048\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 748/3649 [=====>........................] - ETA: 9:33 - loss: 5.5098 - accuracy: 0.2045\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 749/3649 [=====>........................] - ETA: 9:33 - loss: 5.5063 - accuracy: 0.2056\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 750/3649 [=====>........................] - ETA: 9:33 - loss: 5.5075 - accuracy: 0.2053\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 751/3649 [=====>........................] - ETA: 9:33 - loss: 5.5064 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 752/3649 [=====>........................] - ETA: 9:32 - loss: 5.4992 - accuracy: 0.2061\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 753/3649 [=====>........................] - ETA: 9:32 - loss: 5.5074 - accuracy: 0.2058\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 754/3649 [=====>........................] - ETA: 9:32 - loss: 5.5041 - accuracy: 0.2056\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 755/3649 [=====>........................] - ETA: 9:32 - loss: 5.5117 - accuracy: 0.2053\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 756/3649 [=====>........................] - ETA: 9:32 - loss: 5.5133 - accuracy: 0.2050\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 757/3649 [=====>........................] - ETA: 9:31 - loss: 5.5199 - accuracy: 0.2048\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 758/3649 [=====>........................] - ETA: 9:31 - loss: 5.5162 - accuracy: 0.2045\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 759/3649 [=====>........................] - ETA: 9:31 - loss: 5.5179 - accuracy: 0.2042\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 760/3649 [=====>........................] - ETA: 9:31 - loss: 5.5162 - accuracy: 0.2053\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 761/3649 [=====>........................] - ETA: 9:31 - loss: 5.5113 - accuracy: 0.2050\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 762/3649 [=====>........................] - ETA: 9:30 - loss: 5.5146 - accuracy: 0.2047\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 763/3649 [=====>........................] - ETA: 9:30 - loss: 5.5117 - accuracy: 0.2045\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 764/3649 [=====>........................] - ETA: 9:30 - loss: 5.5121 - accuracy: 0.2042\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 765/3649 [=====>........................] - ETA: 9:30 - loss: 5.5204 - accuracy: 0.2039\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 766/3649 [=====>........................] - ETA: 9:30 - loss: 5.5133 - accuracy: 0.2050\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 767/3649 [=====>........................] - ETA: 9:29 - loss: 5.5131 - accuracy: 0.2047\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 768/3649 [=====>........................] - ETA: 9:29 - loss: 5.5129 - accuracy: 0.2044\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 769/3649 [=====>........................] - ETA: 9:29 - loss: 5.5113 - accuracy: 0.2042\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 770/3649 [=====>........................] - ETA: 9:29 - loss: 5.5042 - accuracy: 0.2052\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 771/3649 [=====>........................] - ETA: 9:29 - loss: 5.5099 - accuracy: 0.2049\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 772/3649 [=====>........................] - ETA: 9:28 - loss: 5.5081 - accuracy: 0.2047\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 773/3649 [=====>........................] - ETA: 9:28 - loss: 5.5017 - accuracy: 0.2057\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 774/3649 [=====>........................] - ETA: 9:28 - loss: 5.5022 - accuracy: 0.2054\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 775/3649 [=====>........................] - ETA: 9:28 - loss: 5.4961 - accuracy: 0.2065\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 776/3649 [=====>........................] - ETA: 9:28 - loss: 5.4954 - accuracy: 0.2062\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 777/3649 [=====>........................] - ETA: 9:27 - loss: 5.4903 - accuracy: 0.2059\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 778/3649 [=====>........................] - ETA: 9:27 - loss: 5.4902 - accuracy: 0.2057\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 779/3649 [=====>........................] - ETA: 9:27 - loss: 5.4890 - accuracy: 0.2054\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 780/3649 [=====>........................] - ETA: 9:27 - loss: 5.4888 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 781/3649 [=====>........................] - ETA: 9:27 - loss: 5.4891 - accuracy: 0.2049\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 782/3649 [=====>........................] - ETA: 9:26 - loss: 5.4823 - accuracy: 0.2059\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 783/3649 [=====>........................] - ETA: 9:26 - loss: 5.4838 - accuracy: 0.2056\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 784/3649 [=====>........................] - ETA: 9:26 - loss: 5.4823 - accuracy: 0.2054\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 785/3649 [=====>........................] - ETA: 9:26 - loss: 5.4883 - accuracy: 0.2051\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 786/3649 [=====>........................] - ETA: 9:26 - loss: 5.4865 - accuracy: 0.2061\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 787/3649 [=====>........................] - ETA: 9:25 - loss: 5.4892 - accuracy: 0.2058\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 788/3649 [=====>........................] - ETA: 9:25 - loss: 5.4824 - accuracy: 0.2069\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 789/3649 [=====>........................] - ETA: 9:25 - loss: 5.4821 - accuracy: 0.2066\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 790/3649 [=====>........................] - ETA: 9:25 - loss: 5.4788 - accuracy: 0.2063\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 791/3649 [=====>........................] - ETA: 9:25 - loss: 5.4863 - accuracy: 0.2061\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 792/3649 [=====>........................] - ETA: 9:24 - loss: 5.4796 - accuracy: 0.2071\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 793/3649 [=====>........................] - ETA: 9:24 - loss: 5.4769 - accuracy: 0.2068\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 794/3649 [=====>........................] - ETA: 9:24 - loss: 5.4725 - accuracy: 0.2065\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 795/3649 [=====>........................] - ETA: 9:24 - loss: 5.4666 - accuracy: 0.2075\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 796/3649 [=====>........................] - ETA: 9:24 - loss: 5.4605 - accuracy: 0.2085\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 797/3649 [=====>........................] - ETA: 9:24 - loss: 5.4568 - accuracy: 0.2083\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 798/3649 [=====>........................] - ETA: 9:23 - loss: 5.4527 - accuracy: 0.2080\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 799/3649 [=====>........................] - ETA: 9:23 - loss: 5.4484 - accuracy: 0.2078\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 800/3649 [=====>........................] - ETA: 9:23 - loss: 5.4440 - accuracy: 0.2075\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 801/3649 [=====>........................] - ETA: 9:23 - loss: 5.4390 - accuracy: 0.2072\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 802/3649 [=====>........................] - ETA: 9:23 - loss: 5.4342 - accuracy: 0.2082\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 803/3649 [=====>........................] - ETA: 9:22 - loss: 5.4287 - accuracy: 0.2092\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 804/3649 [=====>........................] - ETA: 9:22 - loss: 5.4238 - accuracy: 0.2090\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 805/3649 [=====>........................] - ETA: 9:22 - loss: 5.4189 - accuracy: 0.2099\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 806/3649 [=====>........................] - ETA: 9:22 - loss: 5.4125 - accuracy: 0.2109\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 807/3649 [=====>........................] - ETA: 9:22 - loss: 5.4178 - accuracy: 0.2107\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 808/3649 [=====>........................] - ETA: 9:21 - loss: 5.4171 - accuracy: 0.2104\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 809/3649 [=====>........................] - ETA: 9:21 - loss: 5.4188 - accuracy: 0.2101\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 810/3649 [=====>........................] - ETA: 9:21 - loss: 5.4273 - accuracy: 0.2099\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 811/3649 [=====>........................] - ETA: 9:21 - loss: 5.4354 - accuracy: 0.2096\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 812/3649 [=====>........................] - ETA: 9:21 - loss: 5.4433 - accuracy: 0.2094\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 813/3649 [=====>........................] - ETA: 9:20 - loss: 5.4428 - accuracy: 0.2091\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 814/3649 [=====>........................] - ETA: 9:20 - loss: 5.4433 - accuracy: 0.2088\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 815/3649 [=====>........................] - ETA: 9:20 - loss: 5.4451 - accuracy: 0.2086\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 816/3649 [=====>........................] - ETA: 9:20 - loss: 5.4439 - accuracy: 0.2083\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 817/3649 [=====>........................] - ETA: 9:20 - loss: 5.4385 - accuracy: 0.2093\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 818/3649 [=====>........................] - ETA: 9:19 - loss: 5.4433 - accuracy: 0.2090\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 819/3649 [=====>........................] - ETA: 9:19 - loss: 5.4409 - accuracy: 0.2088\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 820/3649 [=====>........................] - ETA: 9:19 - loss: 5.4353 - accuracy: 0.2098\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 821/3649 [=====>........................] - ETA: 9:19 - loss: 5.4362 - accuracy: 0.2095\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 822/3649 [=====>........................] - ETA: 9:19 - loss: 5.4341 - accuracy: 0.2092\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 823/3649 [=====>........................] - ETA: 9:18 - loss: 5.4320 - accuracy: 0.2090\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 824/3649 [=====>........................] - ETA: 9:18 - loss: 5.4277 - accuracy: 0.2100\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 825/3649 [=====>........................] - ETA: 9:18 - loss: 5.4221 - accuracy: 0.2109\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 826/3649 [=====>........................] - ETA: 9:18 - loss: 5.4160 - accuracy: 0.2119\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 827/3649 [=====>........................] - ETA: 9:18 - loss: 5.4098 - accuracy: 0.2128\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 828/3649 [=====>........................] - ETA: 9:17 - loss: 5.4037 - accuracy: 0.2138\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 829/3649 [=====>........................] - ETA: 9:17 - loss: 5.3977 - accuracy: 0.2147\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 830/3649 [=====>........................] - ETA: 9:17 - loss: 5.3921 - accuracy: 0.2157\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 831/3649 [=====>........................] - ETA: 9:17 - loss: 5.3865 - accuracy: 0.2166\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 832/3649 [=====>........................] - ETA: 9:17 - loss: 5.3862 - accuracy: 0.2163\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 833/3649 [=====>........................] - ETA: 9:16 - loss: 5.3897 - accuracy: 0.2161\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 834/3649 [=====>........................] - ETA: 9:16 - loss: 5.4005 - accuracy: 0.2158\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 835/3649 [=====>........................] - ETA: 9:16 - loss: 5.4020 - accuracy: 0.2156\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 836/3649 [=====>........................] - ETA: 9:16 - loss: 5.4060 - accuracy: 0.2153\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 837/3649 [=====>........................] - ETA: 9:16 - loss: 5.4133 - accuracy: 0.2151\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 838/3649 [=====>........................] - ETA: 9:15 - loss: 5.4091 - accuracy: 0.2160\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 839/3649 [=====>........................] - ETA: 9:15 - loss: 5.4037 - accuracy: 0.2169\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 840/3649 [=====>........................] - ETA: 9:15 - loss: 5.3975 - accuracy: 0.2179\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 841/3649 [=====>........................] - ETA: 9:15 - loss: 5.3911 - accuracy: 0.2188\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 842/3649 [=====>........................] - ETA: 9:15 - loss: 5.3848 - accuracy: 0.2197\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 843/3649 [=====>........................] - ETA: 9:14 - loss: 5.3785 - accuracy: 0.2206\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 844/3649 [=====>........................] - ETA: 9:14 - loss: 5.3722 - accuracy: 0.2216\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 845/3649 [=====>........................] - ETA: 9:14 - loss: 5.3659 - accuracy: 0.2225\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 846/3649 [=====>........................] - ETA: 9:14 - loss: 5.3596 - accuracy: 0.2234\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 847/3649 [=====>........................] - ETA: 9:14 - loss: 5.3533 - accuracy: 0.2243\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 848/3649 [=====>........................] - ETA: 9:14 - loss: 5.3519 - accuracy: 0.2241\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 849/3649 [=====>........................] - ETA: 9:13 - loss: 5.3512 - accuracy: 0.2238\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 850/3649 [=====>........................] - ETA: 9:13 - loss: 5.3525 - accuracy: 0.2235\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 851/3649 [=====>........................] - ETA: 9:13 - loss: 5.3563 - accuracy: 0.2233\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 852/3649 [======>.......................] - ETA: 9:13 - loss: 5.3567 - accuracy: 0.2230\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 853/3649 [======>.......................] - ETA: 9:13 - loss: 5.3585 - accuracy: 0.2227\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 854/3649 [======>.......................] - ETA: 9:12 - loss: 5.3589 - accuracy: 0.2225\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 855/3649 [======>.......................] - ETA: 9:12 - loss: 5.3586 - accuracy: 0.2222\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 856/3649 [======>.......................] - ETA: 9:12 - loss: 5.3591 - accuracy: 0.2220\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 857/3649 [======>.......................] - ETA: 9:12 - loss: 5.3555 - accuracy: 0.2217\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 858/3649 [======>.......................] - ETA: 9:12 - loss: 5.3498 - accuracy: 0.2226\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 859/3649 [======>.......................] - ETA: 9:11 - loss: 5.3438 - accuracy: 0.2235\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 860/3649 [======>.......................] - ETA: 9:11 - loss: 5.3378 - accuracy: 0.2244\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 861/3649 [======>.......................] - ETA: 9:11 - loss: 5.3319 - accuracy: 0.2253\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 862/3649 [======>.......................] - ETA: 9:11 - loss: 5.3260 - accuracy: 0.2262\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 863/3649 [======>.......................] - ETA: 9:11 - loss: 5.3201 - accuracy: 0.2271\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 864/3649 [======>.......................] - ETA: 9:10 - loss: 5.3142 - accuracy: 0.2280\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 865/3649 [======>.......................] - ETA: 9:10 - loss: 5.3083 - accuracy: 0.2289\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 866/3649 [======>.......................] - ETA: 9:10 - loss: 5.3025 - accuracy: 0.2298\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 867/3649 [======>.......................] - ETA: 9:10 - loss: 5.2966 - accuracy: 0.2307\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 868/3649 [======>.......................] - ETA: 9:10 - loss: 5.2907 - accuracy: 0.2316\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 869/3649 [======>.......................] - ETA: 9:09 - loss: 5.2848 - accuracy: 0.2325\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 870/3649 [======>.......................] - ETA: 9:09 - loss: 5.2851 - accuracy: 0.2322\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 871/3649 [======>.......................] - ETA: 9:09 - loss: 5.2821 - accuracy: 0.2319\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 872/3649 [======>.......................] - ETA: 9:09 - loss: 5.2845 - accuracy: 0.2317\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 873/3649 [======>.......................] - ETA: 9:09 - loss: 5.2789 - accuracy: 0.2325\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 874/3649 [======>.......................] - ETA: 9:08 - loss: 5.2732 - accuracy: 0.2334\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 875/3649 [======>.......................] - ETA: 9:08 - loss: 5.2675 - accuracy: 0.2343\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 876/3649 [======>.......................] - ETA: 9:08 - loss: 5.2618 - accuracy: 0.2352\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 877/3649 [======>.......................] - ETA: 9:08 - loss: 5.2561 - accuracy: 0.2360\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 878/3649 [======>.......................] - ETA: 9:08 - loss: 5.2503 - accuracy: 0.2369\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 879/3649 [======>.......................] - ETA: 9:08 - loss: 5.2446 - accuracy: 0.2378\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 880/3649 [======>.......................] - ETA: 9:07 - loss: 5.2388 - accuracy: 0.2386\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 881/3649 [======>.......................] - ETA: 9:07 - loss: 5.2331 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 882/3649 [======>.......................] - ETA: 9:07 - loss: 5.2273 - accuracy: 0.2404\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 883/3649 [======>.......................] - ETA: 9:07 - loss: 5.2216 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 884/3649 [======>.......................] - ETA: 9:07 - loss: 5.2158 - accuracy: 0.2421\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 885/3649 [======>.......................] - ETA: 9:06 - loss: 5.2101 - accuracy: 0.2429\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 886/3649 [======>.......................] - ETA: 9:06 - loss: 5.2043 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 887/3649 [======>.......................] - ETA: 9:06 - loss: 5.2027 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 888/3649 [======>.......................] - ETA: 9:06 - loss: 5.2040 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 889/3649 [======>.......................] - ETA: 9:06 - loss: 5.2007 - accuracy: 0.2430\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 890/3649 [======>.......................] - ETA: 9:05 - loss: 5.2011 - accuracy: 0.2427\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 891/3649 [======>.......................] - ETA: 9:05 - loss: 5.2002 - accuracy: 0.2424\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 892/3649 [======>.......................] - ETA: 9:05 - loss: 5.1994 - accuracy: 0.2422\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 893/3649 [======>.......................] - ETA: 9:05 - loss: 5.2089 - accuracy: 0.2419\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 894/3649 [======>.......................] - ETA: 9:05 - loss: 5.2178 - accuracy: 0.2416\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 895/3649 [======>.......................] - ETA: 9:04 - loss: 5.2179 - accuracy: 0.2413\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 896/3649 [======>.......................] - ETA: 9:04 - loss: 5.2168 - accuracy: 0.2411\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 897/3649 [======>.......................] - ETA: 9:04 - loss: 5.2163 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 898/3649 [======>.......................] - ETA: 9:04 - loss: 5.2148 - accuracy: 0.2405\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 899/3649 [======>.......................] - ETA: 9:04 - loss: 5.2141 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 900/3649 [======>.......................] - ETA: 9:03 - loss: 5.2211 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 901/3649 [======>.......................] - ETA: 9:03 - loss: 5.2279 - accuracy: 0.2397\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 902/3649 [======>.......................] - ETA: 9:03 - loss: 5.2262 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 903/3649 [======>.......................] - ETA: 9:03 - loss: 5.2241 - accuracy: 0.2392\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 904/3649 [======>.......................] - ETA: 9:03 - loss: 5.2238 - accuracy: 0.2389\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 905/3649 [======>.......................] - ETA: 9:02 - loss: 5.2251 - accuracy: 0.2387\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 906/3649 [======>.......................] - ETA: 9:02 - loss: 5.2247 - accuracy: 0.2384\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 907/3649 [======>.......................] - ETA: 9:02 - loss: 5.2222 - accuracy: 0.2381\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 908/3649 [======>.......................] - ETA: 9:02 - loss: 5.2223 - accuracy: 0.2379\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 909/3649 [======>.......................] - ETA: 9:02 - loss: 5.2286 - accuracy: 0.2376\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 910/3649 [======>.......................] - ETA: 9:01 - loss: 5.2277 - accuracy: 0.2374\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 911/3649 [======>.......................] - ETA: 9:01 - loss: 5.2256 - accuracy: 0.2371\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 912/3649 [======>.......................] - ETA: 9:01 - loss: 5.2286 - accuracy: 0.2368\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 913/3649 [======>.......................] - ETA: 9:01 - loss: 5.2266 - accuracy: 0.2366\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 914/3649 [======>.......................] - ETA: 9:01 - loss: 5.2332 - accuracy: 0.2363\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 915/3649 [======>.......................] - ETA: 9:00 - loss: 5.2341 - accuracy: 0.2361\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 916/3649 [======>.......................] - ETA: 9:00 - loss: 5.2351 - accuracy: 0.2358\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 917/3649 [======>.......................] - ETA: 9:00 - loss: 5.2323 - accuracy: 0.2356\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 918/3649 [======>.......................] - ETA: 9:00 - loss: 5.2281 - accuracy: 0.2364\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 919/3649 [======>.......................] - ETA: 9:00 - loss: 5.2246 - accuracy: 0.2372\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 920/3649 [======>.......................] - ETA: 9:00 - loss: 5.2241 - accuracy: 0.2370\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 921/3649 [======>.......................] - ETA: 8:59 - loss: 5.2260 - accuracy: 0.2367\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 922/3649 [======>.......................] - ETA: 8:59 - loss: 5.2255 - accuracy: 0.2364\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 923/3649 [======>.......................] - ETA: 8:59 - loss: 5.2321 - accuracy: 0.2362\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 924/3649 [======>.......................] - ETA: 8:59 - loss: 5.2330 - accuracy: 0.2359\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 925/3649 [======>.......................] - ETA: 8:59 - loss: 5.2325 - accuracy: 0.2357\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 926/3649 [======>.......................] - ETA: 8:58 - loss: 5.2330 - accuracy: 0.2354\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 927/3649 [======>.......................] - ETA: 8:58 - loss: 5.2338 - accuracy: 0.2352\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 928/3649 [======>.......................] - ETA: 8:58 - loss: 5.2347 - accuracy: 0.2349\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 929/3649 [======>.......................] - ETA: 8:58 - loss: 5.2303 - accuracy: 0.2357\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 930/3649 [======>.......................] - ETA: 8:58 - loss: 5.2257 - accuracy: 0.2366\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 931/3649 [======>.......................] - ETA: 8:57 - loss: 5.2260 - accuracy: 0.2363\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 932/3649 [======>.......................] - ETA: 8:57 - loss: 5.2251 - accuracy: 0.2361\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 933/3649 [======>.......................] - ETA: 8:57 - loss: 5.2255 - accuracy: 0.2358\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 934/3649 [======>.......................] - ETA: 8:57 - loss: 5.2255 - accuracy: 0.2355\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 935/3649 [======>.......................] - ETA: 8:57 - loss: 5.2256 - accuracy: 0.2353\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 936/3649 [======>.......................] - ETA: 8:56 - loss: 5.2209 - accuracy: 0.2361\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 937/3649 [======>.......................] - ETA: 8:56 - loss: 5.2190 - accuracy: 0.2359\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 938/3649 [======>.......................] - ETA: 8:56 - loss: 5.2154 - accuracy: 0.2356\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 939/3649 [======>.......................] - ETA: 8:56 - loss: 5.2136 - accuracy: 0.2354\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 940/3649 [======>.......................] - ETA: 8:56 - loss: 5.2213 - accuracy: 0.2351\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 941/3649 [======>.......................] - ETA: 8:55 - loss: 5.2195 - accuracy: 0.2349\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 942/3649 [======>.......................] - ETA: 8:55 - loss: 5.2190 - accuracy: 0.2346\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 943/3649 [======>.......................] - ETA: 8:55 - loss: 5.2147 - accuracy: 0.2354\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 944/3649 [======>.......................] - ETA: 8:55 - loss: 5.2227 - accuracy: 0.2352\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 945/3649 [======>.......................] - ETA: 8:55 - loss: 5.2203 - accuracy: 0.2349\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 946/3649 [======>.......................] - ETA: 8:54 - loss: 5.2179 - accuracy: 0.2347\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 947/3649 [======>.......................] - ETA: 8:54 - loss: 5.2127 - accuracy: 0.2355\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 948/3649 [======>.......................] - ETA: 8:54 - loss: 5.2074 - accuracy: 0.2363\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 949/3649 [======>.......................] - ETA: 8:54 - loss: 5.2023 - accuracy: 0.2371\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 950/3649 [======>.......................] - ETA: 8:54 - loss: 5.1973 - accuracy: 0.2379\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 951/3649 [======>.......................] - ETA: 8:53 - loss: 5.1924 - accuracy: 0.2387\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 952/3649 [======>.......................] - ETA: 8:53 - loss: 5.1876 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 953/3649 [======>.......................] - ETA: 8:53 - loss: 5.1827 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 954/3649 [======>.......................] - ETA: 8:53 - loss: 5.1777 - accuracy: 0.2411\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 955/3649 [======>.......................] - ETA: 8:53 - loss: 5.1771 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 956/3649 [======>.......................] - ETA: 8:52 - loss: 5.1734 - accuracy: 0.2406\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 957/3649 [======>.......................] - ETA: 8:52 - loss: 5.1757 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 958/3649 [======>.......................] - ETA: 8:52 - loss: 5.1723 - accuracy: 0.2411\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 959/3649 [======>.......................] - ETA: 8:52 - loss: 5.1742 - accuracy: 0.2409\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 960/3649 [======>.......................] - ETA: 8:52 - loss: 5.1703 - accuracy: 0.2417\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 961/3649 [======>.......................] - ETA: 8:52 - loss: 5.1653 - accuracy: 0.2425\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 962/3649 [======>.......................] - ETA: 8:51 - loss: 5.1600 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 963/3649 [======>.......................] - ETA: 8:51 - loss: 5.1548 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 964/3649 [======>.......................] - ETA: 8:51 - loss: 5.1495 - accuracy: 0.2448\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 965/3649 [======>.......................] - ETA: 8:51 - loss: 5.1442 - accuracy: 0.2456\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 966/3649 [======>.......................] - ETA: 8:51 - loss: 5.1390 - accuracy: 0.2464\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 967/3649 [======>.......................] - ETA: 8:50 - loss: 5.1337 - accuracy: 0.2472\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 968/3649 [======>.......................] - ETA: 8:50 - loss: 5.1285 - accuracy: 0.2479\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 969/3649 [======>.......................] - ETA: 8:50 - loss: 5.1232 - accuracy: 0.2487\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 970/3649 [======>.......................] - ETA: 8:50 - loss: 5.1217 - accuracy: 0.2485\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 971/3649 [======>.......................] - ETA: 8:50 - loss: 5.1245 - accuracy: 0.2482\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 972/3649 [======>.......................] - ETA: 8:49 - loss: 5.1219 - accuracy: 0.2479\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 973/3649 [======>.......................] - ETA: 8:49 - loss: 5.1259 - accuracy: 0.2477\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 974/3649 [=======>......................] - ETA: 8:49 - loss: 5.1258 - accuracy: 0.2474\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 975/3649 [=======>......................] - ETA: 8:49 - loss: 5.1240 - accuracy: 0.2472\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 976/3649 [=======>......................] - ETA: 8:49 - loss: 5.1301 - accuracy: 0.2469\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 977/3649 [=======>......................] - ETA: 8:48 - loss: 5.1277 - accuracy: 0.2467\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 978/3649 [=======>......................] - ETA: 8:48 - loss: 5.1257 - accuracy: 0.2464\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 979/3649 [=======>......................] - ETA: 8:48 - loss: 5.1246 - accuracy: 0.2462\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 980/3649 [=======>......................] - ETA: 8:48 - loss: 5.1238 - accuracy: 0.2459\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 981/3649 [=======>......................] - ETA: 8:48 - loss: 5.1251 - accuracy: 0.2457\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 982/3649 [=======>......................] - ETA: 8:47 - loss: 5.1222 - accuracy: 0.2454\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 983/3649 [=======>......................] - ETA: 8:47 - loss: 5.1191 - accuracy: 0.2462\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 984/3649 [=======>......................] - ETA: 8:47 - loss: 5.1169 - accuracy: 0.2459\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 985/3649 [=======>......................] - ETA: 8:47 - loss: 5.1174 - accuracy: 0.2457\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 986/3649 [=======>......................] - ETA: 8:47 - loss: 5.1130 - accuracy: 0.2465\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 987/3649 [=======>......................] - ETA: 8:46 - loss: 5.1103 - accuracy: 0.2462\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 988/3649 [=======>......................] - ETA: 8:46 - loss: 5.1129 - accuracy: 0.2460\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 989/3649 [=======>......................] - ETA: 8:46 - loss: 5.1182 - accuracy: 0.2457\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 990/3649 [=======>......................] - ETA: 8:46 - loss: 5.1157 - accuracy: 0.2455\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 991/3649 [=======>......................] - ETA: 8:46 - loss: 5.1188 - accuracy: 0.2452\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 992/3649 [=======>......................] - ETA: 8:45 - loss: 5.1194 - accuracy: 0.2450\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 993/3649 [=======>......................] - ETA: 8:45 - loss: 5.1249 - accuracy: 0.2447\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 994/3649 [=======>......................] - ETA: 8:45 - loss: 5.1316 - accuracy: 0.2445\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 995/3649 [=======>......................] - ETA: 8:45 - loss: 5.1348 - accuracy: 0.2442\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 996/3649 [=======>......................] - ETA: 8:45 - loss: 5.1393 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 997/3649 [=======>......................] - ETA: 8:44 - loss: 5.1402 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 998/3649 [=======>......................] - ETA: 8:44 - loss: 5.1402 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            " 999/3649 [=======>......................] - ETA: 8:44 - loss: 5.1383 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1000/3649 [=======>......................] - ETA: 8:44 - loss: 5.1447 - accuracy: 0.2430\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1001/3649 [=======>......................] - ETA: 8:44 - loss: 5.1464 - accuracy: 0.2428\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1002/3649 [=======>......................] - ETA: 8:43 - loss: 5.1516 - accuracy: 0.2425\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1003/3649 [=======>......................] - ETA: 8:43 - loss: 5.1524 - accuracy: 0.2423\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1004/3649 [=======>......................] - ETA: 8:43 - loss: 5.1541 - accuracy: 0.2420\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1005/3649 [=======>......................] - ETA: 8:43 - loss: 5.1540 - accuracy: 0.2418\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1006/3649 [=======>......................] - ETA: 8:43 - loss: 5.1580 - accuracy: 0.2416\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1007/3649 [=======>......................] - ETA: 8:42 - loss: 5.1531 - accuracy: 0.2423\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1008/3649 [=======>......................] - ETA: 8:42 - loss: 5.1545 - accuracy: 0.2421\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1009/3649 [=======>......................] - ETA: 8:42 - loss: 5.1512 - accuracy: 0.2428\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1010/3649 [=======>......................] - ETA: 8:42 - loss: 5.1529 - accuracy: 0.2426\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1011/3649 [=======>......................] - ETA: 8:42 - loss: 5.1594 - accuracy: 0.2423\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1012/3649 [=======>......................] - ETA: 8:41 - loss: 5.1591 - accuracy: 0.2421\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1013/3649 [=======>......................] - ETA: 8:41 - loss: 5.1560 - accuracy: 0.2428\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1014/3649 [=======>......................] - ETA: 8:41 - loss: 5.1609 - accuracy: 0.2426\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1015/3649 [=======>......................] - ETA: 8:41 - loss: 5.1635 - accuracy: 0.2424\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1016/3649 [=======>......................] - ETA: 8:41 - loss: 5.1664 - accuracy: 0.2421\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1017/3649 [=======>......................] - ETA: 8:40 - loss: 5.1672 - accuracy: 0.2419\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1018/3649 [=======>......................] - ETA: 8:40 - loss: 5.1693 - accuracy: 0.2417\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1019/3649 [=======>......................] - ETA: 8:40 - loss: 5.1694 - accuracy: 0.2414\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1020/3649 [=======>......................] - ETA: 8:40 - loss: 5.1718 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1021/3649 [=======>......................] - ETA: 8:40 - loss: 5.1669 - accuracy: 0.2419\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1022/3649 [=======>......................] - ETA: 8:39 - loss: 5.1732 - accuracy: 0.2417\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1023/3649 [=======>......................] - ETA: 8:39 - loss: 5.1733 - accuracy: 0.2414\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1024/3649 [=======>......................] - ETA: 8:39 - loss: 5.1749 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1025/3649 [=======>......................] - ETA: 8:39 - loss: 5.1808 - accuracy: 0.2410\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1026/3649 [=======>......................] - ETA: 8:39 - loss: 5.1830 - accuracy: 0.2407\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1027/3649 [=======>......................] - ETA: 8:38 - loss: 5.1891 - accuracy: 0.2405\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1028/3649 [=======>......................] - ETA: 8:38 - loss: 5.1886 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1029/3649 [=======>......................] - ETA: 8:38 - loss: 5.1879 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1030/3649 [=======>......................] - ETA: 8:38 - loss: 5.1837 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1031/3649 [=======>......................] - ETA: 8:38 - loss: 5.1808 - accuracy: 0.2415\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1032/3649 [=======>......................] - ETA: 8:37 - loss: 5.1872 - accuracy: 0.2413\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1033/3649 [=======>......................] - ETA: 8:37 - loss: 5.1898 - accuracy: 0.2410\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1034/3649 [=======>......................] - ETA: 8:37 - loss: 5.1892 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1035/3649 [=======>......................] - ETA: 8:37 - loss: 5.1954 - accuracy: 0.2406\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1036/3649 [=======>......................] - ETA: 8:37 - loss: 5.1942 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1037/3649 [=======>......................] - ETA: 8:36 - loss: 5.1943 - accuracy: 0.2401\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1038/3649 [=======>......................] - ETA: 8:36 - loss: 5.1943 - accuracy: 0.2399\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1039/3649 [=======>......................] - ETA: 8:36 - loss: 5.1999 - accuracy: 0.2397\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1040/3649 [=======>......................] - ETA: 8:36 - loss: 5.1951 - accuracy: 0.2404\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1041/3649 [=======>......................] - ETA: 8:36 - loss: 5.1981 - accuracy: 0.2402\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1042/3649 [=======>......................] - ETA: 8:36 - loss: 5.1995 - accuracy: 0.2399\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1043/3649 [=======>......................] - ETA: 8:35 - loss: 5.2024 - accuracy: 0.2397\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1044/3649 [=======>......................] - ETA: 8:35 - loss: 5.2041 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1045/3649 [=======>......................] - ETA: 8:35 - loss: 5.1998 - accuracy: 0.2402\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1046/3649 [=======>......................] - ETA: 8:35 - loss: 5.2011 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1047/3649 [=======>......................] - ETA: 8:35 - loss: 5.1990 - accuracy: 0.2397\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1048/3649 [=======>......................] - ETA: 8:34 - loss: 5.2046 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1049/3649 [=======>......................] - ETA: 8:34 - loss: 5.1998 - accuracy: 0.2402\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1050/3649 [=======>......................] - ETA: 8:34 - loss: 5.2017 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1051/3649 [=======>......................] - ETA: 8:34 - loss: 5.2024 - accuracy: 0.2398\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1052/3649 [=======>......................] - ETA: 8:34 - loss: 5.2079 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1053/3649 [=======>......................] - ETA: 8:33 - loss: 5.2037 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1054/3649 [=======>......................] - ETA: 8:33 - loss: 5.2028 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1055/3649 [=======>......................] - ETA: 8:33 - loss: 5.1983 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1056/3649 [=======>......................] - ETA: 8:33 - loss: 5.2038 - accuracy: 0.2405\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1057/3649 [=======>......................] - ETA: 8:33 - loss: 5.2000 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1058/3649 [=======>......................] - ETA: 8:32 - loss: 5.2020 - accuracy: 0.2410\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1059/3649 [=======>......................] - ETA: 8:32 - loss: 5.2008 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1060/3649 [=======>......................] - ETA: 8:32 - loss: 5.2018 - accuracy: 0.2406\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1061/3649 [=======>......................] - ETA: 8:32 - loss: 5.2008 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1062/3649 [=======>......................] - ETA: 8:32 - loss: 5.2062 - accuracy: 0.2401\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1063/3649 [=======>......................] - ETA: 8:31 - loss: 5.2095 - accuracy: 0.2399\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1064/3649 [=======>......................] - ETA: 8:31 - loss: 5.2085 - accuracy: 0.2397\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1065/3649 [=======>......................] - ETA: 8:31 - loss: 5.2104 - accuracy: 0.2394\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1066/3649 [=======>......................] - ETA: 8:31 - loss: 5.2085 - accuracy: 0.2402\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1067/3649 [=======>......................] - ETA: 8:31 - loss: 5.2062 - accuracy: 0.2409\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1068/3649 [=======>......................] - ETA: 8:30 - loss: 5.2073 - accuracy: 0.2406\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1069/3649 [=======>......................] - ETA: 8:30 - loss: 5.2056 - accuracy: 0.2413\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1070/3649 [=======>......................] - ETA: 8:30 - loss: 5.2115 - accuracy: 0.2411\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1071/3649 [=======>......................] - ETA: 8:30 - loss: 5.2071 - accuracy: 0.2418\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1072/3649 [=======>......................] - ETA: 8:30 - loss: 5.2078 - accuracy: 0.2416\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1073/3649 [=======>......................] - ETA: 8:29 - loss: 5.2139 - accuracy: 0.2414\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1074/3649 [=======>......................] - ETA: 8:29 - loss: 5.2163 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1075/3649 [=======>......................] - ETA: 8:29 - loss: 5.2188 - accuracy: 0.2409\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1076/3649 [=======>......................] - ETA: 8:29 - loss: 5.2193 - accuracy: 0.2407\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1077/3649 [=======>......................] - ETA: 8:29 - loss: 5.2247 - accuracy: 0.2405\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1078/3649 [=======>......................] - ETA: 8:28 - loss: 5.2231 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1079/3649 [=======>......................] - ETA: 8:28 - loss: 5.2256 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1080/3649 [=======>......................] - ETA: 8:28 - loss: 5.2316 - accuracy: 0.2398\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1081/3649 [=======>......................] - ETA: 8:28 - loss: 5.2310 - accuracy: 0.2396\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1082/3649 [=======>......................] - ETA: 8:28 - loss: 5.2267 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1083/3649 [=======>......................] - ETA: 8:28 - loss: 5.2243 - accuracy: 0.2410\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1084/3649 [=======>......................] - ETA: 8:27 - loss: 5.2197 - accuracy: 0.2417\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1085/3649 [=======>......................] - ETA: 8:27 - loss: 5.2251 - accuracy: 0.2415\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1086/3649 [=======>......................] - ETA: 8:27 - loss: 5.2255 - accuracy: 0.2413\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1087/3649 [=======>......................] - ETA: 8:27 - loss: 5.2266 - accuracy: 0.2410\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1088/3649 [=======>......................] - ETA: 8:27 - loss: 5.2316 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1089/3649 [=======>......................] - ETA: 8:26 - loss: 5.2325 - accuracy: 0.2406\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1090/3649 [=======>......................] - ETA: 8:26 - loss: 5.2348 - accuracy: 0.2404\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1091/3649 [=======>......................] - ETA: 8:26 - loss: 5.2328 - accuracy: 0.2401\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1092/3649 [=======>......................] - ETA: 8:26 - loss: 5.2364 - accuracy: 0.2399\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1093/3649 [=======>......................] - ETA: 8:26 - loss: 5.2367 - accuracy: 0.2397\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1094/3649 [=======>......................] - ETA: 8:25 - loss: 5.2349 - accuracy: 0.2404\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1095/3649 [========>.....................] - ETA: 8:25 - loss: 5.2303 - accuracy: 0.2411\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1096/3649 [========>.....................] - ETA: 8:25 - loss: 5.2354 - accuracy: 0.2409\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1097/3649 [========>.....................] - ETA: 8:25 - loss: 5.2332 - accuracy: 0.2407\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1098/3649 [========>.....................] - ETA: 8:25 - loss: 5.2340 - accuracy: 0.2404\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1099/3649 [========>.....................] - ETA: 8:24 - loss: 5.2400 - accuracy: 0.2402\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1100/3649 [========>.....................] - ETA: 8:24 - loss: 5.2387 - accuracy: 0.2400\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1101/3649 [========>.....................] - ETA: 8:24 - loss: 5.2412 - accuracy: 0.2398\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1102/3649 [========>.....................] - ETA: 8:24 - loss: 5.2426 - accuracy: 0.2396\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1103/3649 [========>.....................] - ETA: 8:24 - loss: 5.2421 - accuracy: 0.2393\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1104/3649 [========>.....................] - ETA: 8:23 - loss: 5.2399 - accuracy: 0.2391\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1105/3649 [========>.....................] - ETA: 8:23 - loss: 5.2451 - accuracy: 0.2389\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1106/3649 [========>.....................] - ETA: 8:23 - loss: 5.2410 - accuracy: 0.2396\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1107/3649 [========>.....................] - ETA: 8:23 - loss: 5.2419 - accuracy: 0.2394\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1108/3649 [========>.....................] - ETA: 8:23 - loss: 5.2396 - accuracy: 0.2392\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1109/3649 [========>.....................] - ETA: 8:22 - loss: 5.2410 - accuracy: 0.2390\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1110/3649 [========>.....................] - ETA: 8:22 - loss: 5.2468 - accuracy: 0.2387\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1111/3649 [========>.....................] - ETA: 8:22 - loss: 5.2477 - accuracy: 0.2385\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1112/3649 [========>.....................] - ETA: 8:22 - loss: 5.2461 - accuracy: 0.2383\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1113/3649 [========>.....................] - ETA: 8:22 - loss: 5.2417 - accuracy: 0.2390\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1114/3649 [========>.....................] - ETA: 8:21 - loss: 5.2426 - accuracy: 0.2388\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1115/3649 [========>.....................] - ETA: 8:21 - loss: 5.2384 - accuracy: 0.2395\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1116/3649 [========>.....................] - ETA: 8:21 - loss: 5.2435 - accuracy: 0.2392\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1117/3649 [========>.....................] - ETA: 8:21 - loss: 5.2408 - accuracy: 0.2390\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1118/3649 [========>.....................] - ETA: 8:21 - loss: 5.2459 - accuracy: 0.2388\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1119/3649 [========>.....................] - ETA: 8:20 - loss: 5.2513 - accuracy: 0.2386\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1120/3649 [========>.....................] - ETA: 8:20 - loss: 5.2502 - accuracy: 0.2384\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1121/3649 [========>.....................] - ETA: 8:20 - loss: 5.2496 - accuracy: 0.2382\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1122/3649 [========>.....................] - ETA: 8:20 - loss: 5.2453 - accuracy: 0.2389\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1123/3649 [========>.....................] - ETA: 8:20 - loss: 5.2504 - accuracy: 0.2386\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1124/3649 [========>.....................] - ETA: 8:19 - loss: 5.2505 - accuracy: 0.2384\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1125/3649 [========>.....................] - ETA: 8:19 - loss: 5.2485 - accuracy: 0.2391\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1126/3649 [========>.....................] - ETA: 8:19 - loss: 5.2490 - accuracy: 0.2389\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1127/3649 [========>.....................] - ETA: 8:19 - loss: 5.2447 - accuracy: 0.2396\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1128/3649 [========>.....................] - ETA: 8:19 - loss: 5.2465 - accuracy: 0.2394\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1129/3649 [========>.....................] - ETA: 8:18 - loss: 5.2462 - accuracy: 0.2391\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1130/3649 [========>.....................] - ETA: 8:18 - loss: 5.2419 - accuracy: 0.2398\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1131/3649 [========>.....................] - ETA: 8:18 - loss: 5.2464 - accuracy: 0.2396\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1132/3649 [========>.....................] - ETA: 8:18 - loss: 5.2421 - accuracy: 0.2403\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1133/3649 [========>.....................] - ETA: 8:18 - loss: 5.2397 - accuracy: 0.2401\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1134/3649 [========>.....................] - ETA: 8:17 - loss: 5.2431 - accuracy: 0.2399\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1135/3649 [========>.....................] - ETA: 8:17 - loss: 5.2387 - accuracy: 0.2405\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1136/3649 [========>.....................] - ETA: 8:17 - loss: 5.2365 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1137/3649 [========>.....................] - ETA: 8:17 - loss: 5.2407 - accuracy: 0.2410\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1138/3649 [========>.....................] - ETA: 8:17 - loss: 5.2402 - accuracy: 0.2408\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1139/3649 [========>.....................] - ETA: 8:16 - loss: 5.2358 - accuracy: 0.2414\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1140/3649 [========>.....................] - ETA: 8:16 - loss: 5.2359 - accuracy: 0.2412\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1141/3649 [========>.....................] - ETA: 8:16 - loss: 5.2315 - accuracy: 0.2419\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1142/3649 [========>.....................] - ETA: 8:16 - loss: 5.2323 - accuracy: 0.2417\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1143/3649 [========>.....................] - ETA: 8:16 - loss: 5.2278 - accuracy: 0.2423\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1144/3649 [========>.....................] - ETA: 8:15 - loss: 5.2277 - accuracy: 0.2421\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1145/3649 [========>.....................] - ETA: 8:15 - loss: 5.2259 - accuracy: 0.2428\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1146/3649 [========>.....................] - ETA: 8:15 - loss: 5.2290 - accuracy: 0.2426\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1147/3649 [========>.....................] - ETA: 8:15 - loss: 5.2248 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1148/3649 [========>.....................] - ETA: 8:15 - loss: 5.2297 - accuracy: 0.2430\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1149/3649 [========>.....................] - ETA: 8:14 - loss: 5.2254 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1150/3649 [========>.....................] - ETA: 8:14 - loss: 5.2269 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1151/3649 [========>.....................] - ETA: 8:14 - loss: 5.2253 - accuracy: 0.2433\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1152/3649 [========>.....................] - ETA: 8:14 - loss: 5.2222 - accuracy: 0.2439\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1153/3649 [========>.....................] - ETA: 8:14 - loss: 5.2243 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1154/3649 [========>.....................] - ETA: 8:13 - loss: 5.2302 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1155/3649 [========>.....................] - ETA: 8:13 - loss: 5.2315 - accuracy: 0.2433\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1156/3649 [========>.....................] - ETA: 8:13 - loss: 5.2369 - accuracy: 0.2431\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1157/3649 [========>.....................] - ETA: 8:13 - loss: 5.2325 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1158/3649 [========>.....................] - ETA: 8:13 - loss: 5.2343 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1159/3649 [========>.....................] - ETA: 8:12 - loss: 5.2324 - accuracy: 0.2433\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1160/3649 [========>.....................] - ETA: 8:12 - loss: 5.2281 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1161/3649 [========>.....................] - ETA: 8:12 - loss: 5.2268 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1162/3649 [========>.....................] - ETA: 8:12 - loss: 5.2271 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1163/3649 [========>.....................] - ETA: 8:12 - loss: 5.2287 - accuracy: 0.2433\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1164/3649 [========>.....................] - ETA: 8:11 - loss: 5.2299 - accuracy: 0.2431\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1165/3649 [========>.....................] - ETA: 8:11 - loss: 5.2330 - accuracy: 0.2429\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1166/3649 [========>.....................] - ETA: 8:11 - loss: 5.2289 - accuracy: 0.2436\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1167/3649 [========>.....................] - ETA: 8:11 - loss: 5.2264 - accuracy: 0.2442\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1168/3649 [========>.....................] - ETA: 8:11 - loss: 5.2280 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1169/3649 [========>.....................] - ETA: 8:10 - loss: 5.2239 - accuracy: 0.2447\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1170/3649 [========>.....................] - ETA: 8:10 - loss: 5.2227 - accuracy: 0.2444\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1171/3649 [========>.....................] - ETA: 8:10 - loss: 5.2221 - accuracy: 0.2442\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1172/3649 [========>.....................] - ETA: 8:10 - loss: 5.2240 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1173/3649 [========>.....................] - ETA: 8:10 - loss: 5.2292 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1174/3649 [========>.....................] - ETA: 8:09 - loss: 5.2341 - accuracy: 0.2436\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1175/3649 [========>.....................] - ETA: 8:09 - loss: 5.2331 - accuracy: 0.2434\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1176/3649 [========>.....................] - ETA: 8:09 - loss: 5.2304 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1177/3649 [========>.....................] - ETA: 8:09 - loss: 5.2304 - accuracy: 0.2430\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1178/3649 [========>.....................] - ETA: 8:09 - loss: 5.2343 - accuracy: 0.2428\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1179/3649 [========>.....................] - ETA: 8:09 - loss: 5.2300 - accuracy: 0.2434\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1180/3649 [========>.....................] - ETA: 8:08 - loss: 5.2353 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1181/3649 [========>.....................] - ETA: 8:08 - loss: 5.2329 - accuracy: 0.2439\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1182/3649 [========>.....................] - ETA: 8:08 - loss: 5.2320 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1183/3649 [========>.....................] - ETA: 8:08 - loss: 5.2290 - accuracy: 0.2443\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1184/3649 [========>.....................] - ETA: 8:08 - loss: 5.2310 - accuracy: 0.2441\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1185/3649 [========>.....................] - ETA: 8:07 - loss: 5.2316 - accuracy: 0.2439\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1186/3649 [========>.....................] - ETA: 8:07 - loss: 5.2275 - accuracy: 0.2445\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1187/3649 [========>.....................] - ETA: 8:07 - loss: 5.2299 - accuracy: 0.2443\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1188/3649 [========>.....................] - ETA: 8:07 - loss: 5.2290 - accuracy: 0.2441\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1189/3649 [========>.....................] - ETA: 8:07 - loss: 5.2246 - accuracy: 0.2447\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1190/3649 [========>.....................] - ETA: 8:06 - loss: 5.2273 - accuracy: 0.2445\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1191/3649 [========>.....................] - ETA: 8:06 - loss: 5.2259 - accuracy: 0.2443\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1192/3649 [========>.....................] - ETA: 8:06 - loss: 5.2267 - accuracy: 0.2441\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1193/3649 [========>.....................] - ETA: 8:06 - loss: 5.2248 - accuracy: 0.2439\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1194/3649 [========>.....................] - ETA: 8:06 - loss: 5.2219 - accuracy: 0.2446\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1195/3649 [========>.....................] - ETA: 8:05 - loss: 5.2202 - accuracy: 0.2444\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1196/3649 [========>.....................] - ETA: 8:05 - loss: 5.2198 - accuracy: 0.2441\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1197/3649 [========>.....................] - ETA: 8:05 - loss: 5.2193 - accuracy: 0.2439\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1198/3649 [========>.....................] - ETA: 8:05 - loss: 5.2192 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1199/3649 [========>.....................] - ETA: 8:05 - loss: 5.2199 - accuracy: 0.2435\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1200/3649 [========>.....................] - ETA: 8:04 - loss: 5.2168 - accuracy: 0.2442\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1201/3649 [========>.....................] - ETA: 8:04 - loss: 5.2174 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1202/3649 [========>.....................] - ETA: 8:04 - loss: 5.2231 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1203/3649 [========>.....................] - ETA: 8:04 - loss: 5.2288 - accuracy: 0.2436\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1204/3649 [========>.....................] - ETA: 8:04 - loss: 5.2247 - accuracy: 0.2442\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1205/3649 [========>.....................] - ETA: 8:03 - loss: 5.2257 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1206/3649 [========>.....................] - ETA: 8:03 - loss: 5.2257 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1207/3649 [========>.....................] - ETA: 8:03 - loss: 5.2263 - accuracy: 0.2436\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1208/3649 [========>.....................] - ETA: 8:03 - loss: 5.2249 - accuracy: 0.2434\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1209/3649 [========>.....................] - ETA: 8:03 - loss: 5.2207 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1210/3649 [========>.....................] - ETA: 8:03 - loss: 5.2238 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1211/3649 [========>.....................] - ETA: 8:02 - loss: 5.2221 - accuracy: 0.2436\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1212/3649 [========>.....................] - ETA: 8:02 - loss: 5.2180 - accuracy: 0.2442\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1213/3649 [========>.....................] - ETA: 8:02 - loss: 5.2177 - accuracy: 0.2440\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1214/3649 [========>.....................] - ETA: 8:02 - loss: 5.2165 - accuracy: 0.2438\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1215/3649 [========>.....................] - ETA: 8:02 - loss: 5.2175 - accuracy: 0.2436\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1216/3649 [========>.....................] - ETA: 8:01 - loss: 5.2171 - accuracy: 0.2434\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1217/3649 [=========>....................] - ETA: 8:01 - loss: 5.2171 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1218/3649 [=========>....................] - ETA: 8:01 - loss: 5.2150 - accuracy: 0.2430\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1219/3649 [=========>....................] - ETA: 8:01 - loss: 5.2178 - accuracy: 0.2428\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1220/3649 [=========>....................] - ETA: 8:01 - loss: 5.2146 - accuracy: 0.2434\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1221/3649 [=========>....................] - ETA: 8:00 - loss: 5.2129 - accuracy: 0.2432\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1222/3649 [=========>....................] - ETA: 8:00 - loss: 5.2087 - accuracy: 0.2439\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1223/3649 [=========>....................] - ETA: 8:00 - loss: 5.2077 - accuracy: 0.2437\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1224/3649 [=========>....................] - ETA: 8:00 - loss: 5.2038 - accuracy: 0.2443\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1225/3649 [=========>....................] - ETA: 8:00 - loss: 5.2027 - accuracy: 0.2441\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1226/3649 [=========>....................] - ETA: 7:59 - loss: 5.1985 - accuracy: 0.2447\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1227/3649 [=========>....................] - ETA: 7:59 - loss: 5.1994 - accuracy: 0.2445\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1228/3649 [=========>....................] - ETA: 7:59 - loss: 5.2004 - accuracy: 0.2443\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1229/3649 [=========>....................] - ETA: 7:59 - loss: 5.1976 - accuracy: 0.2449\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1230/3649 [=========>....................] - ETA: 7:59 - loss: 5.1992 - accuracy: 0.2447\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1231/3649 [=========>....................] - ETA: 7:58 - loss: 5.1970 - accuracy: 0.2445\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1232/3649 [=========>....................] - ETA: 7:58 - loss: 5.1928 - accuracy: 0.2451\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1233/3649 [=========>....................] - ETA: 7:58 - loss: 5.1946 - accuracy: 0.2449\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1234/3649 [=========>....................] - ETA: 7:58 - loss: 5.1906 - accuracy: 0.2455\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1235/3649 [=========>....................] - ETA: 7:58 - loss: 5.1966 - accuracy: 0.2453\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1236/3649 [=========>....................] - ETA: 7:57 - loss: 5.1952 - accuracy: 0.2451\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1237/3649 [=========>....................] - ETA: 7:57 - loss: 5.1938 - accuracy: 0.2449\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1238/3649 [=========>....................] - ETA: 7:57 - loss: 5.1897 - accuracy: 0.2456\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1239/3649 [=========>....................] - ETA: 7:57 - loss: 5.1905 - accuracy: 0.2454\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1240/3649 [=========>....................] - ETA: 7:57 - loss: 5.1894 - accuracy: 0.2452\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1241/3649 [=========>....................] - ETA: 7:56 - loss: 5.1855 - accuracy: 0.2458\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1242/3649 [=========>....................] - ETA: 7:56 - loss: 5.1836 - accuracy: 0.2456\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1243/3649 [=========>....................] - ETA: 7:56 - loss: 5.1852 - accuracy: 0.2454\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1244/3649 [=========>....................] - ETA: 7:56 - loss: 5.1834 - accuracy: 0.2452\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1245/3649 [=========>....................] - ETA: 7:56 - loss: 5.1820 - accuracy: 0.2450\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1246/3649 [=========>....................] - ETA: 7:55 - loss: 5.1780 - accuracy: 0.2456\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1247/3649 [=========>....................] - ETA: 7:55 - loss: 5.1752 - accuracy: 0.2462\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1248/3649 [=========>....................] - ETA: 7:55 - loss: 5.1713 - accuracy: 0.2468\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1249/3649 [=========>....................] - ETA: 7:55 - loss: 5.1712 - accuracy: 0.2466\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1250/3649 [=========>....................] - ETA: 7:55 - loss: 5.1678 - accuracy: 0.2472\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1251/3649 [=========>....................] - ETA: 7:54 - loss: 5.1692 - accuracy: 0.2470\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1252/3649 [=========>....................] - ETA: 7:54 - loss: 5.1707 - accuracy: 0.2468\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1253/3649 [=========>....................] - ETA: 7:54 - loss: 5.1707 - accuracy: 0.2466\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1254/3649 [=========>....................] - ETA: 7:54 - loss: 5.1739 - accuracy: 0.2464\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1255/3649 [=========>....................] - ETA: 7:54 - loss: 5.1701 - accuracy: 0.2470\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1256/3649 [=========>....................] - ETA: 7:53 - loss: 5.1703 - accuracy: 0.2468\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1257/3649 [=========>....................] - ETA: 7:53 - loss: 5.1663 - accuracy: 0.2474\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1258/3649 [=========>....................] - ETA: 7:53 - loss: 5.1659 - accuracy: 0.2472\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1259/3649 [=========>....................] - ETA: 7:53 - loss: 5.1649 - accuracy: 0.2470\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1260/3649 [=========>....................] - ETA: 7:53 - loss: 5.1615 - accuracy: 0.2476\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1261/3649 [=========>....................] - ETA: 7:52 - loss: 5.1608 - accuracy: 0.2474\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1262/3649 [=========>....................] - ETA: 7:52 - loss: 5.1607 - accuracy: 0.2472\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1263/3649 [=========>....................] - ETA: 7:52 - loss: 5.1594 - accuracy: 0.2470\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1264/3649 [=========>....................] - ETA: 7:52 - loss: 5.1555 - accuracy: 0.2476\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1265/3649 [=========>....................] - ETA: 7:52 - loss: 5.1521 - accuracy: 0.2482\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1266/3649 [=========>....................] - ETA: 7:51 - loss: 5.1486 - accuracy: 0.2488\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1267/3649 [=========>....................] - ETA: 7:51 - loss: 5.1508 - accuracy: 0.2486\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1268/3649 [=========>....................] - ETA: 7:51 - loss: 5.1471 - accuracy: 0.2492\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1269/3649 [=========>....................] - ETA: 7:51 - loss: 5.1463 - accuracy: 0.2490\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1270/3649 [=========>....................] - ETA: 7:51 - loss: 5.1424 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1271/3649 [=========>....................] - ETA: 7:50 - loss: 5.1405 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1272/3649 [=========>....................] - ETA: 7:50 - loss: 5.1394 - accuracy: 0.2492\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1273/3649 [=========>....................] - ETA: 7:50 - loss: 5.1368 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1274/3649 [=========>....................] - ETA: 7:50 - loss: 5.1433 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1275/3649 [=========>....................] - ETA: 7:50 - loss: 5.1395 - accuracy: 0.2502\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1276/3649 [=========>....................] - ETA: 7:49 - loss: 5.1401 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1277/3649 [=========>....................] - ETA: 7:49 - loss: 5.1407 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1278/3649 [=========>....................] - ETA: 7:49 - loss: 5.1398 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1279/3649 [=========>....................] - ETA: 7:49 - loss: 5.1360 - accuracy: 0.2502\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1280/3649 [=========>....................] - ETA: 7:49 - loss: 5.1361 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1281/3649 [=========>....................] - ETA: 7:48 - loss: 5.1342 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1282/3649 [=========>....................] - ETA: 7:48 - loss: 5.1322 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1283/3649 [=========>....................] - ETA: 7:48 - loss: 5.1329 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1284/3649 [=========>....................] - ETA: 7:48 - loss: 5.1340 - accuracy: 0.2492\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1285/3649 [=========>....................] - ETA: 7:48 - loss: 5.1329 - accuracy: 0.2490\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1286/3649 [=========>....................] - ETA: 7:47 - loss: 5.1315 - accuracy: 0.2488\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1287/3649 [=========>....................] - ETA: 7:47 - loss: 5.1329 - accuracy: 0.2486\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1288/3649 [=========>....................] - ETA: 7:47 - loss: 5.1333 - accuracy: 0.2484\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1289/3649 [=========>....................] - ETA: 7:47 - loss: 5.1329 - accuracy: 0.2483\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1290/3649 [=========>....................] - ETA: 7:47 - loss: 5.1291 - accuracy: 0.2488\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1291/3649 [=========>....................] - ETA: 7:46 - loss: 5.1288 - accuracy: 0.2486\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1292/3649 [=========>....................] - ETA: 7:46 - loss: 5.1278 - accuracy: 0.2485\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1293/3649 [=========>....................] - ETA: 7:46 - loss: 5.1280 - accuracy: 0.2483\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1294/3649 [=========>....................] - ETA: 7:46 - loss: 5.1300 - accuracy: 0.2481\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1295/3649 [=========>....................] - ETA: 7:46 - loss: 5.1283 - accuracy: 0.2479\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1296/3649 [=========>....................] - ETA: 7:45 - loss: 5.1282 - accuracy: 0.2477\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1297/3649 [=========>....................] - ETA: 7:45 - loss: 5.1262 - accuracy: 0.2475\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1298/3649 [=========>....................] - ETA: 7:45 - loss: 5.1223 - accuracy: 0.2481\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1299/3649 [=========>....................] - ETA: 7:45 - loss: 5.1278 - accuracy: 0.2479\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1300/3649 [=========>....................] - ETA: 7:45 - loss: 5.1264 - accuracy: 0.2477\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1301/3649 [=========>....................] - ETA: 7:44 - loss: 5.1227 - accuracy: 0.2483\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1302/3649 [=========>....................] - ETA: 7:44 - loss: 5.1280 - accuracy: 0.2481\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1303/3649 [=========>....................] - ETA: 7:44 - loss: 5.1255 - accuracy: 0.2487\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1304/3649 [=========>....................] - ETA: 7:44 - loss: 5.1267 - accuracy: 0.2485\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1305/3649 [=========>....................] - ETA: 7:44 - loss: 5.1261 - accuracy: 0.2483\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1306/3649 [=========>....................] - ETA: 7:43 - loss: 5.1316 - accuracy: 0.2481\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1307/3649 [=========>....................] - ETA: 7:43 - loss: 5.1314 - accuracy: 0.2479\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1308/3649 [=========>....................] - ETA: 7:43 - loss: 5.1290 - accuracy: 0.2485\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1309/3649 [=========>....................] - ETA: 7:43 - loss: 5.1341 - accuracy: 0.2483\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1310/3649 [=========>....................] - ETA: 7:43 - loss: 5.1349 - accuracy: 0.2481\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1311/3649 [=========>....................] - ETA: 7:42 - loss: 5.1311 - accuracy: 0.2487\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1312/3649 [=========>....................] - ETA: 7:42 - loss: 5.1307 - accuracy: 0.2485\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1313/3649 [=========>....................] - ETA: 7:42 - loss: 5.1280 - accuracy: 0.2490\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1314/3649 [=========>....................] - ETA: 7:42 - loss: 5.1256 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1315/3649 [=========>....................] - ETA: 7:42 - loss: 5.1277 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1316/3649 [=========>....................] - ETA: 7:41 - loss: 5.1302 - accuracy: 0.2492\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1317/3649 [=========>....................] - ETA: 7:41 - loss: 5.1284 - accuracy: 0.2491\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1318/3649 [=========>....................] - ETA: 7:41 - loss: 5.1327 - accuracy: 0.2489\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1319/3649 [=========>....................] - ETA: 7:41 - loss: 5.1291 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1320/3649 [=========>....................] - ETA: 7:41 - loss: 5.1311 - accuracy: 0.2492\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1321/3649 [=========>....................] - ETA: 7:40 - loss: 5.1307 - accuracy: 0.2491\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1322/3649 [=========>....................] - ETA: 7:40 - loss: 5.1356 - accuracy: 0.2489\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1323/3649 [=========>....................] - ETA: 7:40 - loss: 5.1372 - accuracy: 0.2487\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1324/3649 [=========>....................] - ETA: 7:40 - loss: 5.1363 - accuracy: 0.2485\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1325/3649 [=========>....................] - ETA: 7:40 - loss: 5.1336 - accuracy: 0.2491\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1326/3649 [=========>....................] - ETA: 7:39 - loss: 5.1378 - accuracy: 0.2489\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1327/3649 [=========>....................] - ETA: 7:39 - loss: 5.1382 - accuracy: 0.2487\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1328/3649 [=========>....................] - ETA: 7:39 - loss: 5.1346 - accuracy: 0.2492\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1329/3649 [=========>....................] - ETA: 7:39 - loss: 5.1359 - accuracy: 0.2491\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1330/3649 [=========>....................] - ETA: 7:39 - loss: 5.1333 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1331/3649 [=========>....................] - ETA: 7:38 - loss: 5.1342 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1332/3649 [=========>....................] - ETA: 7:38 - loss: 5.1308 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1333/3649 [=========>....................] - ETA: 7:38 - loss: 5.1283 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1334/3649 [=========>....................] - ETA: 7:38 - loss: 5.1291 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1335/3649 [=========>....................] - ETA: 7:38 - loss: 5.1256 - accuracy: 0.2502\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1336/3649 [=========>....................] - ETA: 7:38 - loss: 5.1242 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1337/3649 [=========>....................] - ETA: 7:37 - loss: 5.1206 - accuracy: 0.2506\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1338/3649 [==========>...................] - ETA: 7:37 - loss: 5.1207 - accuracy: 0.2504\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1339/3649 [==========>...................] - ETA: 7:37 - loss: 5.1233 - accuracy: 0.2502\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1340/3649 [==========>...................] - ETA: 7:37 - loss: 5.1235 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1341/3649 [==========>...................] - ETA: 7:37 - loss: 5.1230 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1342/3649 [==========>...................] - ETA: 7:36 - loss: 5.1242 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1343/3649 [==========>...................] - ETA: 7:36 - loss: 5.1216 - accuracy: 0.2502\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1344/3649 [==========>...................] - ETA: 7:36 - loss: 5.1262 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1345/3649 [==========>...................] - ETA: 7:36 - loss: 5.1233 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1346/3649 [==========>...................] - ETA: 7:36 - loss: 5.1245 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1347/3649 [==========>...................] - ETA: 7:35 - loss: 5.1230 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1348/3649 [==========>...................] - ETA: 7:35 - loss: 5.1217 - accuracy: 0.2493\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1349/3649 [==========>...................] - ETA: 7:35 - loss: 5.1224 - accuracy: 0.2491\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1350/3649 [==========>...................] - ETA: 7:35 - loss: 5.1188 - accuracy: 0.2496\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1351/3649 [==========>...................] - ETA: 7:35 - loss: 5.1206 - accuracy: 0.2494\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1352/3649 [==========>...................] - ETA: 7:34 - loss: 5.1177 - accuracy: 0.2500\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1353/3649 [==========>...................] - ETA: 7:34 - loss: 5.1230 - accuracy: 0.2498\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1354/3649 [==========>...................] - ETA: 7:34 - loss: 5.1203 - accuracy: 0.2504\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1355/3649 [==========>...................] - ETA: 7:34 - loss: 5.1166 - accuracy: 0.2509\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1356/3649 [==========>...................] - ETA: 7:34 - loss: 5.1159 - accuracy: 0.2507\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1357/3649 [==========>...................] - ETA: 7:33 - loss: 5.1132 - accuracy: 0.2513\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1358/3649 [==========>...................] - ETA: 7:33 - loss: 5.1148 - accuracy: 0.2511\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1359/3649 [==========>...................] - ETA: 7:33 - loss: 5.1203 - accuracy: 0.2509\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1360/3649 [==========>...................] - ETA: 7:33 - loss: 5.1167 - accuracy: 0.2515\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1361/3649 [==========>...................] - ETA: 7:33 - loss: 5.1184 - accuracy: 0.2513\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1362/3649 [==========>...................] - ETA: 7:32 - loss: 5.1151 - accuracy: 0.2518\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1363/3649 [==========>...................] - ETA: 7:32 - loss: 5.1181 - accuracy: 0.2517\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1364/3649 [==========>...................] - ETA: 7:32 - loss: 5.1145 - accuracy: 0.2522\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1365/3649 [==========>...................] - ETA: 7:32 - loss: 5.1139 - accuracy: 0.2520\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1366/3649 [==========>...................] - ETA: 7:32 - loss: 5.1193 - accuracy: 0.2518\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1367/3649 [==========>...................] - ETA: 7:31 - loss: 5.1180 - accuracy: 0.2516\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1368/3649 [==========>...................] - ETA: 7:31 - loss: 5.1210 - accuracy: 0.2515\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1369/3649 [==========>...................] - ETA: 7:31 - loss: 5.1224 - accuracy: 0.2513\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1370/3649 [==========>...................] - ETA: 7:31 - loss: 5.1230 - accuracy: 0.2511\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1371/3649 [==========>...................] - ETA: 7:31 - loss: 5.1210 - accuracy: 0.2509\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1372/3649 [==========>...................] - ETA: 7:30 - loss: 5.1233 - accuracy: 0.2507\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1373/3649 [==========>...................] - ETA: 7:30 - loss: 5.1254 - accuracy: 0.2505\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1374/3649 [==========>...................] - ETA: 7:30 - loss: 5.1219 - accuracy: 0.2511\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1375/3649 [==========>...................] - ETA: 7:30 - loss: 5.1198 - accuracy: 0.2516\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1376/3649 [==========>...................] - ETA: 7:30 - loss: 5.1248 - accuracy: 0.2515\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1377/3649 [==========>...................] - ETA: 7:29 - loss: 5.1225 - accuracy: 0.2513\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1378/3649 [==========>...................] - ETA: 7:29 - loss: 5.1269 - accuracy: 0.2511\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1379/3649 [==========>...................] - ETA: 7:29 - loss: 5.1255 - accuracy: 0.2509\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1380/3649 [==========>...................] - ETA: 7:29 - loss: 5.1220 - accuracy: 0.2514\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1381/3649 [==========>...................] - ETA: 7:29 - loss: 5.1205 - accuracy: 0.2513\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1382/3649 [==========>...................] - ETA: 7:28 - loss: 5.1169 - accuracy: 0.2518\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1383/3649 [==========>...................] - ETA: 7:28 - loss: 5.1219 - accuracy: 0.2516\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1384/3649 [==========>...................] - ETA: 7:28 - loss: 5.1216 - accuracy: 0.2514\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1385/3649 [==========>...................] - ETA: 7:28 - loss: 5.1180 - accuracy: 0.2520\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1386/3649 [==========>...................] - ETA: 7:28 - loss: 5.1175 - accuracy: 0.2518\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1387/3649 [==========>...................] - ETA: 7:27 - loss: 5.1176 - accuracy: 0.2516\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1388/3649 [==========>...................] - ETA: 7:27 - loss: 5.1161 - accuracy: 0.2514\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1389/3649 [==========>...................] - ETA: 7:27 - loss: 5.1148 - accuracy: 0.2513\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1390/3649 [==========>...................] - ETA: 7:27 - loss: 5.1135 - accuracy: 0.2511\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1391/3649 [==========>...................] - ETA: 7:27 - loss: 5.1100 - accuracy: 0.2516\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1392/3649 [==========>...................] - ETA: 7:26 - loss: 5.1076 - accuracy: 0.2522\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1393/3649 [==========>...................] - ETA: 7:26 - loss: 5.1063 - accuracy: 0.2520\n",
            "Epoch 00002: loss did not improve from 4.76775\n",
            "1394/3649 [==========>...................] - ETA: 7:26 - loss: 5.1028 - accuracy: 0.2525"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3436kFiqpL7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}